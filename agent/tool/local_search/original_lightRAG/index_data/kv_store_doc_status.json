{
  "doc-32bcdbba2d266d061d85f94faf74b18a": {
    "status": "processed",
    "chunks_count": 1,
    "chunks_list": [
      "chunk-32bcdbba2d266d061d85f94faf74b18a"
    ],
    "content_summary": "ASurveyofReinforcementLearningforLargeReasoningModels 2025-09-11\nA Survey of Reinforcement Learning for Large\nReasoning Models\nKaiyanZhang1‚àó‚Ä†,YuxinZuo1‚àó‚Ä†,BingxiangHe1‚àó,YoubangSun1‚àó,RunzeLiu1‚àó,CheJiang1‚àó,YuchenFan2,3‚àó,\nKaiTian1‚àó,GuoliJia1‚àó,PengfeiLi2,...",
    "content_length": 3353,
    "created_at": "2025-12-08T18:28:02.831627+00:00",
    "updated_at": "2025-12-08T18:30:08.389572+00:00",
    "file_path": "unknown_source",
    "track_id": "insert_20251209_052802_bfe04981",
    "metadata": {
      "processing_start_time": 1765218482,
      "processing_end_time": 1765218608
    }
  },
  "doc-b836ebc3527208cd954c81601d5fc314": {
    "status": "processed",
    "chunks_count": 2,
    "chunks_list": [
      "chunk-7f8d14eb8cca7addf208d3ebd7dc7029",
      "chunk-d8e725479a22750b3ff1aef7d1040743"
    ],
    "content_summary": "Contents\n1 Introduction 4\n2 Preliminaries 5\n2.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2 Frontier Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.3 Rel...",
    "content_length": 2938,
    "created_at": "2025-12-08T18:30:08.722093+00:00",
    "updated_at": "2025-12-08T18:30:41.946614+00:00",
    "file_path": "unknown_source",
    "track_id": "insert_20251209_053008_0c41a67f",
    "metadata": {
      "processing_start_time": 1765218608,
      "processing_end_time": 1765218641
    }
  },
  "doc-0f35d639cf51f0ab7d4017f06d35581f": {
    "status": "processed",
    "chunks_count": 1,
    "chunks_list": [
      "chunk-0f35d639cf51f0ab7d4017f06d35581f"
    ],
    "content_summary": "ASurveyofReinforcementLearningforLargeReasoningModels\n6.4 Multi-Agent Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n6.5 Robotics Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ...",
    "content_length": 1300,
    "created_at": "2025-12-08T18:30:42.265771+00:00",
    "updated_at": "2025-12-08T18:31:26.344265+00:00",
    "file_path": "unknown_source",
    "track_id": "insert_20251209_053042_2e0c22d2",
    "metadata": {
      "processing_start_time": 1765218642,
      "processing_end_time": 1765218686
    }
  },
  "doc-8b7f62b7202fa8bc4469bb44567df186": {
    "status": "processed",
    "chunks_count": 1,
    "chunks_list": [
      "chunk-8b7f62b7202fa8bc4469bb44567df186"
    ],
    "content_summary": "ASurveyofReinforcementLearningforLargeReasoningModels\n1. Introduction\nReinforcement Learning (RL) [Sutton et al., 1998] has repeatedly demonstrated that narrow, well-\nspecified reward signals can drive artificial agents to superhuman competence on co...",
    "content_length": 4287,
    "created_at": "2025-12-08T18:31:26.705355+00:00",
    "updated_at": "2025-12-08T18:32:53.992549+00:00",
    "file_path": "unknown_source",
    "track_id": "insert_20251209_053126_0cfa4c2d",
    "metadata": {
      "processing_start_time": 1765218686,
      "processing_end_time": 1765218773
    }
  },
  "doc-8355cb8033a4afe3e01992fcee557002": {
    "status": "processed",
    "chunks_count": 1,
    "chunks_list": [
      "chunk-8355cb8033a4afe3e01992fcee557002"
    ],
    "content_summary": "ASurveyofReinforcementLearningforLargeReasoningModels\nRL from Human Direct Preference RL with Verifiable\nFeedback Optimization Reward\nGPT-3.5, GPT-4 Llama 3, Qwen 2.5 o1, DeepSeek-R1\nReward-based Reward-free Rule-based\n2022 2023 2025\nyticapaC\ngnivloS...",
    "content_length": 2304,
    "created_at": "2025-12-08T18:32:54.521870+00:00",
    "updated_at": "2025-12-08T18:34:21.814699+00:00",
    "file_path": "unknown_source",
    "track_id": "insert_20251209_053254_25e38691",
    "metadata": {
      "processing_start_time": 1765218774,
      "processing_end_time": 1765218861
    }
  },
  "doc-42002d06ba2f88fbde0f4fdbf912a18f": {
    "status": "processed",
    "chunks_count": 1,
    "chunks_list": [
      "chunk-42002d06ba2f88fbde0f4fdbf912a18f"
    ],
    "content_summary": "ASurveyofReinforcementLearningforLargeReasoningModels\naction\nAgent reward\nR(x,y)\ny 1 y 2\nstate reward action Language Model\nst rt at\n¬∑¬∑¬∑¬∑¬∑¬∑\nr t+1 x 1 x 2 x 3 x T-1 x T\nEnvironment\nr\nt+1 state\nFigure 3 | Basic components of RL and language models (LMs...",
    "content_length": 2627,
    "created_at": "2025-12-08T18:34:22.560737+00:00",
    "updated_at": "2025-12-08T18:35:08.315205+00:00",
    "file_path": "unknown_source",
    "track_id": "insert_20251209_053422_8081ffdb",
    "metadata": {
      "processing_start_time": 1765218862,
      "processing_end_time": 1765218908
    }
  },
  "doc-37f12102e8df23d0bf48e221b1e75d9e": {
    "status": "processed",
    "chunks_count": 1,
    "chunks_list": [
      "chunk-37f12102e8df23d0bf48e221b1e75d9e"
    ],
    "content_summary": "ASurveyofReinforcementLearningforLargeReasoningModels\nIn practice, it is common to regularize the learned policy towards a reference policy ùúã , often\nref\nimplemented as KL-divergence constraints to stabilize training and maintain language quality. In...",
    "content_length": 4204,
    "created_at": "2025-12-08T18:35:09.014900+00:00",
    "updated_at": "2025-12-08T18:37:24.495349+00:00",
    "file_path": "unknown_source",
    "track_id": "insert_20251209_053509_43708951",
    "metadata": {
      "processing_start_time": 1765218909,
      "processing_end_time": 1765219044
    }
  },
  "doc-aa95a088a3c7d1c923aa324fd7c02a63": {
    "status": "processed",
    "chunks_count": 2,
    "chunks_list": [
      "chunk-aa95a088a3c7d1c923aa324fd7c02a63",
      "chunk-519d56991b1bbd2046e61bcd26303d2e"
    ],
    "content_summary": "ASurveyofReinforcementLearningforLargeReasoningModels\n8\nGPT-5\nLegend 6 Claude Opus 4.1\nDeepSeek-V3.1 671B\nMultimodal o3-pro gpt-oss 21/117B\nGemini 2.5 Flash GLM-4.5V 106B\nPublicly Unavailable\n4 Gemini 2.5 Pro InternVL3.5 1-241B\nPublicly Available See...",
    "content_length": 3670,
    "created_at": "2025-12-08T18:37:25.440463+00:00",
    "updated_at": "2025-12-08T18:39:27.121792+00:00",
    "file_path": "unknown_source",
    "track_id": "insert_20251209_053725_9c43ff6b",
    "metadata": {
      "processing_start_time": 1765219045,
      "processing_end_time": 1765219167
    }
  },
  "doc-9f6704aa27cd8e9483d9f72f2cf4c3fe": {
    "status": "processed",
    "chunks_count": 1,
    "chunks_list": [
      "chunk-9f6704aa27cd8e9483d9f72f2cf4c3fe"
    ],
    "content_summary": "ASurveyofReinforcementLearningforLargeReasoningModels\nTable 1 | Comparison of representative open-source models trained with RL. OPMD denotes Online\nPolicy Mirror Descent; MPO denotes Mixed Preference Optimization; CISPO denotes Clipped IS-\nweight Po...",
    "content_length": 2281,
    "created_at": "2025-12-08T18:39:28.178617+00:00",
    "updated_at": "2025-12-08T18:41:02.639794+00:00",
    "file_path": "unknown_source",
    "track_id": "insert_20251209_053928_cd6c39de",
    "metadata": {
      "processing_start_time": 1765219168,
      "processing_end_time": 1765219262
    }
  },
  "doc-5b53f848c47ba7e516ea81d9d2659625": {
    "status": "processing",
    "chunks_count": 1,
    "chunks_list": [
      "chunk-5b53f848c47ba7e516ea81d9d2659625"
    ],
    "content_summary": "ASurveyofReinforcementLearningforLargeReasoningModels\nrecentprogressacrossdiversereasoningtasks,methodologies,andbenchmarks. Zhangetal.[2025b]\nexaminehowRLcanendowLLMswithautonomousdecision-makingandadaptiveagenticcapabilities.\nXu et al. [2025a] move...",
    "content_length": 3729,
    "created_at": "2025-12-08T18:41:03.896614+00:00",
    "updated_at": "2025-12-08T18:51:14.883945+00:00",
    "file_path": "unknown_source",
    "track_id": "insert_20251209_054103_15632f70",
    "metadata": {
      "processing_start_time": 1765219874
    }
  }
}