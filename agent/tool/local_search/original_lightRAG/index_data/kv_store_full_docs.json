{
  "doc-32bcdbba2d266d061d85f94faf74b18a": {
    "content": "ASurveyofReinforcementLearningforLargeReasoningModels 2025-09-11\nA Survey of Reinforcement Learning for Large\nReasoning Models\nKaiyanZhang1âˆ—â€ ,YuxinZuo1âˆ—â€ ,BingxiangHe1âˆ—,YoubangSun1âˆ—,RunzeLiu1âˆ—,CheJiang1âˆ—,YuchenFan2,3âˆ—,\nKaiTian1âˆ—,GuoliJia1âˆ—,PengfeiLi2,6âˆ—,YuFu9âˆ—,XingtaiLv1âˆ—,YuchenZhang2,4âˆ—,SihangZeng7âˆ—,ShangQu1,2âˆ—,\nHaozhanLi1âˆ—,ShijieWang2âˆ—,YuruWang1âˆ—,XinweiLong1,FangfuLiu1,XiangXu5,JiazeMa1,XuekaiZhu3,\nErmoHua1,2,YihaoLiu1,2,ZonglinLi2,HuayuChen1,XiaoyeQu2,YafuLi2,WeizeChen1,ZhenzhaoYuan1,\nJunqiGao6,DongLi6,ZhiyuanMa8,GanquCui2,ZhiyuanLiu1,BiqingQi2â€¡,NingDing1,2â€¡,BowenZhou1,2â€¡\n1TsinghuaUniversity 2ShanghaiAILaboratory 3ShanghaiJiaoTongUniversity 4PekingUniversity\n5UniversityofScienceandTechnologyofChina 6HarbinInstituteofTechnology 7UniversityofWashington\n8HuazhongUniversityofScienceandTechnology 9UniversityCollegeLondon\nâ€ ProjectLead. âˆ—CoreContributors. â€¡CorrespondingAuthors.\n#zhang-ky22@mails.tsinghua.edu.cn Â§TsinghuaC3I/Awesome-RL-for-LRMs\nAbstract | Inthispaper,wesurveyrecentadvancesinReinforcementLearning(RL)forreasoningwithLarge\nLanguageModels(LLMs). RLhasachievedremarkablesuccessinadvancingthefrontierofLLMcapabilities,\nparticularlyinaddressingcomplexlogicaltaskssuchasmathematicsandcoding. Asaresult,RLhasemerged\nasafoundationalmethodologyfortransformingLLMsintoLRMs. Withtherapidprogressofthefield,further\nscaling of RL for LRMs now faces foundational challenges not only in computational resources but also in\nalgorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of\nthisdomain,reassessitstrajectory,andexplorestrategiestoenhancethescalabilityofRLtowardArtificial\nSuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning\nabilities, especially since the release of DeepSeek-R1, including foundational components, core problems,\ntrainingresources,anddownstreamapplications,toidentifyfutureopportunitiesanddirectionsforthisrapidly\nevolvingarea. WehopethisreviewwillpromotefutureresearchonRLforbroaderreasoningmodels.\nSection 3 Section 4\nFoundational Components Foundational Problems\nRLâ€™s Role Model Prior Training Recipes RL vs. SFT Reward Type\nReward Design\nVS VS VS VS VS\nSharpening Discovery Weak Strong Tricks Traps Generalize Memorize Process Outcome\nVerifiable Generative\nRewards Rewards\nR D ew en a s r e d s Un R s e u w pe a r r v d is s ed R S e h w ap a i r n d g s Static Corpus Tr S ain e in c g t R io es n o u 5 rce Dynamic Environment Section 6\nApplications\nPolicy Optimization Math Code RL Infrastructure & Rule Code\nFrameworks\nPolicy Critic-Based STEM Agent Mixture e.g.,OpenRLHF/veRL/AReaL/slime/TRL Game Model Ensemble\nGradient Algorithms\nAgentic Tasks Coding Tasks\nCritic-Free Off-Policy Regularization\nAlgorithms Optimization Objectives\nMultimodal Robotics\nTasks Tasks Sampling Strategy\nDynamic Sampling Multi-Agent Medical\nSampling Hyper-Parameters Systems Tasks\nCompute\necnegilletnI More Scalable\nEvolution(Training Step)\n)edosipE(\nnoitcaretnI\nAction\nat\nAgent Environment\nReward\nrt\nFigure 1 | Overview of the survey. We introduce the foundational components of RL for LRMs,\nalong with open problems, training resources, and applications. Central to this survey is a focus on\nlarge-scale interactions between language agents and environments throughout long-term evolution.\n5202\npeS\n81\n]LC.sc[\n2v72880.9052:viXra",
    "file_path": "unknown_source",
    "create_time": 1765218482,
    "update_time": 1765218482,
    "_id": "doc-32bcdbba2d266d061d85f94faf74b18a"
  },
  "doc-b836ebc3527208cd954c81601d5fc314": {
    "content": "Contents\n1 Introduction 4\n2 Preliminaries 5\n2.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2 Frontier Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.3 Related Surveys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n3 Foundational Components 10\n3.1 Reward Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3.1.1 Verifiable Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3.1.2 Generative Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n3.1.3 Dense Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n3.1.4 Unsupervised Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n3.1.5 Rewards Shaping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n3.2 Policy Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n3.2.1 Policy Gradient Objective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n3.2.2 Critic-based Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n3.2.3 Critic-Free Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n3.2.4 Off-policy Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n3.2.5 Regularization Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n3.3 Sampling Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n3.3.1 Dynamic and Structured Sampling . . . . . . . . . . . . . . . . . . . . . . . . 28\n3.3.2 Sampling Hyper-parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n4 Foundational Problems 30\n4.1 RLâ€™s Role: Sharpening or Discovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n4.2 RL vs. SFT: Generalize or Memorize. . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n4.3 Model Prior: Weak and Strong . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n4.4 Training Recipes: Tricks or Traps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n4.5 Reward Type: Process or Outcome . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n5 Training Resources 37\n5.1 Static Corpus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n5.2 Dynamic Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n5.3 RL Infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n6 Applications 46\n6.1 Coding Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n6.2 Agentic Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n6.3 Multimodal Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\n2",
    "file_path": "unknown_source",
    "create_time": 1765218608,
    "update_time": 1765218608,
    "_id": "doc-b836ebc3527208cd954c81601d5fc314"
  },
  "doc-0f35d639cf51f0ab7d4017f06d35581f": {
    "content": "ASurveyofReinforcementLearningforLargeReasoningModels\n6.4 Multi-Agent Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n6.5 Robotics Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n6.6 Medical Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n7 Future Directions 60\n7.1 Continual RL for LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n7.2 Memory-based RL for LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n7.3 Model-based RL for LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n7.4 Teaching LRMs Efficient Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n7.5 Teaching LLMs Latent Space Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n7.6 RL for LLMs Pre-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n7.7 RL for Diffusion-based LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n7.8 RL for LLMs in Scientific Discovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n7.9 RL for Architecture-Algorithm Co-Design . . . . . . . . . . . . . . . . . . . . . . . . . 63\n8 Conclusion 64\nAuthor Contributions 65\n3",
    "file_path": "unknown_source",
    "create_time": 1765218642,
    "update_time": 1765218642,
    "_id": "doc-0f35d639cf51f0ab7d4017f06d35581f"
  },
  "doc-8b7f62b7202fa8bc4469bb44567df186": {
    "content": "ASurveyofReinforcementLearningforLargeReasoningModels\n1. Introduction\nReinforcement Learning (RL) [Sutton et al., 1998] has repeatedly demonstrated that narrow, well-\nspecified reward signals can drive artificial agents to superhuman competence on complex tasks.\nLandmark systems such as AlphaGo [Silver et al., 2016] and AlphaZero [Silver et al., 2017], which\nlearned exclusively through self-play and reward feedback, surpassed world champions in Go, chess,\nshogi and Stratego [Perolat et al., 2022, Schrittwieser et al., 2020, Silver et al., 2018], establishing\nRL as a practical and promising technology for high-level problem solving. In the era of Large\nLanguage Models (LLMs) [Zhao et al., 2023a], RL initially rose to prominence as a post-training\nstrategyforhumanalignment[Ouyangetal.,2022]. WidelyadoptedmethodssuchasReinforcement\nLearningfromHumanFeedback(RLHF)[Christianoetal.,2017]andDirectPreferenceOptimization\n(DPO) [Rafailov et al., 2023] finetune pre-trained models to follow instructions and reflect human\npreferences, markedly improving helpfulness, honesty, and harmlessness (3H) [Bai et al., 2022b].\nMorerecently,anewtrendhasemerged: RLforLargeReasoningModels(LRMs)[Xuetal.,2025a],\nwhich aims not merely to align behavior but to incentivize reasoning itself. Two recent milestones\n(i.e.,OpenAIo1[Jaechetal.,2024]andDeepSeek-R1[Guoetal.,2025a])demonstratethattraining\nLLMs using reinforcement learning with verifiable rewards (RLVR), such as answer correctness for\nmathematics or unit-test pass rates for code, can enable models to perform long-form reasoning,\nincluding planning, reflection, and self-correction. OpenAI reports [Jaech et al., 2024] that o1â€™s\nperformance improves smoothly with both additional RL (increased train-time compute) and more\ntime spent â€œthinkingâ€ at inference (test-time compute) [Brown et al., 2024, Liu et al., 2025m, Snell\netal.,2024],revealinganewscalingaxisbeyondpre-trainingalone[Aghajanyanetal.,2023,Kaplan\net al., 2020]. DeepSeek-R1 [Guo et al., 2025a] employs explicit, rule-based accuracy rewards for\nmathematics,aswellascompiler-ortest-basedrewardsforcodingtasks. Thisapproachdemonstrates\nthat large-scale reinforcement learning, specifically, Group Relative Policy Optimization (GRPO), can\ninduce sophisticated reasoning behaviors even in base models prior to subsequent alignment stages.\nThis shift reframes reasoning as a capability that can be explicitly trained and scaled [OpenAI,\n2025a,b]: LRMs allocate significant test-time compute to generate, evaluate, and revise intermediate\nchain-of-thought [Wei et al., 2022], and their performance rises as this compute budget increases.\nThisdynamicintroducesacomplementarypathtocapabilitygains,orthogonaltodataandparameter\nscaling during pre-training [Aghajanyan et al., 2023, Kaplan et al., 2020], while leveraging a reward\nmaximization objective [Silver et al., 2021], automatically checkable rewards wherever reliable\nverifiers exist (e.g., competition mathematics [Guo et al., 2025a, Jaech et al., 2024], competitive\nprogramming[El-Kishkyetal.,2025],andselectedscientificdomains[Baietal.,2025]). Furthermore,\nRL can overcome data limitations [Shumailov et al., 2024, Villalobos et al., 2022] by enabling self-\ngenerated training data [Silver et al., 2018, Zhao et al., 2025a]. As a result, RL is increasingly\nregardedasapromisingtechnologyforachievingArtificialSuperIntelligence(ASI)onabroaderrange\nof tasks through continual scaling.\nAt the same time, further scaling of RL for LRMs introduces new constraints, not only in compu-\ntational resources, but also in algorithm design, training data, and infrastructure. How and where\nRL for LRMs can be scaled to achieve high-level intelligence and generate real-world value remain\nunresolved issues. Therefore, we argue that it is timely to revisit the development of this domain and\nexplore strategies to enhance the scalability of RL toward artificial superintelligence. In summary,\nthis survey reviews recent work on RL for LRMs as follows:\nâ€¢ We introduce the preliminary definitions of RL modeling in the context of LRMs (Â§ 2.1) and\noutline the development of frontier reasoning models since the release of OpenAI o1 (Â§ 2.2).\nâ€¢ We review recent literature on the foundational components of RL for LRMs, including reward\n4",
    "file_path": "unknown_source",
    "create_time": 1765218686,
    "update_time": 1765218686,
    "_id": "doc-8b7f62b7202fa8bc4469bb44567df186"
  },
  "doc-8355cb8033a4afe3e01992fcee557002": {
    "content": "ASurveyofReinforcementLearningforLargeReasoningModels\nRL from Human Direct Preference RL with Verifiable\nFeedback Optimization Reward\nGPT-3.5, GPT-4 Llama 3, Qwen 2.5 o1, DeepSeek-R1\nReward-based Reward-free Rule-based\n2022 2023 2025\nyticapaC\ngnivloS\nksaT\nRLHF DPO RLVR Open-ended RL\nFigure2 | RLHFandDPOhavebeenthetwopredominantRLmethodologiesforhumanalignmentin\nrecent years. In contrast, RLVR represents an emerging trend in RL for LRMs, significantly enhancing\ntheir capacity for complex task solving. The next stage of scaling RL for LLMs remains an open\nquestion, with open-ended RL presenting a particularly challenging and promising direction.\ndesign (Â§ 3.1), policy optimization (Â§ 3.2), and sampling strategies (Â§ 3.3), comparing the\ndifferent research directions and technical approaches for each component.\nâ€¢ We discuss foundational and still controversial problems in RL for LRMs (Â§ 4), such as the role of\nRL(Â§4.1),RLversusSupervisedFine-Tuning(SFT)(Â§4.2),modelpriors(Â§4.3),trainingrecipes\n(Â§4.4), andrewarddefinitions(Â§4.5). Wearguethattheseissueswarrantfurtherexplorationto\nenable continued scaling of RL.\nâ€¢ WeexaminetrainingresourcesforRL(Â§5),includingstaticcorpora(Â§5.1),dynamicenvironments\n(Â§ 5.2), and training infrastructure (Â§ 5.3). While these resources are reusable in both research\nand production, further standardization and development are needed.\nâ€¢ We review applications of RL to a wide range of tasks (Â§ 6), such as coding tasks (Â§ 6.1), agentic\ntasks (Â§ 6.2), multimodal tasks (Â§ 6.3), multi-agent systems (Â§ 6.4), robotics tasks (Â§ 6.5), and\nmedical applications (Â§ 6.6).\nâ€¢ Finally, we discuss future directions in RL for language models (Â§ 7), covering novel algorithms,\nmechanisms, features, and additional research avenues.\n2. Preliminaries\n2.1. Background\nInthissubsection,weintroducethebasiccomponentsofRLanddescribehowlanguagemodelscanbe\nconfigured as agents within RL frameworks. As shown in Figure 3, RL provides a general framework\nfor sequential decision making, in which an agent interacts with an environment by taking actions\nto maximize cumulative reward. In classical RL, the problem is typically formulated as a Markov\n5\n\nTables:\n[\n  [\n    [\n      \"RLHF\",\n      \"DPO\",\n      \"RLVR\",\n      \"Open-ended RL\"\n    ]\n  ],\n  [\n    [\n      \"\",\n      \"\"\n    ]\n  ]\n]",
    "file_path": "unknown_source",
    "create_time": 1765218774,
    "update_time": 1765218774,
    "_id": "doc-8355cb8033a4afe3e01992fcee557002"
  },
  "doc-42002d06ba2f88fbde0f4fdbf912a18f": {
    "content": "ASurveyofReinforcementLearningforLargeReasoningModels\naction\nAgent reward\nR(x,y)\ny 1 y 2\nstate reward action Language Model\nst rt at\nÂ·Â·Â·Â·Â·Â·\nr t+1 x 1 x 2 x 3 x T-1 x T\nEnvironment\nr\nt+1 state\nFigure 3 | Basic components of RL and language models (LMs) as agents. The agent selects actions,\nwhile the environment provides states and rewards at each turn. In the context of LMs, completion\ntokensaretreatedasactions,whichareconcatenatedwiththecontexttoformthestate. Rewardsare\ntypically assigned at the level of the entire response.\nDecision Process (MDP) [Sutton et al., 1998], which is defined by a tuple (S,A,P,ğ‘…,ğ›¾). The main\ncomponentsincludeastatespaceS,anactionspaceA,transitiondynamicsP : SÃ—A â†¦â†’ S,areward\nfunction ğ‘… : SÃ—A â†¦â†’ â„, and a discount factor ğ›¾ âˆˆ [0,1]. At each step, the agent observes a state ğ‘ \nğ‘¡\n,\nselectsanaction ğ‘ accordingtoitspolicyğœ‹ parameterizedbyğœƒ,receivesarewardğ‘Ÿ ,andtransitsto\nğ‘¡ ğœƒ ğ‘¡\nthe next state ğ‘  . When applying RL to language models, these concepts can be naturally mapped\nğ‘¡+1\nto the language domain with minimal adaptation. The mapping is summarized as follows:\nâ€¢ Prompt/Task (ğ‘¥): Corresponds to the initial state or environment context, drawn from a data\ndistribution and corresponding to the dataset D.\nâ€¢ Policy (ğœ‹ ğœƒ): Representsthelanguagemodel,whichgeneratesasequenceoflengthğ‘‡ denotingas\nğ‘¦ = (ğ‘¦ 1 ,...,ğ‘¦ ğ‘‡ ) in response to the prompt.\nâ€¢ State (ğ‘  ğ‘¡): Defined as the prompt together with the tokens generated so far, i.e., ğ‘  ğ‘¡ = (ğ‘¥,ğ‘ 1:ğ‘¡âˆ’1 ).\nâ€¢ Action(ğ‘ ğ‘¡): Theunitchosenatstepğ‘¡ fromtheactionspaceA. Dependingonthegranularity,the\naction may be an entire sequence ğ‘¦ (sequence-level), a token ğ‘ âˆˆ V (token-level), or a segment\nğ‘¡\nğ‘¦(ğ‘˜) = (ğ‘¦(ğ‘˜),...,ğ‘¦(ğ‘˜)) (step-level), with a detailed comparison in Table 2.\n1 ğ‘‡\nğ‘˜\nâ€¢ Transition Dynamics (P): The state transition is usually deterministic in the context of LLMs\nsince ğ‘  ğ‘¡+1 = [ğ‘  ğ‘¡ ,ğ‘ ğ‘¡ ], where [Â·,Â·] denotes string concatenation. When the state contains an EOS\ntoken, the policy transits to a terminal state, meaning the trajectory ends.\nâ€¢ Reward (ğ‘…(ğ‘¥,ğ‘¦) or ğ‘Ÿ ğ‘¡): Assigned based on the action granularity, e.g., sequence-level ğ‘…(ğ‘¥,ğ‘¦) at\ntrajectory end, token-level ğ‘Ÿ ğ‘¡ =ğ‘…(ğ‘¥,ğ‘ 1:ğ‘¡ ) per token, or step-level ğ‘Ÿ ğ‘˜ =ğ‘…(ğ‘¥,ğ‘¦(1:ğ‘˜)) per segment.\nâ€¢ Return(ğº): Thecumulativerewardofthewholetrajectory ğ‘¦forpromptğ‘¥(typicallywithğ›¾ =1for\nfinite horizons). It reduces to the single scalar ğ‘…(ğ‘¥,ğ‘¦) with sequence-level reward, or aggregates\nper-token/step rewards otherwise, as detailed in Table 2.\nInthissetting,thelearningobjective[Suttonetal.,1998]istomaximizetheexpectedcumulative\nreward over the data distribution D, that is,\nmaxJ(ğœƒ) :=ğ”¼ [ğº]. (1)\nğœƒ\nğ‘¥âˆ¼D,ğ‘¦âˆ¼ğœ‹ ğœƒ(ğ‘¥)\n6",
    "file_path": "unknown_source",
    "create_time": 1765218862,
    "update_time": 1765218862,
    "_id": "doc-42002d06ba2f88fbde0f4fdbf912a18f"
  },
  "doc-37f12102e8df23d0bf48e221b1e75d9e": {
    "content": "ASurveyofReinforcementLearningforLargeReasoningModels\nIn practice, it is common to regularize the learned policy towards a reference policy ğœ‹ , often\nref\nimplemented as KL-divergence constraints to stabilize training and maintain language quality. In the\nfollowing sections, we present various algorithms that build upon this fundamental formulation.\n2.2. Frontier Models\nIn this subsection, we provide an overview of state-of-the-art large reasoning models trained with\nRL-like methods, organized roughly chronologically along three major directions: LRMs, agentic\nLRMs, and multimodal LRMs.\nOver the past year, RL has progressively expanded the frontier of reasoning models and their\napplications. Thefirstlargereasoningmodels,OpenAIâ€™so1[Jaechetal.,2024]series,establishedthe\neffectiveness of scaling both train-time RL and test-time compute towards more powerful reasoning\nabilities, achieving leading results on mathematics, coding, and science benchmarks. DeepSeekâ€™s\nflagship model R1 [Guo et al., 2025a] followed as the first open-source model to match o1â€™s per-\nformance across benchmarks. It employs a multi-stage training pipeline to ensure well-rounded\nmodel abilities, and explores the route of pure RL without supervised finetuning (i.e., Zero RL).\nOther proprietary model releases promptly followed: Claude-3.7-Sonnet [Anthropic, 2025a] featured\nhybrid reasoning, Gemini 2.0 and 2.5 [Comanici et al., 2025] introduced longer context lengths,\nSeed-Thinking 1.5 [Seed et al., 2025b] featured generalization across domains, and the o3 [OpenAI,\n2025b]seriesshowcasedincreasinglyadvancedreasoningabilities. Recently,OpenAIintroducedtheir\nfirstopen-sourcereasoningmodelgpt-oss-120b[Agarwaletal.,2025a],andsubsequentlyGPT5[Ope-\nnAI,2025a],theirmostcapableAIsystemtodate,whichflexiblyswitchesbetweenanefficientmodel\nand a deeper reasoning model GPT-5 thinking. Parallel open-source efforts continued to expand\nthe landscape. Within the Qwen family, QwQ-32B [Team, 2025g] matched R1â€™s performance, and\nwas followed by the Qwen3 [Yang et al., 2025a] series, with the representative model Qwen3-235B\nfurther improving benchmark scores. The Skywork-OR1 [He et al., 2025d] suite of models were\nbased on R1-distilled models, and achieved scalable RL training through effective data mixtures and\nalgorithmic innovations. Minimax-M1 [Chen et al., 2025a] was the first model to introduce hybrid\nattention to scale RL efficiently. Other works include Llama-Nemotron-Ultra [Bercovich et al., 2025],\nwhichaimedtobalanceaccuracyandefficiency;Magistral24B[Rastogietal.,2025],trainedthrough\nRL from scratch without distillation from prior models; and Seed-OSS [Team, 2025a], emphasizing\nlong-context reasoning abilities.\nModel reasoning improvements have in turn extended their use cases in coding and agentic\nscenarios. The Claude series has been known for their leading performance on agentic coding tasks,\nand this was exemplified by Claude-4.1-Opus [Anthropic, 2025b], which further pushed forward\nthe state-of-the-art results on SWE-bench [Jimenez et al., 2023]. Kimi K2 [Team, 2025d] is a recent\nrepresentative agentic model which was specifically optimized for agentic tasks, forging large-scale\nagentictrainingdatasynthesisandageneralRLprocedurethataccommodatesnon-verifiablerewards.\nShortly after, both the GLM4.5 [Zeng et al., 2025a] and DeepSeek-V3.1 releases emphasized tool-use\nand agentic tasks, showing substantial improvements on relevant benchmarks.\nMultimodality is a key component behind the widespread adoption of reasoning models. Most\nfrontierproprietarymodels,includingGPT-5,o3,Claude,andGeminifamilies,arenativelymultimodal.\nGemini-2.5[Comanicietal.,2025]notablyemphasizedstrongperformanceacrosstext,images,video,\nand audio. On the open-source side, Kimi 1.5 [Team, 2025d] represents an early effort towards\nmultimodalreasoning,highlightinglongcontextscalingaswellasjointreasoningovertextandvision\ndomains. QVQ [Qwen Team, 2025] excels in visual reasoning and analytical thinking, while Skywork\nR1V2 [Wang et al., 2025k] balances reasoning and general abilities through hybrid RL, using both\nMPO and GRPO. As notable additions to the InternVL series, InternVL3 [Zhu et al., 2025c] adopted\n7",
    "file_path": "unknown_source",
    "create_time": 1765218909,
    "update_time": 1765218909,
    "_id": "doc-37f12102e8df23d0bf48e221b1e75d9e"
  },
  "doc-aa95a088a3c7d1c923aa324fd7c02a63": {
    "content": "ASurveyofReinforcementLearningforLargeReasoningModels\n8\nGPT-5\nLegend 6 Claude Opus 4.1\nDeepSeek-V3.1 671B\nMultimodal o3-pro gpt-oss 21/117B\nGemini 2.5 Flash GLM-4.5V 106B\nPublicly Unavailable\n4 Gemini 2.5 Pro InternVL3.5 1-241B\nPublicly Available Seed-Thinking 1.6 9\no3 Magistral 24B\no4-mini Minimax-M1 456B 7 Ring-mi ni-2.0 16B\nSeed-Thinking 1.5 Qwen 3-Next-80B-A3B-Thinking 80B\nPhi-4 Reasoning 14B Gemi ni2. 5Flash-Lite\nSkywork-R1V2 38B Gro k4\n2 InternVL3 1-78B 5 Intern-S 1241B\nMiMo 7B Kim ik 21T\nClaude 3.7 Sonnet Qwen3 0.6-235B Hunyuan-TurboS Ste p 3321B\no3-mini Claude 4 Qwen3-2507-Thinkin g4-235B\no1-preview Gemini 2.0 Flash Skywork OR-1 7/32B GLM-4.1V-Thinkin g9B\no1-mini\no1-2024-12-17 3 INTELLECT-2 32B GLM-4. 5355B\nLlama-Nemotron-Ultra 253B Skywor k-R1V3 38B\nQVQ-Max DeepSeek-R1-0528 671B\nORZ 0.5-32B\n1\nQWQ 32B\n2025 Kimi1.5\n2024 Deep Seek-R1671B\nFigure 4 | Timeline of representative open-source and closed-source reasoning models trained with\nRL, including language models, multimodal models, and agentic models.\na unified native multimodal pretraining phase, and later InternVL3.5 [Wang et al., 2025o] used a\ntwo-stage cascade RL framework, achieving improved efficiency and versatility. More recently, the\nIntern-S1 [Bai et al., 2025] model focused on multimodal scientific reasoning across diverse domains,\nbenefiting from a mixture-of-rewards design during online RL to facilitate simultaneous training on a\nwide range of tasks. Other recent models include Step3 [Wang et al., 2025a], designed for efficient\ntraining and minimizing decoding costs, and GLM-4.5V [Team et al., 2025a], with state-of-the-art\nperformance across most visual multimodal benchmarks.\nIn addition to the aforementioned models, we provide a comprehensive list of reasoning models\nin Figure 4 and detailed information on open-source models in Table 1.\n2.3. Related Surveys\nInthissubsection,wecomparerecentsurveysrelatedtoRLandLLMs. Severalsurveysfocusprimarily\non RL itself, covering both classical RL and its recent extensions. Ghasemi et al. [2024] present\na general RL survey covering algorithms and real-world challenges, Huh and Mohapatra [2023]\nfocuse on multi-agent RL, Zhang et al. [2024b] review self-play techniques, and Wu et al. [2025h]\nsurvey RL in computer vision tasks. While these works offer broad perspectives on RL, they do not\nexplicitlyaddressitsapplicationtoLLMs. Incontrast,othersurveyscenteronLLMsandtheiremerging\ncapabilities, such as long chain-of-thought reasoning [Chen et al., 2025m, Li et al., 2025w, Xia et al.,\n2024] and adaptive behaviors [Feng et al., 2025c, Sui et al., 2025], where RL is often introduced\nas a key method to support these advances. Zhao et al. [2023a] provide a broad overview of LLM\narchitecturesandapplications,whilemorerecentworksconcentratespecificallyonreasoningabilities.\nZhangetal.[2025a]surveyreplicationstudiesonreasoningLLMsinthewakeofDeepSeek-R1,Chen\netal.[2025m]examinelongchain-of-thoughtreasoning, andLietal.[2025w]analyzethetransition\nfrom System 1 to System 2 reasoning. These studies highlight RL-based methods such as RLHF and\nRLVR as useful tools, but treat them as only one element among a wide range of reasoning strategies.\nSun et al. [2025b] offer a broader, structured take on reasoning via foundation models. It highlights\nkey foundation models that are either proposed or adapted specifically for reasoning, as well as\n8\n\nTables:\n[\n  [\n    [\n      \"GPT-5\"\n    ],\n    [\n      \"Claude Opus 4.1\"\n    ],\n    [\n      \"DeepSeek-V3.1 671B\"\n    ],\n    [\n      \"gpt-oss 21/117B\\nGLM-4.5V 106B\"\n    ]\n  ],\n  [\n    [\n      \"o3-pro\\nGemini 2.5 Flash\"\n    ],\n    [\n      \"Gemini 2.5 Pro\\nSeed-Thinking 1.6\"\n    ]\n  ]\n]",
    "file_path": "unknown_source",
    "create_time": 1765219045,
    "update_time": 1765219045,
    "_id": "doc-aa95a088a3c7d1c923aa324fd7c02a63"
  },
  "doc-9f6704aa27cd8e9483d9f72f2cf4c3fe": {
    "content": "ASurveyofReinforcementLearningforLargeReasoningModels\nTable 1 | Comparison of representative open-source models trained with RL. OPMD denotes Online\nPolicy Mirror Descent; MPO denotes Mixed Preference Optimization; CISPO denotes Clipped IS-\nweight Policy Optimization. T, I, and V indicate Text, Image, and Video modalities, respectively.\nDate Model Organization Architecture Parameters Algorithm Modal Link\nDeepSeek-R1\n2025.01 DeepSeek MoE/MLA 671B GRPO Text Â§\n[Guoetal.,2025a]\nORZ\n2025.03 StepAI Dense 0.5-32B PPO Text Â§\n[Huetal.,2025b]\nQwQ\n2025.03 AlibabaQwen Dense 32B - Text Â§\n[Team,2025g]\nPhi-4Reasoning\n2025.04 Microsoft Dense 14B GRPO Text Â§\n[Abdinetal.,2025]\nSkywork-R1V2\n2025.04 Skywork Dense 38B MPO/GRPO T/I Â§\n[Wangetal.,2025k]\nInternVL3\n2025.04 ShanghaiAILab Dense 1-78B MPO T/I/V Â§\n[Zhuetal.,2025c]\nMiMo\n2025.04 Xiaomi Dense 7B GRPO Text Â§\n[Xiaomietal.,2025]\nQwen3\n2025.04 AlibabaQwen MoE/Dense 0.6-235B GRPO Text Â§\n[Yangetal.,2025a]\nLlama-Nemotron-Ultra\n2025.05 NVIDIA Dense 253B GRPO Text Â§\n[Bercovichetal.,2025]\nINTELLECT-2\n2025.05 IntellectAI Dense 32B GRPO Text\n[Teametal.,2025b]\nHunyuan-TurboS\n2025.05 Tencent HybridMoE 560B GRPO Text Â§\n[Teametal.,2025c]\nSkyworkOR-1\n2025.05 Skywork Dense 7B/32B GRPO Text Â§\n[Heetal.,2025d]\nDeepSeek-R1-0528\n2025.05 DeepSeek MoE/MLA 671B GRPO Text Â§\n[Guoetal.,2025a]\nMagistral\n2025.06 MistralAI Dense 24B GRPO Text\n[Rastogietal.,2025]\nMinimax-M1\n2025.06 Minimax HybridMoE 456B CISPO Text Â§\n[Chenetal.,2025a]\nIntern-S1\n2025.07 ShanghaiAILab MoE 241B GRPO T/I/V Â§\n[Baietal.,2025]\nKimiK2\n2025.07 Kimi MoE 1T OPMD Text Â§\n[Team,2025c]\nStep3\n2025.07 StepAI MoE 321B - T/I/V Â§\n[Wangetal.,2025a]\nQwen3-2507\n2025.07 AlibabaQwen MoE/Dense 4-235B GSPO Text Â§\n[Yangetal.,2025a]\nGLM-4.1V-Thinking\n2025.07 ZhipuAI Dense 9B GRPO T/I/V Â§\n[Teametal.,2025a]\nGLM-4.5\n2025.07 ZhipuAI MoE 355B GRPO Text Â§\n[Zengetal.,2025a]\nSkywork-R1V3\n2025.07 Skywork Dense 38B GRPO T/I Â§\n[Shenetal.,2025b]\ngpt-oss\n2025.08 OpenAI MoE 117B/21B - Text Â§\n[Agarwaletal.,2025a]\nSeed-OSS\n2025.08 BytedanceSeed Dense 36B - Text Â§\n[Team,2025a]\nGLM-4.5V\n2025.08 ZhipuAI MoE 106B GRPO T/I/V Â§\n[Teametal.,2025a]\nInternVL3.5\n2025.08 ShanghaiAILab MoE/Dense 1-241B MPO/GSPO T/I/V Â§\n[Wangetal.,2025o]\nERNIE-4.5-Thinking\n2025.09 Baidu MoE 21B-A3B - Text\n[Baidu-ERNIE-Team,2025]\n9",
    "file_path": "unknown_source",
    "create_time": 1765219168,
    "update_time": 1765219168,
    "_id": "doc-9f6704aa27cd8e9483d9f72f2cf4c3fe"
  },
  "doc-5b53f848c47ba7e516ea81d9d2659625": {
    "content": "ASurveyofReinforcementLearningforLargeReasoningModels\nrecentprogressacrossdiversereasoningtasks,methodologies,andbenchmarks. Zhangetal.[2025b]\nexaminehowRLcanendowLLMswithautonomousdecision-makingandadaptiveagenticcapabilities.\nXu et al. [2025a] move closer to our focus by discussing reinforced reasoning for LLMs, emphasizing\nhowtrial-and-erroroptimizationcanimprovecomplexreasoning. Wu[2025]complementthisviewby\nsurveyingrewardmodelsandstrategiesforlearningfromfeedback. Nevertheless,theseworksremain\norientedtowardsreasoningperformanceorrewarddesign,ratherthanofferingasystematictreatment\nofRLmethodsasawholeforLLMs. SrivastavaandAggarwal[2025]representamorerecentattempt\nto bridge the two fields by reviewing RL algorithms for LLM alignment and enhancement, primarily\nthroughmethodssuchasRLHF[Christianoetal.,2017],RLAIF[Leeetal.,2024b],andDPO[Rafailov\net al., 2023]. It remains primarily focused on alignment rather than reasoning capabilities.\nUnlike previous surveys that cover either general RL or reasoning in LLMs, we place RL at the\ncenter and provide a systematic synthesis of its role throughout the LLM training lifecycle, including\nrewarddesign,policyoptimization,andsamplingstrategies. Ouraimistoidentifynewdirectionsfor\nscaling reinforcement learning in LRMs toward ASI, focusing on long-term interactions and evolution.\n3. Foundational Components\nIn this section, we review the foundational components of RL for LRMs, including reward design\n(Â§ 3.1), policy optimization algorithms (Â§ 3.2), and sampling strategies (Â§ 3.3). The taxonomy of the\nfoundational components are shown in Figure 5.\n3.1. Reward Design\nIn this subsection, we provide a comprehensive examination of reward design in RL for LRMs. We\nbegin in Â§ 3.1.1 with verifiable rewards, which offer a natural starting point. There are substantial\nadvances in this direction, exemplified by the success of DeepSeek-R1, which demonstrated the\nscalability of RL through verifiable reward mechanisms. In contrast, Â§ 3.1.2 examines generative\nrewards, wherein the model is engaged to either verify or directly generate reward signals. However,\nboth verifiable and generative rewards are typically expressed as sparse numerical feedback. An\nimportant complementary dimension lies in the density of the reward signal. Â§ 3.1.3 accordingly\nexamines approaches that incorporate dense rewards. A further axis of categorization pertains to\nwhetherrewardsarecomputedfromexternalgroundtruthorinsteadestimateddirectlybythemodel.\nThisdistinctionmotivatesourdiscussionofunsupervisedrewardsinÂ§3.1.4. Buildinguponthesefour\ncategories, we then turn in Â§ 3.1.5 to reward shaping, where we analyze strategies for combining or\ntransforming diverse reward signals to facilitate learning.\n3.1.1. Verifiable Rewards\nTakeaways\nâ€¢ Rule-basedrewardsprovidescalableandreliabletrainingsignalsforRL,especiallyinmath\nand code tasks, by leveraging accuracy and format checks.\nâ€¢ Verifierâ€™s law highlights that tasks with clear and automatic verification enable efficient RL\noptimization, while subjective tasks remain challenging.\nRule-based Rewards. The reward serves as the training signal of RL, determining the optimization\ndirection [Guo et al., 2025a]. Recently, rule-based verifiable rewards have been predominantly em-\nployedtotrainLRMsinlarge-scaleRL.Suchrewardsenablethereliableenhancementofmathematical\n10\n\nTables:\n[\n  [\n    [\n      \"Takeaways\"\n    ],\n    [\n      \"â€¢ Rule-basedrewardsprovidescalableandreliabletrainingsignalsforRL,especiallyinmath\\nand code tasks, by leveraging accuracy and format checks.\\nâ€¢ Verifierâ€™s law highlights that tasks with clear and automatic verification enable efficient RL\\noptimization, while subjective tasks remain challenging.\"\n    ]\n  ]\n]",
    "file_path": "unknown_source",
    "create_time": 1765219263,
    "update_time": 1765219263,
    "_id": "doc-5b53f848c47ba7e516ea81d9d2659625"
  }
}