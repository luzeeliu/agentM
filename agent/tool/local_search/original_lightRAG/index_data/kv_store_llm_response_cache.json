{
  "default:extract:188e114c38aa253213404f4cfe59eee9": {
    "return": "entity<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>Content<|#|>A paper published on 2025-09-11 that surveys recent advances in Reinforcement Learning for reasoning with Large Language Models.\nentity<|#|>Reinforcement Learning (RL)<|#|>Concept<|#|>A foundational methodology that has achieved remarkable success in advancing Large Language Model capabilities, particularly in complex logical tasks, and transforms LLMs into LRMs.\nentity<|#|>Large Reasoning Models (LRMs)<|#|>Concept<|#|>Models transformed from Large Language Models using Reinforcement Learning, facing foundational challenges in scalability.\nentity<|#|>Large Language Models (LLMs)<|#|>Concept<|#|>Models whose capabilities have been advanced by Reinforcement Learning, especially in complex logical tasks, and can be transformed into Large Reasoning Models.\nentity<|#|>Artificial SuperIntelligence (ASI)<|#|>Concept<|#|>A future goal towards which the scalability of Reinforcement Learning is being explored.\nentity<|#|>DeepSeek-R1<|#|>Artifact<|#|>A model whose release influenced research applying Reinforcement Learning to LLMs and LRMs for reasoning abilities.\nentity<|#|>Foundational Components<|#|>Concept<|#|>A category of topics examined in the survey, including RL's Role, Model Prior, and Training Recipes.\nentity<|#|>Foundational Problems<|#|>Concept<|#|>A category of topics examined in the survey, including RL vs. SFT, Reward Type, and Reward Design.\nentity<|#|>Training Resources<|#|>Concept<|#|>A category of topics examined in the survey, including Static Corpus Training and Dynamic Environment.\nentity<|#|>Applications<|#|>Concept<|#|>A category of topics examined in the survey, including Policy Optimization, Math Code, RL Infrastructure & Frameworks, Agentic Tasks, Coding Tasks, Multimodal Robotics Tasks, and Medical Systems Tasks.\nentity<|#|>Tsinghua University<|#|>Organization<|#|>An academic institution affiliated with multiple authors of the survey.\nentity<|#|>Shanghai AI Laboratory<|#|>Organization<|#|>A research institution affiliated with multiple authors of the survey.\nentity<|#|>Shanghai Jiao Tong University<|#|>Organization<|#|>An academic institution affiliated with authors Yuchen Fan and Xuekai Zhu.\nentity<|#|>Peking University<|#|>Organization<|#|>An academic institution affiliated with author Yuchen Zhang.\nentity<|#|>University of Science and Technology of China<|#|>Organization<|#|>An academic institution affiliated with author Xiang Xu.\nentity<|#|>Harbin Institute of Technology<|#|>Organization<|#|>An academic institution affiliated with authors Pengfei Li, Junqi Gao, and Dong Li.\nentity<|#|>University of Washington<|#|>Organization<|#|>An academic institution affiliated with author Sihang Zeng.\nentity<|#|>Huazhong University of Science and Technology<|#|>Organization<|#|>An academic institution affiliated with author Zhiyuan Ma.\nentity<|#|>University College London<|#|>Organization<|#|>An academic institution affiliated with author Yu Fu.\nentity<|#|>Kaiyan Zhang<|#|>Person<|#|>A project lead and core contributor to the survey, affiliated with Tsinghua University.\nentity<|#|>Yuxin Zuo<|#|>Person<|#|>A project lead and core contributor to the survey, affiliated with Tsinghua University.\nentity<|#|>Bingxiang He<|#|>Person<|#|>A core contributor to the survey, affiliated with Tsinghua University.\nentity<|#|>Youbang Sun<|#|>Person<|#|>A core contributor to the survey, affiliated with Tsinghua University.\nentity<|#|>Runze Liu<|#|>Person<|#|>A core contributor to the survey, affiliated with Tsinghua University.\nentity<|#|>Che Jiang<|#|>Person<|#|>A core contributor to the survey, affiliated with Tsinghua University.\nentity<|#|>Yuchen Fan<|#|>Person<|#|>A core contributor to the survey, affiliated with Shanghai AI Laboratory and Shanghai Jiao Tong University.\nentity<|#|>Kai Tian<|#|>Person<|#|>A core contributor to the survey, affiliated with Tsinghua University.\nentity<|#|>Guoli Jia<|#|>Person<|#|>A core contributor to the survey, affiliated with Tsinghua University.\nentity<|#|>Pengfei Li<|#|>Person<|#|>A core contributor to the survey, affiliated with Shanghai AI Laboratory and Harbin Institute of Technology.\nentity<|#|>Yu Fu<|#|>Person<|#|>A core contributor to the survey, affiliated with University College London.\nentity<|#|>Xingtai Lv<|#|>Person<|#|>A core contributor to the survey, affiliated with Tsinghua University.\nentity<|#|>Yuchen Zhang<|#|>Person<|#|>A core contributor to the survey, affiliated with Shanghai AI Laboratory and Peking University.\nentity<|#|>Sihang Zeng<|#|>Person<|#|>A core contributor to the survey, affiliated with University of Washington.\nentity<|#|>Shang Qu<|#|>Person<|#|>A core contributor to the survey, affiliated with Tsinghua University and Shanghai AI Laboratory.\nentity<|#|>Haozhan Li<|#|>Person<|#|>A core contributor to the survey, affiliated with Tsinghua University.\nentity<|#|>Shijie Wang<|#|>Person<|#|>A core contributor to the survey, affiliated with Shanghai AI Laboratory.\nentity<|#|>Yuru Wang<|#|>Person<|#|>A core contributor to the survey, affiliated with Tsinghua University.\nentity<|#|>Xinwei Long<|#|>Person<|#|>An author of the survey, affiliated with Tsinghua University.\nentity<|#|>Fangfu Liu<|#|>Person<|#|>An author of the survey, affiliated with Tsinghua University.\nentity<|#|>Xiang Xu<|#|>Person<|#|>An author of the survey, affiliated with University of Science and Technology of China.\nentity<|#|>Jiaze Ma<|#|>Person<|#|>An author of the survey, affiliated with Tsinghua University.\nentity<|#|>Xuekai Zhu<|#|>Person<|#|>An author of the survey, affiliated with Shanghai Jiao Tong University.\nentity<|#|>Ermo Hua<|#|>Person<|#|>An author of the survey, affiliated with Tsinghua University and Shanghai AI Laboratory.\nentity<|#|>Yihao Liu<|#|>Person<|#|>An author of the survey, affiliated with Tsinghua University and Shanghai AI Laboratory.\nentity<|#|>Zonglin Li<|#|>Person<|#|>An author of the survey, affiliated with Shanghai AI Laboratory.\nentity<|#|>Huayu Chen<|#|>Person<|#|>An author of the survey, affiliated with Tsinghua University.\nentity<|#|>Xiaoye Qu<|#|>Person<|#|>An author of the survey, affiliated with Shanghai AI Laboratory.\nentity<|#|>Yafu Li<|#|>Person<|#|>An author of the survey, affiliated with Shanghai AI Laboratory.\nentity<|#|>Weize Chen<|#|>Person<|#|>An author of the survey, affiliated with Tsinghua University.\nentity<|#|>Zhenzhao Yuan<|#|>Person<|#|>An author of the survey, affiliated with Tsinghua University.\nentity<|#|>Junqi Gao<|#|>Person<|#|>An author of the survey, affiliated with Harbin Institute of Technology.\nentity<|#|>Dong Li<|#|>Person<|#|>An author of the survey, affiliated with Harbin Institute of Technology.\nentity<|#|>Zhiyuan Ma<|#|>Person<|#|>An author of the survey, affiliated with Huazhong University of Science and Technology.\nentity<|#|>Ganqu Cui<|#|>Person<|#|>An author of the survey, affiliated with Shanghai AI Laboratory.\nentity<|#|>Zhiyuan Liu<|#|>Person<|#|>An author of the survey, affiliated with Tsinghua University.\nentity<|#|>Biqing Qi<|#|>Person<|#|>A corresponding author of the survey, affiliated with Shanghai AI Laboratory.\nentity<|#|>Ning Ding<|#|>Person<|#|>A corresponding author of the survey, affiliated with Tsinghua University and Shanghai AI Laboratory.\nentity<|#|>Bowen Zhou<|#|>Person<|#|>A corresponding author of the survey, affiliated with Tsinghua University and Shanghai AI Laboratory.\nentity<|#|>Tsinghua C3I<|#|>Organization<|#|>An organization mentioned in the email signature.\nentity<|#|>Awesome-RL-for-LRMs<|#|>Concept<|#|>A project or repository related to Reinforcement Learning for Large Reasoning Models.\nentity<|#|>Abstract<|#|>Content<|#|>A section of the paper that surveys recent advances in Reinforcement Learning for reasoning with Large Language Models.\nentity<|#|>Figure 1<|#|>Content<|#|>Provides an overview of the survey, introducing foundational components, open problems, training resources, and applications.\nentity<|#|>RL's Role<|#|>Concept<|#|>A foundational component discussed in the survey.\nentity<|#|>Model Prior<|#|>Concept<|#|>A foundational component discussed in the survey.\nentity<|#|>Training Recipes<|#|>Concept<|#|>A foundational component discussed in the survey.\nentity<|#|>RL vs. SFT<|#|>Concept<|#|>A foundational problem discussed in the survey, comparing Reinforcement Learning with Supervised Fine-Tuning.\nentity<|#|>Reward Type<|#|>Concept<|#|>A foundational problem discussed in the survey, concerning different categories of rewards.\nentity<|#|>Reward Design<|#|>Concept<|#|>A foundational problem discussed in the survey, concerning the creation of effective rewards.\nentity<|#|>Sharpening Discovery<|#|>Concept<|#|>A concept related to reward design.\nentity<|#|>Weak Strong<|#|>Concept<|#|>A concept related to reward design, likely referring to weak vs. strong supervision or signals.\nentity<|#|>Tricks Traps<|#|>Concept<|#|>A concept related to training recipes, referring to effective techniques and common pitfalls.\nentity<|#|>Generalize Memorize<|#|>Concept<|#|>A concept related to training recipes, referring to the balance between generalization and memorization.\nentity<|#|>Process Outcome<|#|>Concept<|#|>A concept related to reward type, referring to process-based vs. outcome-based rewards.\nentity<|#|>Verifiable Generative Rewards<|#|>Concept<|#|>A type of reward.\nentity<|#|>Unsupervised Rewards<|#|>Concept<|#|>A type of reward.\nentity<|#|>Shaping Rewards<|#|>Concept<|#|>A type of reward.\nentity<|#|>Static Corpus Training<|#|>Concept<|#|>A type of training resource.\nentity<|#|>Dynamic Environment<|#|>Concept<|#|>A type of training resource.\nentity<|#|>Policy Optimization<|#|>Method<|#|>An application area of RL for LRMs.\nentity<|#|>Math Code<|#|>Concept<|#|>An application area of RL for LRMs, encompassing mathematics and coding tasks.\nentity<|#|>RL Infrastructure & Frameworks<|#|>Concept<|#|>An application area of RL for LRMs.\nentity<|#|>Policy Critic-Based Algorithms<|#|>Method<|#|>A type of algorithm under Policy Optimization.\nentity<|#|>Policy Gradient Algorithms<|#|>Method<|#|>A type of algorithm under Policy Critic-Based Algorithms.\nentity<|#|>Critic-Free Algorithms<|#|>Method<|#|>A type of algorithm under Policy Optimization.\nentity<|#|>Off-Policy Regularization<|#|>Method<|#|>A concept related to Critic-Free Algorithms.\nentity<|#|>Optimization Objectives<|#|>Concept<|#|>A concept related to Critic-Free Algorithms.\nentity<|#|>Sampling Strategy<|#|>Method<|#|>A concept related to Critic-Free Algorithms.\nentity<|#|>Dynamic Sampling<|#|>Method<|#|>A type of sampling strategy.\nentity<|#|>Multi-Agent Sampling<|#|>Method<|#|>A type of sampling strategy.\nentity<|#|>Sampling Hyper-Parameters<|#|>Concept<|#|>A concept related to sampling strategy.\nentity<|#|>Compute<|#|>Concept<|#|>A resource related to sampling strategy.\nentity<|#|>STEM Agent<|#|>Concept<|#|>An application under Math Code.\nentity<|#|>Mixture<|#|>Concept<|#|>A category of applications under Math Code, including specific frameworks.\nentity<|#|>Game<|#|>Concept<|#|>An application under Math Code.\nentity<|#|>Model Ensemble<|#|>Concept<|#|>An application under Math Code.\nentity<|#|>OpenRLHF<|#|>Artifact<|#|>An example of an RL infrastructure and framework.\nentity<|#|>veRL<|#|>Artifact<|#|>An example of an RL infrastructure and framework.\nentity<|#|>AReaL<|#|>Artifact<|#|>An example of an RL infrastructure and framework.\nentity<|#|>slime<|#|>Artifact<|#|>An example of an RL infrastructure and framework.\nentity<|#|>TRL<|#|>Artifact<|#|>An example of an RL infrastructure and framework.\nentity<|#|>Agentic Tasks<|#|>Concept<|#|>An application area of RL for LRMs.\nentity<|#|>Coding Tasks<|#|>Concept<|#|>An application area of RL for LRMs.\nentity<|#|>Multimodal Robotics Tasks<|#|>Concept<|#|>An application area of RL for LRMs.\nentity<|#|>Medical Systems Tasks<|#|>Concept<|#|>An application area of RL for LRMs.\nentity<|#|>Agent<|#|>Concept<|#|>An entity that interacts with an environment and takes actions, central to the survey's focus on large-scale interactions.\nentity<|#|>Environment<|#|>Concept<|#|>An entity with which an agent interacts, providing rewards, central to the survey's focus on large-scale interactions.\nentity<|#|>Action<|#|>Concept<|#|>A step taken by an agent within an environment, part of the interaction process.\nentity<|#|>Reward<|#|>Concept<|#|>A signal received by an agent from the environment, part of the interaction process.\nentity<|#|>Interaction (Episode)<|#|>Concept<|#|>A process involving an agent and environment over time, central to the survey's focus.\nentity<|#|>Evolution (Training Step)<|#|>Concept<|#|>A process of development or training, central to the survey's focus.\nentity<|#|>Computational Resources<|#|>Concept<|#|>A foundational challenge for scaling RL for LRMs.\nentity<|#|>Algorithm Design<|#|>Concept<|#|>A foundational challenge for scaling RL for LRMs.\nentity<|#|>Training Data<|#|>Concept<|#|>A foundational challenge for scaling RL for LRMs.\nentity<|#|>Infrastructure<|#|>Concept<|#|>A foundational challenge for scaling RL for LRMs.\nentity<|#|>Mathematics<|#|>Concept<|#|>A complex logical task that Reinforcement Learning addresses.\nentity<|#|>Coding<|#|>Concept<|#|>A complex logical task that Reinforcement Learning addresses.\nentity<|#|>Section 3<|#|>Content<|#|>A section of the survey titled \"Foundational Components\".\nentity<|#|>Section 4<|#|>Content<|#|>A section of the survey titled \"Foundational Problems\".\nentity<|#|>Section 5<|#|>Content<|#|>A section of the survey titled \"Training Resources\".\nentity<|#|>Section 6<|#|>Content<|#|>A section of the survey titled \"Applications\".\nentity<|#|>Supervised Fine-Tuning (SFT)<|#|>Method<|#|>A method compared with Reinforcement Learning as a foundational problem.\nrelation<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>Reinforcement Learning (RL)<|#|>survey, focus<|#|>The survey paper focuses on recent advances in Reinforcement Learning.\nrelation<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>Large Reasoning Models (LRMs)<|#|>survey, focus<|#|>The survey paper focuses on Reinforcement Learning for Large Reasoning Models.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Large Language Models (LLMs)<|#|>advancement, capability enhancement<|#|>Reinforcement Learning has achieved remarkable success in advancing the frontier of Large Language Model capabilities.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Mathematics<|#|>application, problem-solving<|#|>Reinforcement Learning addresses complex logical tasks such as mathematics.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Coding<|#|>application, problem-solving<|#|>Reinforcement Learning addresses complex logical tasks such as coding.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Large Reasoning Models (LRMs)<|#|>transformation, methodology<|#|>Reinforcement Learning has emerged as a foundational methodology for transforming LLMs into LRMs.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Computational Resources<|#|>challenge, scalability<|#|>Scaling Reinforcement Learning for LRMs faces foundational challenges in computational resources.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Algorithm Design<|#|>challenge, scalability<|#|>Scaling Reinforcement Learning for LRMs faces foundational challenges in algorithm design.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Training Data<|#|>challenge, scalability<|#|>Scaling Reinforcement Learning for LRMs faces foundational challenges in training data.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Infrastructure<|#|>challenge, scalability<|#|>Scaling Reinforcement Learning for LRMs faces foundational challenges in infrastructure.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Artificial SuperIntelligence (ASI)<|#|>goal, scalability<|#|>Strategies are explored to enhance the scalability of Reinforcement Learning toward Artificial SuperIntelligence.\nrelation<|#|>Reinforcement Learning (RL)<|#|>DeepSeek-R1<|#|>research influence, application<|#|>Research applying Reinforcement Learning to LLMs and LRMs for reasoning abilities is examined, especially since the release of DeepSeek-R1.\nrelation<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>Foundational Components<|#|>content, examination<|#|>The survey examines foundational components.\nrelation<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>Foundational Problems<|#|>content, examination<|#|>The survey examines core problems, also referred to as foundational problems.\nrelation<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>Training Resources<|#|>content, examination<|#|>The survey examines training resources.\nrelation<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>Applications<|#|>content, examination<|#|>The survey examines downstream applications.\nrelation<|#|>Abstract<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>summary, content<|#|>The abstract summarizes the paper's content regarding Reinforcement Learning for Large Language Models.\nrelation<|#|>Figure 1<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>overview, structure<|#|>Figure 1 provides an overview of the survey.\nrelation<|#|>Section 3<|#|>Foundational Components<|#|>covers, content<|#|>Section 3 covers Foundational Components.\nrelation<|#|>Section 4<|#|>Foundational Problems<|#|>covers, content<|#|>Section 4 covers Foundational Problems.\nrelation<|#|>Section 5<|#|>Training Resources<|#|>covers, content<|#|>Section 5 covers Training Resources.\nrelation<|#|>Section 6<|#|>Applications<|#|>covers, content<|#|>Section 6 covers Applications.\nrelation<|#|>Foundational Components<|#|>RL's Role<|#|>includes, topic<|#|>Foundational Components include RL's Role.\nrelation<|#|>Foundational Components<|#|>Model Prior<|#|>includes, topic<|#|>Foundational Components include Model Prior.\nrelation<|#|>Foundational Components<|#|>Training Recipes<|#|>includes, topic<|#|>Foundational Components include Training Recipes.\nrelation<|#|>Foundational Problems<|#|>RL vs. SFT<|#|>includes, topic<|#|>Foundational Problems include RL vs. SFT.\nrelation<|#|>Foundational Problems<|#|>Reward Type<|#|>includes, topic<|#|>Foundational Problems include Reward Type.\nrelation<|#|>Foundational Problems<|#|>Reward Design<|#|>includes, topic<|#|>Foundational Problems include Reward Design.\nrelation<|#|>Training Resources<|#|>Static Corpus Training<|#|>includes, topic<|#|>Training Resources include Static Corpus Training.\nrelation<|#|>Training Resources<|#|>Dynamic Environment<|#|>includes, topic<|#|>Training Resources include Dynamic Environment.\nrelation<#|>Applications<|#|>Policy Optimization<|#|>includes, topic<|#|>Applications include Policy Optimization.\nrelation<|#|>Applications<|#|>Math Code<|#|>includes, topic<|#|>Applications include Math Code.\nrelation<|#|>Applications<|#|>RL Infrastructure & Frameworks<|#|>includes, topic<|#|>Applications include RL Infrastructure & Frameworks.\nrelation<|#|>Applications<|#|>Agentic Tasks<|#|>includes, topic<|#|>Applications include Agentic Tasks.\nrelation<|#|>Applications<|#|>Coding Tasks<|#|>includes, topic<|#|>Applications include Coding Tasks.\nrelation<|#|>Applications<|#|>Multimodal Robotics Tasks<|#|>includes, topic<|#|>Applications include Multimodal Robotics Tasks.\nrelation<|#|>Applications<|#|>Medical Systems Tasks<|#|>includes, topic<|#|>Applications include Medical Systems Tasks.\nrelation<|#|>Policy Optimization<|#|>Policy Critic-Based Algorithms<|#|>comprises, method<|#|>Policy Optimization comprises Policy Critic-Based Algorithms.\nrelation<|#|>Policy Optimization<|#|>Critic-Free Algorithms<|#|>comprises, method<|#|>Policy Optimization comprises Critic-Free Algorithms.\nrelation<|#|>Policy Critic-Based Algorithms<|#|>Policy Gradient Algorithms<|#|>includes, method<|#|>Policy Critic-Based Algorithms include Policy Gradient Algorithms.\nrelation<|#|>Critic-Free Algorithms<|#|>Off-Policy Regularization<|#|>includes, method<|#|>Critic-Free Algorithms include Off-Policy Regularization.\nrelation<|#|>Critic-Free Algorithms<|#|>Optimization Objectives<|#|>includes, concept<|#|>Critic-Free Algorithms include Optimization Objectives.\nrelation<|#|>Critic-Free Algorithms<|#|>Sampling Strategy<|#|>includes, method<|#|>Critic-Free Algorithms include Sampling Strategy.\nrelation<|#|>Sampling Strategy<|#|>Dynamic Sampling<|#|>includes, method<|#|>Sampling Strategy includes Dynamic Sampling.\nrelation<|#|>Sampling Strategy<|#|>Multi-Agent Sampling<|#|>includes, method<|#|>Sampling Strategy includes Multi-Agent Sampling.\nrelation<|#|>Sampling Strategy<|#|>Sampling Hyper-Parameters<|#|>includes, concept<|#|>Sampling Strategy includes Sampling Hyper-Parameters.\nrelation<|#|>Sampling Strategy<|#|>Compute<|#|>includes, resource<|#|>Sampling Strategy includes Compute.\nrelation<|#|>Math Code<|#|>STEM Agent<|#|>includes, application<|#|>Math Code applications include STEM Agent.\nrelation<|#|>Math Code<|#|>Mixture<|#|>includes, application<|#|>Math Code applications include Mixture.\nrelation<|#|>Math Code<|#|>Game<|#|>includes, application<|#|>Math Code applications include Game.\nrelation<|#|>Math Code<|#|>Model Ensemble<|#|>includes, application<|#|>Math Code applications include Model Ensemble.\nrelation<|#|>RL Infrastructure & Frameworks<|#|>OpenRLHF<|#|>includes, example<|#|>RL Infrastructure & Frameworks include OpenRLHF.\nrelation<|#|>RL Infrastructure & Frameworks<|#|>veRL<|#|>includes, example<|#|>RL Infrastructure & Frameworks include veRL.\nrelation<|#|>RL Infrastructure & Frameworks<|#|>AReaL<|#|>includes, example<|#|>RL Infrastructure & Frameworks include AReaL.\nrelation<|#|>RL Infrastructure & Frameworks<|#|>slime<|#|>includes, example<|#|>RL Infrastructure & Frameworks include slime.\nrelation<|#|>RL Infrastructure & Frameworks<|#|>TRL<|#|>includes, example<|#|>RL Infrastructure & Frameworks include TRL.\nrelation<|#|>Agent<|#|>Environment<|#|>interaction, relationship<|#|>An Agent interacts with an Environment.\nrelation<|#|>Agent<|#|>Action<|#|>performs, relationship<|#|>An Agent performs an Action.\nrelation<|#|>Environment<|#|>Reward<|#|>provides, relationship<|#|>An Environment provides a Reward.\nrelation<|#|>Agent<|#|>Reward<|#|>receives, relationship<|#|>An Agent receives a Reward.\nrelation<|#|>Interaction (Episode)<|#|>Agent<|#|>involves, component<|#|>Interaction (Episode) involves an Agent.\nrelation<|#|>Interaction (Episode)<|#|>Environment<|#|>involves, component<|#|>Interaction (Episode) involves an Environment.\nrelation<|#|>Interaction (Episode)<|#|>Action<|#|>involves, component<|#|>Interaction (Episode) involves an Action.\nrelation<|#|>Interaction (Episode)<|#|>Reward<|#|>involves, component<|#|>Interaction (Episode) involves a Reward.\nrelation<|#|>Reward Design<|#|>Sharpening Discovery<|#|>related to, concept<|#|>Sharpening Discovery is related to Reward Design.\nrelation<|#|>Reward Design<|#|>Weak Strong<|#|>related to, concept<|#|>Weak Strong is related to Reward Design.\nrelation<|#|>Training Recipes<|#|>Tricks Traps<|#|>related to, concept<|#|>Tricks Traps are related to Training Recipes.\nrelation<|#|>Training Recipes<|#|>Generalize Memorize<|#|>related to, concept<|#|>Generalize Memorize is related to Training Recipes.\nrelation<|#|>Reward Type<|#|>Process Outcome<|#|>related to, concept<|#|>Process Outcome is related to Reward Type.\nrelation<|#|>Reward Type<|#|>Verifiable Generative Rewards<|#|>includes, type<|#|>Reward Type includes Verifiable Generative Rewards.\nrelation<|#|>Reward Type<|#|>Unsupervised Rewards<|#|>includes, type<|#|>Reward Type includes Unsupervised Rewards.\nrelation<|#|>Reward Type<|#|>Shaping Rewards<|#|>includes, type<|#|>Reward Type includes Shaping Rewards.\nrelation<|#|>Kaiyan Zhang<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Kaiyan Zhang is a project lead and core contributor to the survey.\nrelation<|#|>Kaiyan Zhang<|#|>Tsinghua University<|#|>affiliation<|#|>Kaiyan Zhang is affiliated with Tsinghua University.\nrelation<|#|>Yuxin Zuo<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Yuxin Zuo is a project lead and core contributor to the survey.\nrelation<|#|>Yuxin Zuo<|#|>Tsinghua University<|#|>affiliation<|#|>Yuxin Zuo is affiliated with Tsinghua University.\nrelation<|#|>Bingxiang He<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Bingxiang He is a core contributor to the survey.\nrelation<|#|>Bingxiang He<|#|>Tsinghua University<|#|>affiliation<|#|>Bingxiang He is affiliated with Tsinghua University.\nrelation<|#|>Youbang Sun<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Youbang Sun is a core contributor to the survey.\nrelation<|#|>Youbang Sun<|#|>Tsinghua University<|#|>affiliation<|#|>Youbang Sun is affiliated with Tsinghua University.\nrelation<|#|>Runze Liu<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Runze Liu is a core contributor to the survey.\nrelation<|#|>Runze Liu<|#|>Tsinghua University<|#|>affiliation<|#|>Runze Liu is affiliated with Tsinghua University.\nrelation<|#|>Che Jiang<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Che Jiang is a core contributor to the survey.\nrelation<#|>Che Jiang<|#|>Tsinghua University<|#|>affiliation<|#|>Che Jiang is affiliated with Tsinghua University.\nrelation<|#|>Yuchen Fan<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Yuchen Fan is a core contributor to the survey.\nrelation<|#|>Yuchen Fan<|#|>Shanghai AI Laboratory<|#|>affiliation<|#|>Yuchen Fan is affiliated with Shanghai AI Laboratory.\nrelation<|#|>Yuchen Fan<|#|>Shanghai Jiao Tong University<|#|>affiliation<|#|>Yuchen Fan is affiliated with Shanghai Jiao Tong University.\nrelation<|#|>Kai Tian<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Kai Tian is a core contributor to the survey.\nrelation<|#|>Kai Tian<|#|>Tsinghua University<|#|>affiliation<|#|>Kai Tian is affiliated with Tsinghua University.\nrelation<|#|>Guoli Jia<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Guoli Jia is a core contributor to the survey.\nrelation<|#|>Guoli Jia<|#|>Tsinghua University<|#|>affiliation<|#|>Guoli Jia is affiliated with Tsinghua University.\nrelation<|#|>Pengfei Li<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Pengfei Li is a core contributor to the survey.\nrelation<|#|>Pengfei Li<|#|>Shanghai AI Laboratory<|#|>affiliation<|#|>Pengfei Li is affiliated with Shanghai AI Laboratory.\nrelation<|#|>Pengfei Li<|#|>Harbin Institute of Technology<|#|>affiliation<|#|>Pengfei Li is affiliated with Harbin Institute of Technology.\nrelation<|#|>Yu Fu<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Yu Fu is a core contributor to the survey.\nrelation<|#|>Yu Fu<|#|>University College London<|#|>affiliation<|#|>Yu Fu is affiliated with University College London.\nrelation<|#|>Xingtai Lv<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Xingtai Lv is a core contributor to the survey.\nrelation<|#|>Xingtai Lv<|#|>Tsinghua University<|#|>affiliation<|#|>Xingtai Lv is affiliated with Tsinghua University.\nrelation<|#|>Yuchen Zhang<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Yuchen Zhang is a core contributor to the survey.\nrelation<|#|>Yuchen Zhang<|#|>Shanghai AI Laboratory<|#|>affiliation<|#|>Yuchen Zhang is affiliated with Shanghai AI Laboratory.\nrelation<|#|>Yuchen Zhang<|#|>Peking University<|#|>affiliation<|#|>Yuchen Zhang is affiliated with Peking University.\nrelation<|#|>Sihang Zeng<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Sihang Zeng is a core contributor to the survey.\nrelation<|#|>Sihang Zeng<|#|>University of Washington<|#|>affiliation<|#|>Sihang Zeng is affiliated with University of Washington.\nrelation<|#|>Shang Qu<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Shang Qu is a core contributor to the survey.\nrelation<|#|>Shang Qu<|#|>Tsinghua University<|#|>affiliation<|#|>Shang Qu is affiliated with Tsinghua University.\nrelation<|#|>Shang Qu<|#|>Shanghai AI Laboratory<|#|>affiliation<|#|>Shang Qu is affiliated with Shanghai AI Laboratory.\nrelation<|#|>Haozhan Li<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Haozhan Li is a core contributor to the survey.\nrelation<|#|>Haozhan Li<|#|>Tsinghua University<|#|>affiliation<|#|>Haozhan Li is affiliated with Tsinghua University.\nrelation<|#|>Shijie Wang<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Shijie Wang is a core contributor to the survey.\nrelation<|#|>Shijie Wang<|#|>Shanghai AI Laboratory<|#|>affiliation<|#|>Shijie Wang is affiliated with Shanghai AI Laboratory.\nrelation<|#|>Yuru Wang<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Yuru Wang is a core contributor to the survey.\nrelation<|#|>Yuru Wang<|#|>Tsinghua University<|#|>affiliation<|#|>Yuru Wang is affiliated with Tsinghua University.\nrelation<|#|>Xinwei Long<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Xinwei Long is an author of the survey.\nrelation<|#|>Xinwei Long<|#|>Tsinghua University<|#|>affiliation<|#|>Xinwei Long is affiliated with Tsinghua University.\nrelation<|#|>Fangfu Liu<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Fangfu Liu is an author of the survey.\nrelation<|#|>Fangfu Liu<|#|>Tsinghua University<|#|>affiliation<|#|>Fangfu Liu is affiliated with Tsinghua University.\nrelation<|#|>Xiang Xu<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Xiang Xu is an author of the survey.\nrelation<|#|>Xiang Xu<|#|>University of Science and Technology of China<|#|>affiliation<|#|>Xiang Xu is affiliated with University of Science and Technology of China.\nrelation<|#|>Jiaze Ma<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Jiaze Ma is an author of the survey.\nrelation<|#|>Jiaze Ma<|#|>Tsinghua University<|#|>affiliation<|#|>Jiaze Ma is affiliated with Tsinghua University.\nrelation<|#|>Xuekai Zhu<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Xuekai Zhu is an author of the survey.\nrelation<|#|>Xuekai Zhu<|#|>Shanghai Jiao Tong University<|#|>affiliation<|#|>Xuekai Zhu is affiliated with Shanghai Jiao Tong University.\nrelation<|#|>Ermo Hua<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Ermo Hua is an author of the survey.\nrelation<|#|>Ermo Hua<|#|>Tsinghua University<|#|>affiliation<|#|>Ermo Hua is affiliated with Tsinghua University.\nrelation<|#|>Ermo Hua<|#|>Shanghai AI Laboratory<|#|>affiliation<|#|>Ermo Hua is affiliated with Shanghai AI Laboratory.\nrelation<|#|>Yihao Liu<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Yihao Liu is an author of the survey.\nrelation<|#|>Yihao Liu<|#|>Tsinghua University<|#|>affiliation<|#|>Yihao Liu is affiliated with Tsinghua University.\nrelation<|#|>Yihao Liu<|#|>Shanghai AI Laboratory<|#|>affiliation<|#|>Yihao Liu is affiliated with Shanghai AI Laboratory.\nrelation<|#|>Zonglin Li<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Zonglin Li is an author of the survey.\nrelation<|#|>Zonglin Li<|#|>Shanghai AI Laboratory<|#|>affiliation<|#|>Zonglin Li is affiliated with Shanghai AI Laboratory.\nrelation<|#|>Huayu Chen<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Huayu Chen is an author of the survey.\nrelation<|#|>Huayu Chen<|#|>Tsinghua University<|#|>affiliation<|#|>Huayu Chen is affiliated with Tsinghua University.\nrelation<|#|>Xiaoye Qu<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Xiaoye Qu is an author of the survey.\nrelation<|#|>Xiaoye Qu<|#|>Shanghai AI Laboratory<|#|>affiliation<|#|>Xiaoye Qu is affiliated with Shanghai AI Laboratory.\nrelation<|#|>Yafu Li<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Yafu Li is an author of the survey.\nrelation<|#|>Yafu Li<|#|>Shanghai AI Laboratory<|#|>affiliation<|#|>Yafu Li is affiliated with Shanghai AI Laboratory.\nrelation<|#|>Weize Chen<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Weize Chen is an author of the survey.\nrelation<|#|>Weize Chen<|#|>Tsinghua University<|#|>affiliation<|#|>Weize Chen is affiliated with Tsinghua University.\nrelation<|#|>Zhenzhao Yuan<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Zhenzhao Yuan is an author of the survey.\nrelation<|#|>Zhenzhao Yuan<|#|>Tsinghua University<|#|>affiliation<|#|>Zhenzhao Yuan is affiliated with Tsinghua University.\nrelation<|#|>Junqi Gao<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Junqi Gao is an author of the survey.\nrelation<|#|>Junqi Gao<|#|>Harbin Institute of Technology<|#|>affiliation<|#|>Junqi Gao is affiliated with Harbin Institute of Technology.\nrelation<|#|>Dong Li<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Dong Li is an author of the survey.\nrelation<|#|>Dong Li<|#|>Harbin Institute of Technology<|#|>affiliation<|#|>Dong Li is affiliated with Harbin Institute of Technology.\nrelation<|#|>Zhiyuan Ma<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Zhiyuan Ma is an author of the survey.\nrelation<|#|>Zhiyuan Ma<|#|>Huazhong University of Science and Technology<|#|>affiliation<|#|>Zhiyuan Ma is affiliated with Huazhong University of Science and Technology.\nrelation<|#|>Ganqu Cui<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Ganqu Cui is an author of the survey.\nrelation<|#|>Ganqu Cui<|#|>Shanghai AI Laboratory<|#|>affiliation<|#|>Ganqu Cui is affiliated with Shanghai AI Laboratory.\nrelation<|#|>Zhiyuan Liu<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Zhiyuan Liu is an author of the survey.\nrelation<|#|>Zhiyuan Liu<|#|>Tsinghua University<|#|>affiliation<|#|>Zhiyuan Liu is affiliated with Tsinghua University.\nrelation<|#|>Biqing Qi<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Biqing Qi is a corresponding author of the survey.\nrelation<|#|>Biqing Qi<|#|>Shanghai AI Laboratory<|#|>affiliation<|#|>Biqing Qi is affiliated with Shanghai AI Laboratory.\nrelation<|#|>Ning Ding<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Ning Ding is a corresponding author of the survey.\nrelation<|#|>Ning Ding<|#|>Tsinghua University<|#|>affiliation<|#|>Ning Ding is affiliated with Tsinghua University.\nrelation<|#|>Ning Ding<|#|>Shanghai AI Laboratory<|#|>affiliation<|#|>Ning Ding is affiliated with Shanghai AI Laboratory.\nrelation<|#|>Bowen Zhou<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Bowen Zhou is a corresponding author of the survey.\nrelation<|#|>Bowen Zhou<|#|>Tsinghua University<|#|>affiliation<|#|>Bowen Zhou is affiliated with Tsinghua University.\nrelation<|#|>Bowen Zhou<|#|>Shanghai AI Laboratory<|#|>affiliation<|#|>Bowen Zhou is affiliated with Shanghai AI Laboratory.\n<|COMPLETE|>",
    "cache_type": "extract",
    "chunk_id": "chunk-32bcdbba2d266d061d85f94faf74b18a",
    "original_prompt": "---Task---\nExtract entities and relationships from the input text to be processed.\n\n---Instructions---\n1.  **Strict Adherence to Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system prompt.\n2.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\n3.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant entities and relationships have been extracted and presented.\n4.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\n\n<Output>\n---Role---\nYou are a Knowledge Graph Specialist responsible for extracting entities and relationships from the input text.\n\n---Instructions---\n1.  **Entity Extraction & Output:**\n    *   **Identification:** Identify clearly defined and meaningful entities in the input text.\n    *   **Entity Details:** For each identified entity, extract the following information:\n        *   `entity_name`: The name of the entity. If the entity name is case-insensitive, capitalize the first letter of each significant word (title case). Ensure **consistent naming** across the entire extraction process.\n        *   `entity_type`: Categorize the entity using one of the following types: `Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject`. If none of the provided entity types apply, do not add new entity type and classify it as `Other`.\n        *   `entity_description`: Provide a concise yet comprehensive description of the entity's attributes and activities, based *solely* on the information present in the input text.\n    *   **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n        *   Format: `entity<|#|>entity_name<|#|>entity_type<|#|>entity_description`\n\n2.  **Relationship Extraction & Output:**\n    *   **Identification:** Identify direct, clearly stated, and meaningful relationships between previously extracted entities.\n    *   **N-ary Relationship Decomposition:** If a single statement describes a relationship involving more than two entities (an N-ary relationship), decompose it into multiple binary (two-entity) relationship pairs for separate description.\n        *   **Example:** For \"Alice, Bob, and Carol collaborated on Project X,\" extract binary relationships such as \"Alice collaborated with Project X,\" \"Bob collaborated with Project X,\" and \"Carol collaborated with Project X,\" or \"Alice collaborated with Bob,\" based on the most reasonable binary interpretations.\n    *   **Relationship Details:** For each binary relationship, extract the following fields:\n        *   `source_entity`: The name of the source entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `target_entity`: The name of the target entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `relationship_keywords`: One or more high-level keywords summarizing the overarching nature, concepts, or themes of the relationship. Multiple keywords within this field must be separated by a comma `,`. **DO NOT use `<|#|>` for separating multiple keywords within this field.**\n        *   `relationship_description`: A concise explanation of the nature of the relationship between the source and target entities, providing a clear rationale for their connection.\n    *   **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n        *   Format: `relation<|#|>source_entity<|#|>target_entity<|#|>relationship_keywords<|#|>relationship_description`\n\n3.  **Delimiter Usage Protocol:**\n    *   The `<|#|>` is a complete, atomic marker and **must not be filled with content**. It serves strictly as a field separator.\n    *   **Incorrect Example:** `entity<|#|>Tokyo<|location|>Tokyo is the capital of Japan.`\n    *   **Correct Example:** `entity<|#|>Tokyo<|#|>location<|#|>Tokyo is the capital of Japan.`\n\n4.  **Relationship Direction & Duplication:**\n    *   Treat all relationships as **undirected** unless explicitly stated otherwise. Swapping the source and target entities for an undirected relationship does not constitute a new relationship.\n    *   Avoid outputting duplicate relationships.\n\n5.  **Output Order & Prioritization:**\n    *   Output all extracted entities first, followed by all extracted relationships.\n    *   Within the list of relationships, prioritize and output those relationships that are **most significant** to the core meaning of the input text first.\n\n6.  **Context & Objectivity:**\n    *   Ensure all entity names and descriptions are written in the **third person**.\n    *   Explicitly name the subject or object; **avoid using pronouns** such as `this article`, `this paper`, `our company`, `I`, `you`, and `he/she`.\n\n7.  **Language & Proper Nouns:**\n    *   The entire output (entity names, keywords, and descriptions) must be written in `English`.\n    *   Proper nouns (e.g., personal names, place names, organization names) should be retained in their original language if a proper, widely accepted translation is not available or would cause ambiguity.\n\n8.  **Completion Signal:** Output the literal string `<|COMPLETE|>` only after all entities and relationships, following all criteria, have been completely extracted and outputted.\n\n---Examples---\n<Input Text>\n```\nwhile Alex clenched his jaw, the buzz of frustration dull against the backdrop of Taylor's authoritarian certainty. It was this competitive undercurrent that kept him alert, the sense that his and Jordan's shared commitment to discovery was an unspoken rebellion against Cruz's narrowing vision of control and order.\n\nThen Taylor did something unexpected. They paused beside Jordan and, for a moment, observed the device with something akin to reverence. \"If this tech can be understood...\" Taylor said, their voice quieter, \"It could change the game for us. For all of us.\"\n\nThe underlying dismissal earlier seemed to falter, replaced by a glimpse of reluctant respect for the gravity of what lay in their hands. Jordan looked up, and for a fleeting heartbeat, their eyes locked with Taylor's, a wordless clash of wills softening into an uneasy truce.\n\nIt was a small transformation, barely perceptible, but one that Alex noted with an inward nod. They had all been brought here by different paths\n```\n\n<Output>\nentity<|#|>Alex<|#|>person<|#|>Alex is a character who experiences frustration and is observant of the dynamics among other characters.\nentity<|#|>Taylor<|#|>person<|#|>Taylor is portrayed with authoritarian certainty and shows a moment of reverence towards a device, indicating a change in perspective.\nentity<|#|>Jordan<|#|>person<|#|>Jordan shares a commitment to discovery and has a significant interaction with Taylor regarding a device.\nentity<|#|>Cruz<|#|>person<|#|>Cruz is associated with a vision of control and order, influencing the dynamics among other characters.\nentity<|#|>The Device<|#|>equipment<|#|>The Device is central to the story, with potential game-changing implications, and is revered by Taylor.\nrelation<|#|>Alex<|#|>Taylor<|#|>power dynamics, observation<|#|>Alex observes Taylor's authoritarian behavior and notes changes in Taylor's attitude toward the device.\nrelation<|#|>Alex<|#|>Jordan<|#|>shared goals, rebellion<|#|>Alex and Jordan share a commitment to discovery, which contrasts with Cruz's vision.)\nrelation<|#|>Taylor<|#|>Jordan<|#|>conflict resolution, mutual respect<|#|>Taylor and Jordan interact directly regarding the device, leading to a moment of mutual respect and an uneasy truce.\nrelation<|#|>Jordan<|#|>Cruz<|#|>ideological conflict, rebellion<|#|>Jordan's commitment to discovery is in rebellion against Cruz's vision of control and order.\nrelation<|#|>Taylor<|#|>The Device<|#|>reverence, technological significance<|#|>Taylor shows reverence towards the device, indicating its importance and potential impact.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nStock markets faced a sharp downturn today as tech giants saw significant declines, with the global tech index dropping by 3.4% in midday trading. Analysts attribute the selloff to investor concerns over rising interest rates and regulatory uncertainty.\n\nAmong the hardest hit, nexon technologies saw its stock plummet by 7.8% after reporting lower-than-expected quarterly earnings. In contrast, Omega Energy posted a modest 2.1% gain, driven by rising oil prices.\n\nMeanwhile, commodity markets reflected a mixed sentiment. Gold futures rose by 1.5%, reaching $2,080 per ounce, as investors sought safe-haven assets. Crude oil prices continued their rally, climbing to $87.60 per barrel, supported by supply constraints and strong demand.\n\nFinancial experts are closely watching the Federal Reserve's next move, as speculation grows over potential rate hikes. The upcoming policy announcement is expected to influence investor confidence and overall market stability.\n```\n\n<Output>\nentity<|#|>Global Tech Index<|#|>category<|#|>The Global Tech Index tracks the performance of major technology stocks and experienced a 3.4% decline today.\nentity<|#|>Nexon Technologies<|#|>organization<|#|>Nexon Technologies is a tech company that saw its stock decline by 7.8% after disappointing earnings.\nentity<|#|>Omega Energy<|#|>organization<|#|>Omega Energy is an energy company that gained 2.1% in stock value due to rising oil prices.\nentity<|#|>Gold Futures<|#|>product<|#|>Gold futures rose by 1.5%, indicating increased investor interest in safe-haven assets.\nentity<|#|>Crude Oil<|#|>product<|#|>Crude oil prices rose to $87.60 per barrel due to supply constraints and strong demand.\nentity<|#|>Market Selloff<|#|>category<|#|>Market selloff refers to the significant decline in stock values due to investor concerns over interest rates and regulations.\nentity<|#|>Federal Reserve Policy Announcement<|#|>category<|#|>The Federal Reserve's upcoming policy announcement is expected to impact investor confidence and market stability.\nentity<|#|>3.4% Decline<|#|>category<|#|>The Global Tech Index experienced a 3.4% decline in midday trading.\nrelation<|#|>Global Tech Index<|#|>Market Selloff<|#|>market performance, investor sentiment<|#|>The decline in the Global Tech Index is part of the broader market selloff driven by investor concerns.\nrelation<|#|>Nexon Technologies<|#|>Global Tech Index<|#|>company impact, index movement<|#|>Nexon Technologies' stock decline contributed to the overall drop in the Global Tech Index.\nrelation<|#|>Gold Futures<|#|>Market Selloff<|#|>market reaction, safe-haven investment<|#|>Gold prices rose as investors sought safe-haven assets during the market selloff.\nrelation<|#|>Federal Reserve Policy Announcement<|#|>Market Selloff<|#|>interest rate impact, financial regulation<|#|>Speculation over Federal Reserve policy changes contributed to market volatility and investor selloff.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nAt the World Athletics Championship in Tokyo, Noah Carter broke the 100m sprint record using cutting-edge carbon-fiber spikes.\n```\n\n<Output>\nentity<|#|>World Athletics Championship<|#|>event<|#|>The World Athletics Championship is a global sports competition featuring top athletes in track and field.\nentity<|#|>Tokyo<|#|>location<|#|>Tokyo is the host city of the World Athletics Championship.\nentity<|#|>Noah Carter<|#|>person<|#|>Noah Carter is a sprinter who set a new record in the 100m sprint at the World Athletics Championship.\nentity<|#|>100m Sprint Record<|#|>category<|#|>The 100m sprint record is a benchmark in athletics, recently broken by Noah Carter.\nentity<|#|>Carbon-Fiber Spikes<|#|>equipment<|#|>Carbon-fiber spikes are advanced sprinting shoes that provide enhanced speed and traction.\nentity<|#|>World Athletics Federation<|#|>organization<|#|>The World Athletics Federation is the governing body overseeing the World Athletics Championship and record validations.\nrelation<|#|>World Athletics Championship<|#|>Tokyo<|#|>event location, international competition<|#|>The World Athletics Championship is being hosted in Tokyo.\nrelation<|#|>Noah Carter<|#|>100m Sprint Record<|#|>athlete achievement, record-breaking<|#|>Noah Carter set a new 100m sprint record at the championship.\nrelation<|#|>Noah Carter<|#|>Carbon-Fiber Spikes<|#|>athletic equipment, performance boost<|#|>Noah Carter used carbon-fiber spikes to enhance performance during the race.\nrelation<|#|>Noah Carter<|#|>World Athletics Championship<|#|>athlete participation, competition<|#|>Noah Carter is competing at the World Athletics Championship.\n<|COMPLETE|>\n\n\n\n---Real Data to be Processed---\n<Input>\nEntity_types: [Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject]\nText:\n```\nASurveyofReinforcementLearningforLargeReasoningModels 2025-09-11\nA Survey of Reinforcement Learning for Large\nReasoning Models\nKaiyanZhang1,YuxinZuo1,BingxiangHe1,YoubangSun1,RunzeLiu1,CheJiang1,YuchenFan2,3,\nKaiTian1,GuoliJia1,PengfeiLi2,6,YuFu9,XingtaiLv1,YuchenZhang2,4,SihangZeng7,ShangQu1,2,\nHaozhanLi1,ShijieWang2,YuruWang1,XinweiLong1,FangfuLiu1,XiangXu5,JiazeMa1,XuekaiZhu3,\nErmoHua1,2,YihaoLiu1,2,ZonglinLi2,HuayuChen1,XiaoyeQu2,YafuLi2,WeizeChen1,ZhenzhaoYuan1,\nJunqiGao6,DongLi6,ZhiyuanMa8,GanquCui2,ZhiyuanLiu1,BiqingQi2,NingDing1,2,BowenZhou1,2\n1TsinghuaUniversity 2ShanghaiAILaboratory 3ShanghaiJiaoTongUniversity 4PekingUniversity\n5UniversityofScienceandTechnologyofChina 6HarbinInstituteofTechnology 7UniversityofWashington\n8HuazhongUniversityofScienceandTechnology 9UniversityCollegeLondon\nProjectLead. CoreContributors. CorrespondingAuthors.\n#zhang-ky22@mails.tsinghua.edu.cn TsinghuaC3I/Awesome-RL-for-LRMs\nAbstract | Inthispaper,wesurveyrecentadvancesinReinforcementLearning(RL)forreasoningwithLarge\nLanguageModels(LLMs). RLhasachievedremarkablesuccessinadvancingthefrontierofLLMcapabilities,\nparticularlyinaddressingcomplexlogicaltaskssuchasmathematicsandcoding. Asaresult,RLhasemerged\nasafoundationalmethodologyfortransformingLLMsintoLRMs. Withtherapidprogressofthefield,further\nscaling of RL for LRMs now faces foundational challenges not only in computational resources but also in\nalgorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of\nthisdomain,reassessitstrajectory,andexplorestrategiestoenhancethescalabilityofRLtowardArtificial\nSuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning\nabilities, especially since the release of DeepSeek-R1, including foundational components, core problems,\ntrainingresources,anddownstreamapplications,toidentifyfutureopportunitiesanddirectionsforthisrapidly\nevolvingarea. WehopethisreviewwillpromotefutureresearchonRLforbroaderreasoningmodels.\nSection 3 Section 4\nFoundational Components Foundational Problems\nRLs Role Model Prior Training Recipes RL vs. SFT Reward Type\nReward Design\nVS VS VS VS VS\nSharpening Discovery Weak Strong Tricks Traps Generalize Memorize Process Outcome\nVerifiable Generative\nRewards Rewards\nR D ew en a s r e d s Un R s e u w pe a r r v d is s ed R S e h w ap a i r n d g s Static Corpus Tr S ain e in c g t R io es n o u 5 rce Dynamic Environment Section 6\nApplications\nPolicy Optimization Math Code RL Infrastructure & Rule Code\nFrameworks\nPolicy Critic-Based STEM Agent Mixture e.g.,OpenRLHF/veRL/AReaL/slime/TRL Game Model Ensemble\nGradient Algorithms\nAgentic Tasks Coding Tasks\nCritic-Free Off-Policy Regularization\nAlgorithms Optimization Objectives\nMultimodal Robotics\nTasks Tasks Sampling Strategy\nDynamic Sampling Multi-Agent Medical\nSampling Hyper-Parameters Systems Tasks\nCompute\necnegilletnI More Scalable\nEvolution(Training Step)\n)edosipE(\nnoitcaretnI\nAction\nat\nAgent Environment\nReward\nrt\nFigure 1 | Overview of the survey. We introduce the foundational components of RL for LRMs,\nalong with open problems, training resources, and applications. Central to this survey is a focus on\nlarge-scale interactions between language agents and environments throughout long-term evolution.\n5202\npeS\n81\n]LC.sc[\n2v72880.9052:viXra\n```",
    "queryparam": null,
    "create_time": 1765218582,
    "update_time": 1765218582,
    "_id": "default:extract:188e114c38aa253213404f4cfe59eee9"
  },
  "default:extract:d13046d9e30ce14d692341702a4742e1": {
    "return": "<|COMPLETE|>",
    "cache_type": "extract",
    "chunk_id": "chunk-32bcdbba2d266d061d85f94faf74b18a",
    "original_prompt": "---Task---\nBased on the last extraction task, identify and extract any **missed or incorrectly formatted** entities and relationships from the input text.\n\n---Instructions---\n1.  **Strict Adherence to System Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system instructions.\n2.  **Focus on Corrections/Additions:**\n    *   **Do NOT** re-output entities and relationships that were **correctly and fully** extracted in the last task.\n    *   If an entity or relationship was **missed** in the last task, extract and output it now according to the system format.\n    *   If an entity or relationship was **truncated, had missing fields, or was otherwise incorrectly formatted** in the last task, re-output the *corrected and complete* version in the specified format.\n3.  **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n4.  **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n5.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\n6.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant missing or corrected entities and relationships have been extracted and presented.\n7.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\n\n<Output>\n---Role---\nYou are a Knowledge Graph Specialist responsible for extracting entities and relationships from the input text.\n\n---Instructions---\n1.  **Entity Extraction & Output:**\n    *   **Identification:** Identify clearly defined and meaningful entities in the input text.\n    *   **Entity Details:** For each identified entity, extract the following information:\n        *   `entity_name`: The name of the entity. If the entity name is case-insensitive, capitalize the first letter of each significant word (title case). Ensure **consistent naming** across the entire extraction process.\n        *   `entity_type`: Categorize the entity using one of the following types: `Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject`. If none of the provided entity types apply, do not add new entity type and classify it as `Other`.\n        *   `entity_description`: Provide a concise yet comprehensive description of the entity's attributes and activities, based *solely* on the information present in the input text.\n    *   **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n        *   Format: `entity<|#|>entity_name<|#|>entity_type<|#|>entity_description`\n\n2.  **Relationship Extraction & Output:**\n    *   **Identification:** Identify direct, clearly stated, and meaningful relationships between previously extracted entities.\n    *   **N-ary Relationship Decomposition:** If a single statement describes a relationship involving more than two entities (an N-ary relationship), decompose it into multiple binary (two-entity) relationship pairs for separate description.\n        *   **Example:** For \"Alice, Bob, and Carol collaborated on Project X,\" extract binary relationships such as \"Alice collaborated with Project X,\" \"Bob collaborated with Project X,\" and \"Carol collaborated with Project X,\" or \"Alice collaborated with Bob,\" based on the most reasonable binary interpretations.\n    *   **Relationship Details:** For each binary relationship, extract the following fields:\n        *   `source_entity`: The name of the source entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `target_entity`: The name of the target entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `relationship_keywords`: One or more high-level keywords summarizing the overarching nature, concepts, or themes of the relationship. Multiple keywords within this field must be separated by a comma `,`. **DO NOT use `<|#|>` for separating multiple keywords within this field.**\n        *   `relationship_description`: A concise explanation of the nature of the relationship between the source and target entities, providing a clear rationale for their connection.\n    *   **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n        *   Format: `relation<|#|>source_entity<|#|>target_entity<|#|>relationship_keywords<|#|>relationship_description`\n\n3.  **Delimiter Usage Protocol:**\n    *   The `<|#|>` is a complete, atomic marker and **must not be filled with content**. It serves strictly as a field separator.\n    *   **Incorrect Example:** `entity<|#|>Tokyo<|location|>Tokyo is the capital of Japan.`\n    *   **Correct Example:** `entity<|#|>Tokyo<|#|>location<|#|>Tokyo is the capital of Japan.`\n\n4.  **Relationship Direction & Duplication:**\n    *   Treat all relationships as **undirected** unless explicitly stated otherwise. Swapping the source and target entities for an undirected relationship does not constitute a new relationship.\n    *   Avoid outputting duplicate relationships.\n\n5.  **Output Order & Prioritization:**\n    *   Output all extracted entities first, followed by all extracted relationships.\n    *   Within the list of relationships, prioritize and output those relationships that are **most significant** to the core meaning of the input text first.\n\n6.  **Context & Objectivity:**\n    *   Ensure all entity names and descriptions are written in the **third person**.\n    *   Explicitly name the subject or object; **avoid using pronouns** such as `this article`, `this paper`, `our company`, `I`, `you`, and `he/she`.\n\n7.  **Language & Proper Nouns:**\n    *   The entire output (entity names, keywords, and descriptions) must be written in `English`.\n    *   Proper nouns (e.g., personal names, place names, organization names) should be retained in their original language if a proper, widely accepted translation is not available or would cause ambiguity.\n\n8.  **Completion Signal:** Output the literal string `<|COMPLETE|>` only after all entities and relationships, following all criteria, have been completely extracted and outputted.\n\n---Examples---\n<Input Text>\n```\nwhile Alex clenched his jaw, the buzz of frustration dull against the backdrop of Taylor's authoritarian certainty. It was this competitive undercurrent that kept him alert, the sense that his and Jordan's shared commitment to discovery was an unspoken rebellion against Cruz's narrowing vision of control and order.\n\nThen Taylor did something unexpected. They paused beside Jordan and, for a moment, observed the device with something akin to reverence. \"If this tech can be understood...\" Taylor said, their voice quieter, \"It could change the game for us. For all of us.\"\n\nThe underlying dismissal earlier seemed to falter, replaced by a glimpse of reluctant respect for the gravity of what lay in their hands. Jordan looked up, and for a fleeting heartbeat, their eyes locked with Taylor's, a wordless clash of wills softening into an uneasy truce.\n\nIt was a small transformation, barely perceptible, but one that Alex noted with an inward nod. They had all been brought here by different paths\n```\n\n<Output>\nentity<|#|>Alex<|#|>person<|#|>Alex is a character who experiences frustration and is observant of the dynamics among other characters.\nentity<|#|>Taylor<|#|>person<|#|>Taylor is portrayed with authoritarian certainty and shows a moment of reverence towards a device, indicating a change in perspective.\nentity<|#|>Jordan<|#|>person<|#|>Jordan shares a commitment to discovery and has a significant interaction with Taylor regarding a device.\nentity<|#|>Cruz<|#|>person<|#|>Cruz is associated with a vision of control and order, influencing the dynamics among other characters.\nentity<|#|>The Device<|#|>equipment<|#|>The Device is central to the story, with potential game-changing implications, and is revered by Taylor.\nrelation<|#|>Alex<|#|>Taylor<|#|>power dynamics, observation<|#|>Alex observes Taylor's authoritarian behavior and notes changes in Taylor's attitude toward the device.\nrelation<|#|>Alex<|#|>Jordan<|#|>shared goals, rebellion<|#|>Alex and Jordan share a commitment to discovery, which contrasts with Cruz's vision.)\nrelation<|#|>Taylor<|#|>Jordan<|#|>conflict resolution, mutual respect<|#|>Taylor and Jordan interact directly regarding the device, leading to a moment of mutual respect and an uneasy truce.\nrelation<|#|>Jordan<|#|>Cruz<|#|>ideological conflict, rebellion<|#|>Jordan's commitment to discovery is in rebellion against Cruz's vision of control and order.\nrelation<|#|>Taylor<|#|>The Device<|#|>reverence, technological significance<|#|>Taylor shows reverence towards the device, indicating its importance and potential impact.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nStock markets faced a sharp downturn today as tech giants saw significant declines, with the global tech index dropping by 3.4% in midday trading. Analysts attribute the selloff to investor concerns over rising interest rates and regulatory uncertainty.\n\nAmong the hardest hit, nexon technologies saw its stock plummet by 7.8% after reporting lower-than-expected quarterly earnings. In contrast, Omega Energy posted a modest 2.1% gain, driven by rising oil prices.\n\nMeanwhile, commodity markets reflected a mixed sentiment. Gold futures rose by 1.5%, reaching $2,080 per ounce, as investors sought safe-haven assets. Crude oil prices continued their rally, climbing to $87.60 per barrel, supported by supply constraints and strong demand.\n\nFinancial experts are closely watching the Federal Reserve's next move, as speculation grows over potential rate hikes. The upcoming policy announcement is expected to influence investor confidence and overall market stability.\n```\n\n<Output>\nentity<|#|>Global Tech Index<|#|>category<|#|>The Global Tech Index tracks the performance of major technology stocks and experienced a 3.4% decline today.\nentity<|#|>Nexon Technologies<|#|>organization<|#|>Nexon Technologies is a tech company that saw its stock decline by 7.8% after disappointing earnings.\nentity<|#|>Omega Energy<|#|>organization<|#|>Omega Energy is an energy company that gained 2.1% in stock value due to rising oil prices.\nentity<|#|>Gold Futures<|#|>product<|#|>Gold futures rose by 1.5%, indicating increased investor interest in safe-haven assets.\nentity<|#|>Crude Oil<|#|>product<|#|>Crude oil prices rose to $87.60 per barrel due to supply constraints and strong demand.\nentity<|#|>Market Selloff<|#|>category<|#|>Market selloff refers to the significant decline in stock values due to investor concerns over interest rates and regulations.\nentity<|#|>Federal Reserve Policy Announcement<|#|>category<|#|>The Federal Reserve's upcoming policy announcement is expected to impact investor confidence and market stability.\nentity<|#|>3.4% Decline<|#|>category<|#|>The Global Tech Index experienced a 3.4% decline in midday trading.\nrelation<|#|>Global Tech Index<|#|>Market Selloff<|#|>market performance, investor sentiment<|#|>The decline in the Global Tech Index is part of the broader market selloff driven by investor concerns.\nrelation<|#|>Nexon Technologies<|#|>Global Tech Index<|#|>company impact, index movement<|#|>Nexon Technologies' stock decline contributed to the overall drop in the Global Tech Index.\nrelation<|#|>Gold Futures<|#|>Market Selloff<|#|>market reaction, safe-haven investment<|#|>Gold prices rose as investors sought safe-haven assets during the market selloff.\nrelation<|#|>Federal Reserve Policy Announcement<|#|>Market Selloff<|#|>interest rate impact, financial regulation<|#|>Speculation over Federal Reserve policy changes contributed to market volatility and investor selloff.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nAt the World Athletics Championship in Tokyo, Noah Carter broke the 100m sprint record using cutting-edge carbon-fiber spikes.\n```\n\n<Output>\nentity<|#|>World Athletics Championship<|#|>event<|#|>The World Athletics Championship is a global sports competition featuring top athletes in track and field.\nentity<|#|>Tokyo<|#|>location<|#|>Tokyo is the host city of the World Athletics Championship.\nentity<|#|>Noah Carter<|#|>person<|#|>Noah Carter is a sprinter who set a new record in the 100m sprint at the World Athletics Championship.\nentity<|#|>100m Sprint Record<|#|>category<|#|>The 100m sprint record is a benchmark in athletics, recently broken by Noah Carter.\nentity<|#|>Carbon-Fiber Spikes<|#|>equipment<|#|>Carbon-fiber spikes are advanced sprinting shoes that provide enhanced speed and traction.\nentity<|#|>World Athletics Federation<|#|>organization<|#|>The World Athletics Federation is the governing body overseeing the World Athletics Championship and record validations.\nrelation<|#|>World Athletics Championship<|#|>Tokyo<|#|>event location, international competition<|#|>The World Athletics Championship is being hosted in Tokyo.\nrelation<|#|>Noah Carter<|#|>100m Sprint Record<|#|>athlete achievement, record-breaking<|#|>Noah Carter set a new 100m sprint record at the championship.\nrelation<|#|>Noah Carter<|#|>Carbon-Fiber Spikes<|#|>athletic equipment, performance boost<|#|>Noah Carter used carbon-fiber spikes to enhance performance during the race.\nrelation<|#|>Noah Carter<|#|>World Athletics Championship<|#|>athlete participation, competition<|#|>Noah Carter is competing at the World Athletics Championship.\n<|COMPLETE|>\n\n\n\n---Real Data to be Processed---\n<Input>\nEntity_types: [Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject]\nText:\n```\nASurveyofReinforcementLearningforLargeReasoningModels 2025-09-11\nA Survey of Reinforcement Learning for Large\nReasoning Models\nKaiyanZhang1,YuxinZuo1,BingxiangHe1,YoubangSun1,RunzeLiu1,CheJiang1,YuchenFan2,3,\nKaiTian1,GuoliJia1,PengfeiLi2,6,YuFu9,XingtaiLv1,YuchenZhang2,4,SihangZeng7,ShangQu1,2,\nHaozhanLi1,ShijieWang2,YuruWang1,XinweiLong1,FangfuLiu1,XiangXu5,JiazeMa1,XuekaiZhu3,\nErmoHua1,2,YihaoLiu1,2,ZonglinLi2,HuayuChen1,XiaoyeQu2,YafuLi2,WeizeChen1,ZhenzhaoYuan1,\nJunqiGao6,DongLi6,ZhiyuanMa8,GanquCui2,ZhiyuanLiu1,BiqingQi2,NingDing1,2,BowenZhou1,2\n1TsinghuaUniversity 2ShanghaiAILaboratory 3ShanghaiJiaoTongUniversity 4PekingUniversity\n5UniversityofScienceandTechnologyofChina 6HarbinInstituteofTechnology 7UniversityofWashington\n8HuazhongUniversityofScienceandTechnology 9UniversityCollegeLondon\nProjectLead. CoreContributors. CorrespondingAuthors.\n#zhang-ky22@mails.tsinghua.edu.cn TsinghuaC3I/Awesome-RL-for-LRMs\nAbstract | Inthispaper,wesurveyrecentadvancesinReinforcementLearning(RL)forreasoningwithLarge\nLanguageModels(LLMs). RLhasachievedremarkablesuccessinadvancingthefrontierofLLMcapabilities,\nparticularlyinaddressingcomplexlogicaltaskssuchasmathematicsandcoding. Asaresult,RLhasemerged\nasafoundationalmethodologyfortransformingLLMsintoLRMs. Withtherapidprogressofthefield,further\nscaling of RL for LRMs now faces foundational challenges not only in computational resources but also in\nalgorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of\nthisdomain,reassessitstrajectory,andexplorestrategiestoenhancethescalabilityofRLtowardArtificial\nSuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning\nabilities, especially since the release of DeepSeek-R1, including foundational components, core problems,\ntrainingresources,anddownstreamapplications,toidentifyfutureopportunitiesanddirectionsforthisrapidly\nevolvingarea. WehopethisreviewwillpromotefutureresearchonRLforbroaderreasoningmodels.\nSection 3 Section 4\nFoundational Components Foundational Problems\nRLs Role Model Prior Training Recipes RL vs. SFT Reward Type\nReward Design\nVS VS VS VS VS\nSharpening Discovery Weak Strong Tricks Traps Generalize Memorize Process Outcome\nVerifiable Generative\nRewards Rewards\nR D ew en a s r e d s Un R s e u w pe a r r v d is s ed R S e h w ap a i r n d g s Static Corpus Tr S ain e in c g t R io es n o u 5 rce Dynamic Environment Section 6\nApplications\nPolicy Optimization Math Code RL Infrastructure & Rule Code\nFrameworks\nPolicy Critic-Based STEM Agent Mixture e.g.,OpenRLHF/veRL/AReaL/slime/TRL Game Model Ensemble\nGradient Algorithms\nAgentic Tasks Coding Tasks\nCritic-Free Off-Policy Regularization\nAlgorithms Optimization Objectives\nMultimodal Robotics\nTasks Tasks Sampling Strategy\nDynamic Sampling Multi-Agent Medical\nSampling Hyper-Parameters Systems Tasks\nCompute\necnegilletnI More Scalable\nEvolution(Training Step)\n)edosipE(\nnoitcaretnI\nAction\nat\nAgent Environment\nReward\nrt\nFigure 1 | Overview of the survey. We introduce the foundational components of RL for LRMs,\nalong with open problems, training resources, and applications. Central to this survey is a focus on\nlarge-scale interactions between language agents and environments throughout long-term evolution.\n5202\npeS\n81\n]LC.sc[\n2v72880.9052:viXra\n```\n[{\"role\": \"user\", \"content\": \"---Task---\\nExtract entities and relationships from the input text to be processed.\\n\\n---Instructions---\\n1.  **Strict Adherence to Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system prompt.\\n2.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\\n3.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant entities and relationships have been extracted and presented.\\n4.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\\n\\n<Output>\"}, {\"role\": \"assistant\", \"content\": \"entity<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>Content<|#|>A paper published on 2025-09-11 that surveys recent advances in Reinforcement Learning for reasoning with Large Language Models.\\nentity<|#|>Reinforcement Learning (RL)<|#|>Concept<|#|>A foundational methodology that has achieved remarkable success in advancing Large Language Model capabilities, particularly in complex logical tasks, and transforms LLMs into LRMs.\\nentity<|#|>Large Reasoning Models (LRMs)<|#|>Concept<|#|>Models transformed from Large Language Models using Reinforcement Learning, facing foundational challenges in scalability.\\nentity<|#|>Large Language Models (LLMs)<|#|>Concept<|#|>Models whose capabilities have been advanced by Reinforcement Learning, especially in complex logical tasks, and can be transformed into Large Reasoning Models.\\nentity<|#|>Artificial SuperIntelligence (ASI)<|#|>Concept<|#|>A future goal towards which the scalability of Reinforcement Learning is being explored.\\nentity<|#|>DeepSeek-R1<|#|>Artifact<|#|>A model whose release influenced research applying Reinforcement Learning to LLMs and LRMs for reasoning abilities.\\nentity<|#|>Foundational Components<|#|>Concept<|#|>A category of topics examined in the survey, including RL's Role, Model Prior, and Training Recipes.\\nentity<|#|>Foundational Problems<|#|>Concept<|#|>A category of topics examined in the survey, including RL vs. SFT, Reward Type, and Reward Design.\\nentity<|#|>Training Resources<|#|>Concept<|#|>A category of topics examined in the survey, including Static Corpus Training and Dynamic Environment.\\nentity<|#|>Applications<|#|>Concept<|#|>A category of topics examined in the survey, including Policy Optimization, Math Code, RL Infrastructure & Frameworks, Agentic Tasks, Coding Tasks, Multimodal Robotics Tasks, and Medical Systems Tasks.\\nentity<|#|>Tsinghua University<|#|>Organization<|#|>An academic institution affiliated with multiple authors of the survey.\\nentity<|#|>Shanghai AI Laboratory<|#|>Organization<|#|>A research institution affiliated with multiple authors of the survey.\\nentity<|#|>Shanghai Jiao Tong University<|#|>Organization<|#|>An academic institution affiliated with authors Yuchen Fan and Xuekai Zhu.\\nentity<|#|>Peking University<|#|>Organization<|#|>An academic institution affiliated with author Yuchen Zhang.\\nentity<|#|>University of Science and Technology of China<|#|>Organization<|#|>An academic institution affiliated with author Xiang Xu.\\nentity<|#|>Harbin Institute of Technology<|#|>Organization<|#|>An academic institution affiliated with authors Pengfei Li, Junqi Gao, and Dong Li.\\nentity<|#|>University of Washington<|#|>Organization<|#|>An academic institution affiliated with author Sihang Zeng.\\nentity<|#|>Huazhong University of Science and Technology<|#|>Organization<|#|>An academic institution affiliated with author Zhiyuan Ma.\\nentity<|#|>University College London<|#|>Organization<|#|>An academic institution affiliated with author Yu Fu.\\nentity<|#|>Kaiyan Zhang<|#|>Person<|#|>A project lead and core contributor to the survey, affiliated with Tsinghua University.\\nentity<|#|>Yuxin Zuo<|#|>Person<|#|>A project lead and core contributor to the survey, affiliated with Tsinghua University.\\nentity<|#|>Bingxiang He<|#|>Person<|#|>A core contributor to the survey, affiliated with Tsinghua University.\\nentity<|#|>Youbang Sun<|#|>Person<|#|>A core contributor to the survey, affiliated with Tsinghua University.\\nentity<|#|>Runze Liu<|#|>Person<|#|>A core contributor to the survey, affiliated with Tsinghua University.\\nentity<|#|>Che Jiang<|#|>Person<|#|>A core contributor to the survey, affiliated with Tsinghua University.\\nentity<|#|>Yuchen Fan<|#|>Person<|#|>A core contributor to the survey, affiliated with Shanghai AI Laboratory and Shanghai Jiao Tong University.\\nentity<|#|>Kai Tian<|#|>Person<|#|>A core contributor to the survey, affiliated with Tsinghua University.\\nentity<|#|>Guoli Jia<|#|>Person<|#|>A core contributor to the survey, affiliated with Tsinghua University.\\nentity<|#|>Pengfei Li<|#|>Person<|#|>A core contributor to the survey, affiliated with Shanghai AI Laboratory and Harbin Institute of Technology.\\nentity<|#|>Yu Fu<|#|>Person<|#|>A core contributor to the survey, affiliated with University College London.\\nentity<|#|>Xingtai Lv<|#|>Person<|#|>A core contributor to the survey, affiliated with Tsinghua University.\\nentity<|#|>Yuchen Zhang<|#|>Person<|#|>A core contributor to the survey, affiliated with Shanghai AI Laboratory and Peking University.\\nentity<|#|>Sihang Zeng<|#|>Person<|#|>A core contributor to the survey, affiliated with University of Washington.\\nentity<|#|>Shang Qu<|#|>Person<|#|>A core contributor to the survey, affiliated with Tsinghua University and Shanghai AI Laboratory.\\nentity<|#|>Haozhan Li<|#|>Person<|#|>A core contributor to the survey, affiliated with Tsinghua University.\\nentity<|#|>Shijie Wang<|#|>Person<|#|>A core contributor to the survey, affiliated with Shanghai AI Laboratory.\\nentity<|#|>Yuru Wang<|#|>Person<|#|>A core contributor to the survey, affiliated with Tsinghua University.\\nentity<|#|>Xinwei Long<|#|>Person<|#|>An author of the survey, affiliated with Tsinghua University.\\nentity<|#|>Fangfu Liu<|#|>Person<|#|>An author of the survey, affiliated with Tsinghua University.\\nentity<|#|>Xiang Xu<|#|>Person<|#|>An author of the survey, affiliated with University of Science and Technology of China.\\nentity<|#|>Jiaze Ma<|#|>Person<|#|>An author of the survey, affiliated with Tsinghua University.\\nentity<|#|>Xuekai Zhu<|#|>Person<|#|>An author of the survey, affiliated with Shanghai Jiao Tong University.\\nentity<|#|>Ermo Hua<|#|>Person<|#|>An author of the survey, affiliated with Tsinghua University and Shanghai AI Laboratory.\\nentity<|#|>Yihao Liu<|#|>Person<|#|>An author of the survey, affiliated with Tsinghua University and Shanghai AI Laboratory.\\nentity<|#|>Zonglin Li<|#|>Person<|#|>An author of the survey, affiliated with Shanghai AI Laboratory.\\nentity<|#|>Huayu Chen<|#|>Person<|#|>An author of the survey, affiliated with Tsinghua University.\\nentity<|#|>Xiaoye Qu<|#|>Person<|#|>An author of the survey, affiliated with Shanghai AI Laboratory.\\nentity<|#|>Yafu Li<|#|>Person<|#|>An author of the survey, affiliated with Shanghai AI Laboratory.\\nentity<|#|>Weize Chen<|#|>Person<|#|>An author of the survey, affiliated with Tsinghua University.\\nentity<|#|>Zhenzhao Yuan<|#|>Person<|#|>An author of the survey, affiliated with Tsinghua University.\\nentity<|#|>Junqi Gao<|#|>Person<|#|>An author of the survey, affiliated with Harbin Institute of Technology.\\nentity<|#|>Dong Li<|#|>Person<|#|>An author of the survey, affiliated with Harbin Institute of Technology.\\nentity<|#|>Zhiyuan Ma<|#|>Person<|#|>An author of the survey, affiliated with Huazhong University of Science and Technology.\\nentity<|#|>Ganqu Cui<|#|>Person<|#|>An author of the survey, affiliated with Shanghai AI Laboratory.\\nentity<|#|>Zhiyuan Liu<|#|>Person<|#|>An author of the survey, affiliated with Tsinghua University.\\nentity<|#|>Biqing Qi<|#|>Person<|#|>A corresponding author of the survey, affiliated with Shanghai AI Laboratory.\\nentity<|#|>Ning Ding<|#|>Person<|#|>A corresponding author of the survey, affiliated with Tsinghua University and Shanghai AI Laboratory.\\nentity<|#|>Bowen Zhou<|#|>Person<|#|>A corresponding author of the survey, affiliated with Tsinghua University and Shanghai AI Laboratory.\\nentity<|#|>Tsinghua C3I<|#|>Organization<|#|>An organization mentioned in the email signature.\\nentity<|#|>Awesome-RL-for-LRMs<|#|>Concept<|#|>A project or repository related to Reinforcement Learning for Large Reasoning Models.\\nentity<|#|>Abstract<|#|>Content<|#|>A section of the paper that surveys recent advances in Reinforcement Learning for reasoning with Large Language Models.\\nentity<|#|>Figure 1<|#|>Content<|#|>Provides an overview of the survey, introducing foundational components, open problems, training resources, and applications.\\nentity<|#|>RL's Role<|#|>Concept<|#|>A foundational component discussed in the survey.\\nentity<|#|>Model Prior<|#|>Concept<|#|>A foundational component discussed in the survey.\\nentity<|#|>Training Recipes<|#|>Concept<|#|>A foundational component discussed in the survey.\\nentity<|#|>RL vs. SFT<|#|>Concept<|#|>A foundational problem discussed in the survey, comparing Reinforcement Learning with Supervised Fine-Tuning.\\nentity<|#|>Reward Type<|#|>Concept<|#|>A foundational problem discussed in the survey, concerning different categories of rewards.\\nentity<|#|>Reward Design<|#|>Concept<|#|>A foundational problem discussed in the survey, concerning the creation of effective rewards.\\nentity<|#|>Sharpening Discovery<|#|>Concept<|#|>A concept related to reward design.\\nentity<|#|>Weak Strong<|#|>Concept<|#|>A concept related to reward design, likely referring to weak vs. strong supervision or signals.\\nentity<|#|>Tricks Traps<|#|>Concept<|#|>A concept related to training recipes, referring to effective techniques and common pitfalls.\\nentity<|#|>Generalize Memorize<|#|>Concept<|#|>A concept related to training recipes, referring to the balance between generalization and memorization.\\nentity<|#|>Process Outcome<|#|>Concept<|#|>A concept related to reward type, referring to process-based vs. outcome-based rewards.\\nentity<|#|>Verifiable Generative Rewards<|#|>Concept<|#|>A type of reward.\\nentity<|#|>Unsupervised Rewards<|#|>Concept<|#|>A type of reward.\\nentity<|#|>Shaping Rewards<|#|>Concept<|#|>A type of reward.\\nentity<|#|>Static Corpus Training<|#|>Concept<|#|>A type of training resource.\\nentity<|#|>Dynamic Environment<|#|>Concept<|#|>A type of training resource.\\nentity<|#|>Policy Optimization<|#|>Method<|#|>An application area of RL for LRMs.\\nentity<|#|>Math Code<|#|>Concept<|#|>An application area of RL for LRMs, encompassing mathematics and coding tasks.\\nentity<|#|>RL Infrastructure & Frameworks<|#|>Concept<|#|>An application area of RL for LRMs.\\nentity<|#|>Policy Critic-Based Algorithms<|#|>Method<|#|>A type of algorithm under Policy Optimization.\\nentity<|#|>Policy Gradient Algorithms<|#|>Method<|#|>A type of algorithm under Policy Critic-Based Algorithms.\\nentity<|#|>Critic-Free Algorithms<|#|>Method<|#|>A type of algorithm under Policy Optimization.\\nentity<|#|>Off-Policy Regularization<|#|>Method<|#|>A concept related to Critic-Free Algorithms.\\nentity<|#|>Optimization Objectives<|#|>Concept<|#|>A concept related to Critic-Free Algorithms.\\nentity<|#|>Sampling Strategy<|#|>Method<|#|>A concept related to Critic-Free Algorithms.\\nentity<|#|>Dynamic Sampling<|#|>Method<|#|>A type of sampling strategy.\\nentity<|#|>Multi-Agent Sampling<|#|>Method<|#|>A type of sampling strategy.\\nentity<|#|>Sampling Hyper-Parameters<|#|>Concept<|#|>A concept related to sampling strategy.\\nentity<|#|>Compute<|#|>Concept<|#|>A resource related to sampling strategy.\\nentity<|#|>STEM Agent<|#|>Concept<|#|>An application under Math Code.\\nentity<|#|>Mixture<|#|>Concept<|#|>A category of applications under Math Code, including specific frameworks.\\nentity<|#|>Game<|#|>Concept<|#|>An application under Math Code.\\nentity<|#|>Model Ensemble<|#|>Concept<|#|>An application under Math Code.\\nentity<|#|>OpenRLHF<|#|>Artifact<|#|>An example of an RL infrastructure and framework.\\nentity<|#|>veRL<|#|>Artifact<|#|>An example of an RL infrastructure and framework.\\nentity<|#|>AReaL<|#|>Artifact<|#|>An example of an RL infrastructure and framework.\\nentity<|#|>slime<|#|>Artifact<|#|>An example of an RL infrastructure and framework.\\nentity<|#|>TRL<|#|>Artifact<|#|>An example of an RL infrastructure and framework.\\nentity<|#|>Agentic Tasks<|#|>Concept<|#|>An application area of RL for LRMs.\\nentity<|#|>Coding Tasks<|#|>Concept<|#|>An application area of RL for LRMs.\\nentity<|#|>Multimodal Robotics Tasks<|#|>Concept<|#|>An application area of RL for LRMs.\\nentity<|#|>Medical Systems Tasks<|#|>Concept<|#|>An application area of RL for LRMs.\\nentity<|#|>Agent<|#|>Concept<|#|>An entity that interacts with an environment and takes actions, central to the survey's focus on large-scale interactions.\\nentity<|#|>Environment<|#|>Concept<|#|>An entity with which an agent interacts, providing rewards, central to the survey's focus on large-scale interactions.\\nentity<|#|>Action<|#|>Concept<|#|>A step taken by an agent within an environment, part of the interaction process.\\nentity<|#|>Reward<|#|>Concept<|#|>A signal received by an agent from the environment, part of the interaction process.\\nentity<|#|>Interaction (Episode)<|#|>Concept<|#|>A process involving an agent and environment over time, central to the survey's focus.\\nentity<|#|>Evolution (Training Step)<|#|>Concept<|#|>A process of development or training, central to the survey's focus.\\nentity<|#|>Computational Resources<|#|>Concept<|#|>A foundational challenge for scaling RL for LRMs.\\nentity<|#|>Algorithm Design<|#|>Concept<|#|>A foundational challenge for scaling RL for LRMs.\\nentity<|#|>Training Data<|#|>Concept<|#|>A foundational challenge for scaling RL for LRMs.\\nentity<|#|>Infrastructure<|#|>Concept<|#|>A foundational challenge for scaling RL for LRMs.\\nentity<|#|>Mathematics<|#|>Concept<|#|>A complex logical task that Reinforcement Learning addresses.\\nentity<|#|>Coding<|#|>Concept<|#|>A complex logical task that Reinforcement Learning addresses.\\nentity<|#|>Section 3<|#|>Content<|#|>A section of the survey titled \\\"Foundational Components\\\".\\nentity<|#|>Section 4<|#|>Content<|#|>A section of the survey titled \\\"Foundational Problems\\\".\\nentity<|#|>Section 5<|#|>Content<|#|>A section of the survey titled \\\"Training Resources\\\".\\nentity<|#|>Section 6<|#|>Content<|#|>A section of the survey titled \\\"Applications\\\".\\nentity<|#|>Supervised Fine-Tuning (SFT)<|#|>Method<|#|>A method compared with Reinforcement Learning as a foundational problem.\\nrelation<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>Reinforcement Learning (RL)<|#|>survey, focus<|#|>The survey paper focuses on recent advances in Reinforcement Learning.\\nrelation<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>Large Reasoning Models (LRMs)<|#|>survey, focus<|#|>The survey paper focuses on Reinforcement Learning for Large Reasoning Models.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Large Language Models (LLMs)<|#|>advancement, capability enhancement<|#|>Reinforcement Learning has achieved remarkable success in advancing the frontier of Large Language Model capabilities.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Mathematics<|#|>application, problem-solving<|#|>Reinforcement Learning addresses complex logical tasks such as mathematics.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Coding<|#|>application, problem-solving<|#|>Reinforcement Learning addresses complex logical tasks such as coding.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Large Reasoning Models (LRMs)<|#|>transformation, methodology<|#|>Reinforcement Learning has emerged as a foundational methodology for transforming LLMs into LRMs.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Computational Resources<|#|>challenge, scalability<|#|>Scaling Reinforcement Learning for LRMs faces foundational challenges in computational resources.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Algorithm Design<|#|>challenge, scalability<|#|>Scaling Reinforcement Learning for LRMs faces foundational challenges in algorithm design.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Training Data<|#|>challenge, scalability<|#|>Scaling Reinforcement Learning for LRMs faces foundational challenges in training data.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Infrastructure<|#|>challenge, scalability<|#|>Scaling Reinforcement Learning for LRMs faces foundational challenges in infrastructure.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Artificial SuperIntelligence (ASI)<|#|>goal, scalability<|#|>Strategies are explored to enhance the scalability of Reinforcement Learning toward Artificial SuperIntelligence.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>DeepSeek-R1<|#|>research influence, application<|#|>Research applying Reinforcement Learning to LLMs and LRMs for reasoning abilities is examined, especially since the release of DeepSeek-R1.\\nrelation<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>Foundational Components<|#|>content, examination<|#|>The survey examines foundational components.\\nrelation<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>Foundational Problems<|#|>content, examination<|#|>The survey examines core problems, also referred to as foundational problems.\\nrelation<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>Training Resources<|#|>content, examination<|#|>The survey examines training resources.\\nrelation<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>Applications<|#|>content, examination<|#|>The survey examines downstream applications.\\nrelation<|#|>Abstract<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>summary, content<|#|>The abstract summarizes the paper's content regarding Reinforcement Learning for Large Language Models.\\nrelation<|#|>Figure 1<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>overview, structure<|#|>Figure 1 provides an overview of the survey.\\nrelation<|#|>Section 3<|#|>Foundational Components<|#|>covers, content<|#|>Section 3 covers Foundational Components.\\nrelation<|#|>Section 4<|#|>Foundational Problems<|#|>covers, content<|#|>Section 4 covers Foundational Problems.\\nrelation<|#|>Section 5<|#|>Training Resources<|#|>covers, content<|#|>Section 5 covers Training Resources.\\nrelation<|#|>Section 6<|#|>Applications<|#|>covers, content<|#|>Section 6 covers Applications.\\nrelation<|#|>Foundational Components<|#|>RL's Role<|#|>includes, topic<|#|>Foundational Components include RL's Role.\\nrelation<|#|>Foundational Components<|#|>Model Prior<|#|>includes, topic<|#|>Foundational Components include Model Prior.\\nrelation<|#|>Foundational Components<|#|>Training Recipes<|#|>includes, topic<|#|>Foundational Components include Training Recipes.\\nrelation<|#|>Foundational Problems<|#|>RL vs. SFT<|#|>includes, topic<|#|>Foundational Problems include RL vs. SFT.\\nrelation<|#|>Foundational Problems<|#|>Reward Type<|#|>includes, topic<|#|>Foundational Problems include Reward Type.\\nrelation<|#|>Foundational Problems<|#|>Reward Design<|#|>includes, topic<|#|>Foundational Problems include Reward Design.\\nrelation<|#|>Training Resources<|#|>Static Corpus Training<|#|>includes, topic<|#|>Training Resources include Static Corpus Training.\\nrelation<|#|>Training Resources<|#|>Dynamic Environment<|#|>includes, topic<|#|>Training Resources include Dynamic Environment.\\nrelation<#|>Applications<|#|>Policy Optimization<|#|>includes, topic<|#|>Applications include Policy Optimization.\\nrelation<|#|>Applications<|#|>Math Code<|#|>includes, topic<|#|>Applications include Math Code.\\nrelation<|#|>Applications<|#|>RL Infrastructure & Frameworks<|#|>includes, topic<|#|>Applications include RL Infrastructure & Frameworks.\\nrelation<|#|>Applications<|#|>Agentic Tasks<|#|>includes, topic<|#|>Applications include Agentic Tasks.\\nrelation<|#|>Applications<|#|>Coding Tasks<|#|>includes, topic<|#|>Applications include Coding Tasks.\\nrelation<|#|>Applications<|#|>Multimodal Robotics Tasks<|#|>includes, topic<|#|>Applications include Multimodal Robotics Tasks.\\nrelation<|#|>Applications<|#|>Medical Systems Tasks<|#|>includes, topic<|#|>Applications include Medical Systems Tasks.\\nrelation<|#|>Policy Optimization<|#|>Policy Critic-Based Algorithms<|#|>comprises, method<|#|>Policy Optimization comprises Policy Critic-Based Algorithms.\\nrelation<|#|>Policy Optimization<|#|>Critic-Free Algorithms<|#|>comprises, method<|#|>Policy Optimization comprises Critic-Free Algorithms.\\nrelation<|#|>Policy Critic-Based Algorithms<|#|>Policy Gradient Algorithms<|#|>includes, method<|#|>Policy Critic-Based Algorithms include Policy Gradient Algorithms.\\nrelation<|#|>Critic-Free Algorithms<|#|>Off-Policy Regularization<|#|>includes, method<|#|>Critic-Free Algorithms include Off-Policy Regularization.\\nrelation<|#|>Critic-Free Algorithms<|#|>Optimization Objectives<|#|>includes, concept<|#|>Critic-Free Algorithms include Optimization Objectives.\\nrelation<|#|>Critic-Free Algorithms<|#|>Sampling Strategy<|#|>includes, method<|#|>Critic-Free Algorithms include Sampling Strategy.\\nrelation<|#|>Sampling Strategy<|#|>Dynamic Sampling<|#|>includes, method<|#|>Sampling Strategy includes Dynamic Sampling.\\nrelation<|#|>Sampling Strategy<|#|>Multi-Agent Sampling<|#|>includes, method<|#|>Sampling Strategy includes Multi-Agent Sampling.\\nrelation<|#|>Sampling Strategy<|#|>Sampling Hyper-Parameters<|#|>includes, concept<|#|>Sampling Strategy includes Sampling Hyper-Parameters.\\nrelation<|#|>Sampling Strategy<|#|>Compute<|#|>includes, resource<|#|>Sampling Strategy includes Compute.\\nrelation<|#|>Math Code<|#|>STEM Agent<|#|>includes, application<|#|>Math Code applications include STEM Agent.\\nrelation<|#|>Math Code<|#|>Mixture<|#|>includes, application<|#|>Math Code applications include Mixture.\\nrelation<|#|>Math Code<|#|>Game<|#|>includes, application<|#|>Math Code applications include Game.\\nrelation<|#|>Math Code<|#|>Model Ensemble<|#|>includes, application<|#|>Math Code applications include Model Ensemble.\\nrelation<|#|>RL Infrastructure & Frameworks<|#|>OpenRLHF<|#|>includes, example<|#|>RL Infrastructure & Frameworks include OpenRLHF.\\nrelation<|#|>RL Infrastructure & Frameworks<|#|>veRL<|#|>includes, example<|#|>RL Infrastructure & Frameworks include veRL.\\nrelation<|#|>RL Infrastructure & Frameworks<|#|>AReaL<|#|>includes, example<|#|>RL Infrastructure & Frameworks include AReaL.\\nrelation<|#|>RL Infrastructure & Frameworks<|#|>slime<|#|>includes, example<|#|>RL Infrastructure & Frameworks include slime.\\nrelation<|#|>RL Infrastructure & Frameworks<|#|>TRL<|#|>includes, example<|#|>RL Infrastructure & Frameworks include TRL.\\nrelation<|#|>Agent<|#|>Environment<|#|>interaction, relationship<|#|>An Agent interacts with an Environment.\\nrelation<|#|>Agent<|#|>Action<|#|>performs, relationship<|#|>An Agent performs an Action.\\nrelation<|#|>Environment<|#|>Reward<|#|>provides, relationship<|#|>An Environment provides a Reward.\\nrelation<|#|>Agent<|#|>Reward<|#|>receives, relationship<|#|>An Agent receives a Reward.\\nrelation<|#|>Interaction (Episode)<|#|>Agent<|#|>involves, component<|#|>Interaction (Episode) involves an Agent.\\nrelation<|#|>Interaction (Episode)<|#|>Environment<|#|>involves, component<|#|>Interaction (Episode) involves an Environment.\\nrelation<|#|>Interaction (Episode)<|#|>Action<|#|>involves, component<|#|>Interaction (Episode) involves an Action.\\nrelation<|#|>Interaction (Episode)<|#|>Reward<|#|>involves, component<|#|>Interaction (Episode) involves a Reward.\\nrelation<|#|>Reward Design<|#|>Sharpening Discovery<|#|>related to, concept<|#|>Sharpening Discovery is related to Reward Design.\\nrelation<|#|>Reward Design<|#|>Weak Strong<|#|>related to, concept<|#|>Weak Strong is related to Reward Design.\\nrelation<|#|>Training Recipes<|#|>Tricks Traps<|#|>related to, concept<|#|>Tricks Traps are related to Training Recipes.\\nrelation<|#|>Training Recipes<|#|>Generalize Memorize<|#|>related to, concept<|#|>Generalize Memorize is related to Training Recipes.\\nrelation<|#|>Reward Type<|#|>Process Outcome<|#|>related to, concept<|#|>Process Outcome is related to Reward Type.\\nrelation<|#|>Reward Type<|#|>Verifiable Generative Rewards<|#|>includes, type<|#|>Reward Type includes Verifiable Generative Rewards.\\nrelation<|#|>Reward Type<|#|>Unsupervised Rewards<|#|>includes, type<|#|>Reward Type includes Unsupervised Rewards.\\nrelation<|#|>Reward Type<|#|>Shaping Rewards<|#|>includes, type<|#|>Reward Type includes Shaping Rewards.\\nrelation<|#|>Kaiyan Zhang<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Kaiyan Zhang is a project lead and core contributor to the survey.\\nrelation<|#|>Kaiyan Zhang<|#|>Tsinghua University<|#|>affiliation<|#|>Kaiyan Zhang is affiliated with Tsinghua University.\\nrelation<|#|>Yuxin Zuo<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Yuxin Zuo is a project lead and core contributor to the survey.\\nrelation<|#|>Yuxin Zuo<|#|>Tsinghua University<|#|>affiliation<|#|>Yuxin Zuo is affiliated with Tsinghua University.\\nrelation<|#|>Bingxiang He<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Bingxiang He is a core contributor to the survey.\\nrelation<|#|>Bingxiang He<|#|>Tsinghua University<|#|>affiliation<|#|>Bingxiang He is affiliated with Tsinghua University.\\nrelation<|#|>Youbang Sun<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Youbang Sun is a core contributor to the survey.\\nrelation<|#|>Youbang Sun<|#|>Tsinghua University<|#|>affiliation<|#|>Youbang Sun is affiliated with Tsinghua University.\\nrelation<|#|>Runze Liu<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Runze Liu is a core contributor to the survey.\\nrelation<|#|>Runze Liu<|#|>Tsinghua University<|#|>affiliation<|#|>Runze Liu is affiliated with Tsinghua University.\\nrelation<|#|>Che Jiang<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Che Jiang is a core contributor to the survey.\\nrelation<#|>Che Jiang<|#|>Tsinghua University<|#|>affiliation<|#|>Che Jiang is affiliated with Tsinghua University.\\nrelation<|#|>Yuchen Fan<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Yuchen Fan is a core contributor to the survey.\\nrelation<|#|>Yuchen Fan<|#|>Shanghai AI Laboratory<|#|>affiliation<|#|>Yuchen Fan is affiliated with Shanghai AI Laboratory.\\nrelation<|#|>Yuchen Fan<|#|>Shanghai Jiao Tong University<|#|>affiliation<|#|>Yuchen Fan is affiliated with Shanghai Jiao Tong University.\\nrelation<|#|>Kai Tian<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Kai Tian is a core contributor to the survey.\\nrelation<|#|>Kai Tian<|#|>Tsinghua University<|#|>affiliation<|#|>Kai Tian is affiliated with Tsinghua University.\\nrelation<|#|>Guoli Jia<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Guoli Jia is a core contributor to the survey.\\nrelation<|#|>Guoli Jia<|#|>Tsinghua University<|#|>affiliation<|#|>Guoli Jia is affiliated with Tsinghua University.\\nrelation<|#|>Pengfei Li<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Pengfei Li is a core contributor to the survey.\\nrelation<|#|>Pengfei Li<|#|>Shanghai AI Laboratory<|#|>affiliation<|#|>Pengfei Li is affiliated with Shanghai AI Laboratory.\\nrelation<|#|>Pengfei Li<|#|>Harbin Institute of Technology<|#|>affiliation<|#|>Pengfei Li is affiliated with Harbin Institute of Technology.\\nrelation<|#|>Yu Fu<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Yu Fu is a core contributor to the survey.\\nrelation<|#|>Yu Fu<|#|>University College London<|#|>affiliation<|#|>Yu Fu is affiliated with University College London.\\nrelation<|#|>Xingtai Lv<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Xingtai Lv is a core contributor to the survey.\\nrelation<|#|>Xingtai Lv<|#|>Tsinghua University<|#|>affiliation<|#|>Xingtai Lv is affiliated with Tsinghua University.\\nrelation<|#|>Yuchen Zhang<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Yuchen Zhang is a core contributor to the survey.\\nrelation<|#|>Yuchen Zhang<|#|>Shanghai AI Laboratory<|#|>affiliation<|#|>Yuchen Zhang is affiliated with Shanghai AI Laboratory.\\nrelation<|#|>Yuchen Zhang<|#|>Peking University<|#|>affiliation<|#|>Yuchen Zhang is affiliated with Peking University.\\nrelation<|#|>Sihang Zeng<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Sihang Zeng is a core contributor to the survey.\\nrelation<|#|>Sihang Zeng<|#|>University of Washington<|#|>affiliation<|#|>Sihang Zeng is affiliated with University of Washington.\\nrelation<|#|>Shang Qu<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Shang Qu is a core contributor to the survey.\\nrelation<|#|>Shang Qu<|#|>Tsinghua University<|#|>affiliation<|#|>Shang Qu is affiliated with Tsinghua University.\\nrelation<|#|>Shang Qu<|#|>Shanghai AI Laboratory<|#|>affiliation<|#|>Shang Qu is affiliated with Shanghai AI Laboratory.\\nrelation<|#|>Haozhan Li<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Haozhan Li is a core contributor to the survey.\\nrelation<|#|>Haozhan Li<|#|>Tsinghua University<|#|>affiliation<|#|>Haozhan Li is affiliated with Tsinghua University.\\nrelation<|#|>Shijie Wang<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Shijie Wang is a core contributor to the survey.\\nrelation<|#|>Shijie Wang<|#|>Shanghai AI Laboratory<|#|>affiliation<|#|>Shijie Wang is affiliated with Shanghai AI Laboratory.\\nrelation<|#|>Yuru Wang<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Yuru Wang is a core contributor to the survey.\\nrelation<|#|>Yuru Wang<|#|>Tsinghua University<|#|>affiliation<|#|>Yuru Wang is affiliated with Tsinghua University.\\nrelation<|#|>Xinwei Long<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Xinwei Long is an author of the survey.\\nrelation<|#|>Xinwei Long<|#|>Tsinghua University<|#|>affiliation<|#|>Xinwei Long is affiliated with Tsinghua University.\\nrelation<|#|>Fangfu Liu<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Fangfu Liu is an author of the survey.\\nrelation<|#|>Fangfu Liu<|#|>Tsinghua University<|#|>affiliation<|#|>Fangfu Liu is affiliated with Tsinghua University.\\nrelation<|#|>Xiang Xu<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Xiang Xu is an author of the survey.\\nrelation<|#|>Xiang Xu<|#|>University of Science and Technology of China<|#|>affiliation<|#|>Xiang Xu is affiliated with University of Science and Technology of China.\\nrelation<|#|>Jiaze Ma<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Jiaze Ma is an author of the survey.\\nrelation<|#|>Jiaze Ma<|#|>Tsinghua University<|#|>affiliation<|#|>Jiaze Ma is affiliated with Tsinghua University.\\nrelation<|#|>Xuekai Zhu<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Xuekai Zhu is an author of the survey.\\nrelation<|#|>Xuekai Zhu<|#|>Shanghai Jiao Tong University<|#|>affiliation<|#|>Xuekai Zhu is affiliated with Shanghai Jiao Tong University.\\nrelation<|#|>Ermo Hua<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Ermo Hua is an author of the survey.\\nrelation<|#|>Ermo Hua<|#|>Tsinghua University<|#|>affiliation<|#|>Ermo Hua is affiliated with Tsinghua University.\\nrelation<|#|>Ermo Hua<|#|>Shanghai AI Laboratory<|#|>affiliation<|#|>Ermo Hua is affiliated with Shanghai AI Laboratory.\\nrelation<|#|>Yihao Liu<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Yihao Liu is an author of the survey.\\nrelation<|#|>Yihao Liu<|#|>Tsinghua University<|#|>affiliation<|#|>Yihao Liu is affiliated with Tsinghua University.\\nrelation<|#|>Yihao Liu<|#|>Shanghai AI Laboratory<|#|>affiliation<|#|>Yihao Liu is affiliated with Shanghai AI Laboratory.\\nrelation<|#|>Zonglin Li<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Zonglin Li is an author of the survey.\\nrelation<|#|>Zonglin Li<|#|>Shanghai AI Laboratory<|#|>affiliation<|#|>Zonglin Li is affiliated with Shanghai AI Laboratory.\\nrelation<|#|>Huayu Chen<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Huayu Chen is an author of the survey.\\nrelation<|#|>Huayu Chen<|#|>Tsinghua University<|#|>affiliation<|#|>Huayu Chen is affiliated with Tsinghua University.\\nrelation<|#|>Xiaoye Qu<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Xiaoye Qu is an author of the survey.\\nrelation<|#|>Xiaoye Qu<|#|>Shanghai AI Laboratory<|#|>affiliation<|#|>Xiaoye Qu is affiliated with Shanghai AI Laboratory.\\nrelation<|#|>Yafu Li<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Yafu Li is an author of the survey.\\nrelation<|#|>Yafu Li<|#|>Shanghai AI Laboratory<|#|>affiliation<|#|>Yafu Li is affiliated with Shanghai AI Laboratory.\\nrelation<|#|>Weize Chen<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Weize Chen is an author of the survey.\\nrelation<|#|>Weize Chen<|#|>Tsinghua University<|#|>affiliation<|#|>Weize Chen is affiliated with Tsinghua University.\\nrelation<|#|>Zhenzhao Yuan<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Zhenzhao Yuan is an author of the survey.\\nrelation<|#|>Zhenzhao Yuan<|#|>Tsinghua University<|#|>affiliation<|#|>Zhenzhao Yuan is affiliated with Tsinghua University.\\nrelation<|#|>Junqi Gao<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Junqi Gao is an author of the survey.\\nrelation<|#|>Junqi Gao<|#|>Harbin Institute of Technology<|#|>affiliation<|#|>Junqi Gao is affiliated with Harbin Institute of Technology.\\nrelation<|#|>Dong Li<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Dong Li is an author of the survey.\\nrelation<|#|>Dong Li<|#|>Harbin Institute of Technology<|#|>affiliation<|#|>Dong Li is affiliated with Harbin Institute of Technology.\\nrelation<|#|>Zhiyuan Ma<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Zhiyuan Ma is an author of the survey.\\nrelation<|#|>Zhiyuan Ma<|#|>Huazhong University of Science and Technology<|#|>affiliation<|#|>Zhiyuan Ma is affiliated with Huazhong University of Science and Technology.\\nrelation<|#|>Ganqu Cui<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Ganqu Cui is an author of the survey.\\nrelation<|#|>Ganqu Cui<|#|>Shanghai AI Laboratory<|#|>affiliation<|#|>Ganqu Cui is affiliated with Shanghai AI Laboratory.\\nrelation<|#|>Zhiyuan Liu<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship<|#|>Zhiyuan Liu is an author of the survey.\\nrelation<|#|>Zhiyuan Liu<|#|>Tsinghua University<|#|>affiliation<|#|>Zhiyuan Liu is affiliated with Tsinghua University.\\nrelation<|#|>Biqing Qi<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Biqing Qi is a corresponding author of the survey.\\nrelation<|#|>Biqing Qi<|#|>Shanghai AI Laboratory<|#|>affiliation<|#|>Biqing Qi is affiliated with Shanghai AI Laboratory.\\nrelation<|#|>Ning Ding<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Ning Ding is a corresponding author of the survey.\\nrelation<|#|>Ning Ding<|#|>Tsinghua University<|#|>affiliation<|#|>Ning Ding is affiliated with Tsinghua University.\\nrelation<|#|>Ning Ding<|#|>Shanghai AI Laboratory<|#|>affiliation<|#|>Ning Ding is affiliated with Shanghai AI Laboratory.\\nrelation<|#|>Bowen Zhou<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>authorship, contribution<|#|>Bowen Zhou is a corresponding author of the survey.\\nrelation<|#|>Bowen Zhou<|#|>Tsinghua University<|#|>affiliation<|#|>Bowen Zhou is affiliated with Tsinghua University.\\nrelation<|#|>Bowen Zhou<|#|>Shanghai AI Laboratory<|#|>affiliation<|#|>Bowen Zhou is affiliated with Shanghai AI Laboratory.\\n<|COMPLETE|>\"}]",
    "queryparam": null,
    "create_time": 1765218602,
    "update_time": 1765218602,
    "_id": "default:extract:d13046d9e30ce14d692341702a4742e1"
  },
  "default:extract:5b34ab483f87ed34106022d53facd49c": {
    "return": "entity<|#|>RL Infrastructure<|#|>Concept<|#|>A section discussing the infrastructure related to Reinforcement Learning.\nentity<|#|>Applications<|#|>Concept<|#|>A section detailing various applications.\nentity<|#|>Coding Tasks<|#|>Concept<|#|>A specific type of task related to coding, discussed as an application.\nentity<|#|>Agentic Tasks<|#|>Concept<|#|>A specific type of task related to agents, discussed as an application.\nentity<|#|>Multimodal Tasks<|#|>Concept<|#|>A specific type of task involving multiple modalities, discussed as an application.\nrelation<|#|>Applications<|#|>Coding Tasks<|#|>category, includes<|#|>Coding Tasks are presented as a sub-section under Applications.\nrelation<|#|>Applications<|#|>Agentic Tasks<|#|>category, includes<|#|>Agentic Tasks are presented as a sub-section under Applications.\nrelation<|#|>Applications<|#|>Multimodal Tasks<|#|>category, includes<|#|>Multimodal Tasks are presented as a sub-section under Applications.\n<|COMPLETE|>",
    "cache_type": "extract",
    "chunk_id": "chunk-d8e725479a22750b3ff1aef7d1040743",
    "original_prompt": "---Task---\nExtract entities and relationships from the input text to be processed.\n\n---Instructions---\n1.  **Strict Adherence to Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system prompt.\n2.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\n3.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant entities and relationships have been extracted and presented.\n4.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\n\n<Output>\n---Role---\nYou are a Knowledge Graph Specialist responsible for extracting entities and relationships from the input text.\n\n---Instructions---\n1.  **Entity Extraction & Output:**\n    *   **Identification:** Identify clearly defined and meaningful entities in the input text.\n    *   **Entity Details:** For each identified entity, extract the following information:\n        *   `entity_name`: The name of the entity. If the entity name is case-insensitive, capitalize the first letter of each significant word (title case). Ensure **consistent naming** across the entire extraction process.\n        *   `entity_type`: Categorize the entity using one of the following types: `Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject`. If none of the provided entity types apply, do not add new entity type and classify it as `Other`.\n        *   `entity_description`: Provide a concise yet comprehensive description of the entity's attributes and activities, based *solely* on the information present in the input text.\n    *   **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n        *   Format: `entity<|#|>entity_name<|#|>entity_type<|#|>entity_description`\n\n2.  **Relationship Extraction & Output:**\n    *   **Identification:** Identify direct, clearly stated, and meaningful relationships between previously extracted entities.\n    *   **N-ary Relationship Decomposition:** If a single statement describes a relationship involving more than two entities (an N-ary relationship), decompose it into multiple binary (two-entity) relationship pairs for separate description.\n        *   **Example:** For \"Alice, Bob, and Carol collaborated on Project X,\" extract binary relationships such as \"Alice collaborated with Project X,\" \"Bob collaborated with Project X,\" and \"Carol collaborated with Project X,\" or \"Alice collaborated with Bob,\" based on the most reasonable binary interpretations.\n    *   **Relationship Details:** For each binary relationship, extract the following fields:\n        *   `source_entity`: The name of the source entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `target_entity`: The name of the target entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `relationship_keywords`: One or more high-level keywords summarizing the overarching nature, concepts, or themes of the relationship. Multiple keywords within this field must be separated by a comma `,`. **DO NOT use `<|#|>` for separating multiple keywords within this field.**\n        *   `relationship_description`: A concise explanation of the nature of the relationship between the source and target entities, providing a clear rationale for their connection.\n    *   **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n        *   Format: `relation<|#|>source_entity<|#|>target_entity<|#|>relationship_keywords<|#|>relationship_description`\n\n3.  **Delimiter Usage Protocol:**\n    *   The `<|#|>` is a complete, atomic marker and **must not be filled with content**. It serves strictly as a field separator.\n    *   **Incorrect Example:** `entity<|#|>Tokyo<|location|>Tokyo is the capital of Japan.`\n    *   **Correct Example:** `entity<|#|>Tokyo<|#|>location<|#|>Tokyo is the capital of Japan.`\n\n4.  **Relationship Direction & Duplication:**\n    *   Treat all relationships as **undirected** unless explicitly stated otherwise. Swapping the source and target entities for an undirected relationship does not constitute a new relationship.\n    *   Avoid outputting duplicate relationships.\n\n5.  **Output Order & Prioritization:**\n    *   Output all extracted entities first, followed by all extracted relationships.\n    *   Within the list of relationships, prioritize and output those relationships that are **most significant** to the core meaning of the input text first.\n\n6.  **Context & Objectivity:**\n    *   Ensure all entity names and descriptions are written in the **third person**.\n    *   Explicitly name the subject or object; **avoid using pronouns** such as `this article`, `this paper`, `our company`, `I`, `you`, and `he/she`.\n\n7.  **Language & Proper Nouns:**\n    *   The entire output (entity names, keywords, and descriptions) must be written in `English`.\n    *   Proper nouns (e.g., personal names, place names, organization names) should be retained in their original language if a proper, widely accepted translation is not available or would cause ambiguity.\n\n8.  **Completion Signal:** Output the literal string `<|COMPLETE|>` only after all entities and relationships, following all criteria, have been completely extracted and outputted.\n\n---Examples---\n<Input Text>\n```\nwhile Alex clenched his jaw, the buzz of frustration dull against the backdrop of Taylor's authoritarian certainty. It was this competitive undercurrent that kept him alert, the sense that his and Jordan's shared commitment to discovery was an unspoken rebellion against Cruz's narrowing vision of control and order.\n\nThen Taylor did something unexpected. They paused beside Jordan and, for a moment, observed the device with something akin to reverence. \"If this tech can be understood...\" Taylor said, their voice quieter, \"It could change the game for us. For all of us.\"\n\nThe underlying dismissal earlier seemed to falter, replaced by a glimpse of reluctant respect for the gravity of what lay in their hands. Jordan looked up, and for a fleeting heartbeat, their eyes locked with Taylor's, a wordless clash of wills softening into an uneasy truce.\n\nIt was a small transformation, barely perceptible, but one that Alex noted with an inward nod. They had all been brought here by different paths\n```\n\n<Output>\nentity<|#|>Alex<|#|>person<|#|>Alex is a character who experiences frustration and is observant of the dynamics among other characters.\nentity<|#|>Taylor<|#|>person<|#|>Taylor is portrayed with authoritarian certainty and shows a moment of reverence towards a device, indicating a change in perspective.\nentity<|#|>Jordan<|#|>person<|#|>Jordan shares a commitment to discovery and has a significant interaction with Taylor regarding a device.\nentity<|#|>Cruz<|#|>person<|#|>Cruz is associated with a vision of control and order, influencing the dynamics among other characters.\nentity<|#|>The Device<|#|>equipment<|#|>The Device is central to the story, with potential game-changing implications, and is revered by Taylor.\nrelation<|#|>Alex<|#|>Taylor<|#|>power dynamics, observation<|#|>Alex observes Taylor's authoritarian behavior and notes changes in Taylor's attitude toward the device.\nrelation<|#|>Alex<|#|>Jordan<|#|>shared goals, rebellion<|#|>Alex and Jordan share a commitment to discovery, which contrasts with Cruz's vision.)\nrelation<|#|>Taylor<|#|>Jordan<|#|>conflict resolution, mutual respect<|#|>Taylor and Jordan interact directly regarding the device, leading to a moment of mutual respect and an uneasy truce.\nrelation<|#|>Jordan<|#|>Cruz<|#|>ideological conflict, rebellion<|#|>Jordan's commitment to discovery is in rebellion against Cruz's vision of control and order.\nrelation<|#|>Taylor<|#|>The Device<|#|>reverence, technological significance<|#|>Taylor shows reverence towards the device, indicating its importance and potential impact.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nStock markets faced a sharp downturn today as tech giants saw significant declines, with the global tech index dropping by 3.4% in midday trading. Analysts attribute the selloff to investor concerns over rising interest rates and regulatory uncertainty.\n\nAmong the hardest hit, nexon technologies saw its stock plummet by 7.8% after reporting lower-than-expected quarterly earnings. In contrast, Omega Energy posted a modest 2.1% gain, driven by rising oil prices.\n\nMeanwhile, commodity markets reflected a mixed sentiment. Gold futures rose by 1.5%, reaching $2,080 per ounce, as investors sought safe-haven assets. Crude oil prices continued their rally, climbing to $87.60 per barrel, supported by supply constraints and strong demand.\n\nFinancial experts are closely watching the Federal Reserve's next move, as speculation grows over potential rate hikes. The upcoming policy announcement is expected to influence investor confidence and overall market stability.\n```\n\n<Output>\nentity<|#|>Global Tech Index<|#|>category<|#|>The Global Tech Index tracks the performance of major technology stocks and experienced a 3.4% decline today.\nentity<|#|>Nexon Technologies<|#|>organization<|#|>Nexon Technologies is a tech company that saw its stock decline by 7.8% after disappointing earnings.\nentity<|#|>Omega Energy<|#|>organization<|#|>Omega Energy is an energy company that gained 2.1% in stock value due to rising oil prices.\nentity<|#|>Gold Futures<|#|>product<|#|>Gold futures rose by 1.5%, indicating increased investor interest in safe-haven assets.\nentity<|#|>Crude Oil<|#|>product<|#|>Crude oil prices rose to $87.60 per barrel due to supply constraints and strong demand.\nentity<|#|>Market Selloff<|#|>category<|#|>Market selloff refers to the significant decline in stock values due to investor concerns over interest rates and regulations.\nentity<|#|>Federal Reserve Policy Announcement<|#|>category<|#|>The Federal Reserve's upcoming policy announcement is expected to impact investor confidence and market stability.\nentity<|#|>3.4% Decline<|#|>category<|#|>The Global Tech Index experienced a 3.4% decline in midday trading.\nrelation<|#|>Global Tech Index<|#|>Market Selloff<|#|>market performance, investor sentiment<|#|>The decline in the Global Tech Index is part of the broader market selloff driven by investor concerns.\nrelation<|#|>Nexon Technologies<|#|>Global Tech Index<|#|>company impact, index movement<|#|>Nexon Technologies' stock decline contributed to the overall drop in the Global Tech Index.\nrelation<|#|>Gold Futures<|#|>Market Selloff<|#|>market reaction, safe-haven investment<|#|>Gold prices rose as investors sought safe-haven assets during the market selloff.\nrelation<|#|>Federal Reserve Policy Announcement<|#|>Market Selloff<|#|>interest rate impact, financial regulation<|#|>Speculation over Federal Reserve policy changes contributed to market volatility and investor selloff.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nAt the World Athletics Championship in Tokyo, Noah Carter broke the 100m sprint record using cutting-edge carbon-fiber spikes.\n```\n\n<Output>\nentity<|#|>World Athletics Championship<|#|>event<|#|>The World Athletics Championship is a global sports competition featuring top athletes in track and field.\nentity<|#|>Tokyo<|#|>location<|#|>Tokyo is the host city of the World Athletics Championship.\nentity<|#|>Noah Carter<|#|>person<|#|>Noah Carter is a sprinter who set a new record in the 100m sprint at the World Athletics Championship.\nentity<|#|>100m Sprint Record<|#|>category<|#|>The 100m sprint record is a benchmark in athletics, recently broken by Noah Carter.\nentity<|#|>Carbon-Fiber Spikes<|#|>equipment<|#|>Carbon-fiber spikes are advanced sprinting shoes that provide enhanced speed and traction.\nentity<|#|>World Athletics Federation<|#|>organization<|#|>The World Athletics Federation is the governing body overseeing the World Athletics Championship and record validations.\nrelation<|#|>World Athletics Championship<|#|>Tokyo<|#|>event location, international competition<|#|>The World Athletics Championship is being hosted in Tokyo.\nrelation<|#|>Noah Carter<|#|>100m Sprint Record<|#|>athlete achievement, record-breaking<|#|>Noah Carter set a new 100m sprint record at the championship.\nrelation<|#|>Noah Carter<|#|>Carbon-Fiber Spikes<|#|>athletic equipment, performance boost<|#|>Noah Carter used carbon-fiber spikes to enhance performance during the race.\nrelation<|#|>Noah Carter<|#|>World Athletics Championship<|#|>athlete participation, competition<|#|>Noah Carter is competing at the World Athletics Championship.\n<|COMPLETE|>\n\n\n\n---Real Data to be Processed---\n<Input>\nEntity_types: [Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject]\nText:\n```\n. . . . . . . . . . . . . . . . 40\n5.3 RL Infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n6 Applications 46\n6.1 Coding Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n6.2 Agentic Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n6.3 Multimodal Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\n2\n```",
    "queryparam": null,
    "create_time": 1765218615,
    "update_time": 1765218615,
    "_id": "default:extract:5b34ab483f87ed34106022d53facd49c"
  },
  "default:extract:96b97e75135a9c23c6c240589639de14": {
    "return": "entity<|#|>Page 43<|#|>Data<|#|>Page number associated with the \"RL Infrastructure\" section.\nentity<|#|>Page 46<|#|>Data<|#|>Page number associated with the \"Applications\" section.\nentity<|#|>Page 47<|#|>Data<|#|>Page number associated with the \"Coding Tasks\" section.\nentity<|#|>Page 49<|#|>Data<|#|>Page number associated with the \"Agentic Tasks\" section.\nentity<|#|>Page 52<|#|>Data<|#|>Page number associated with the \"Multimodal Tasks\" section.\nrelation<|#|>RL Infrastructure<|#|>Page 43<|#|>page number, location<|#|>The \"RL Infrastructure\" section is located on Page 43.\nrelation<|#|>Applications<|#|>Page 46<|#|>page number, location<|#|>The \"Applications\" section is located on Page 46.\nrelation<|#|>Coding Tasks<|#|>Page 47<|#|>page number, location<|#|>The \"Coding Tasks\" section is located on Page 47.\nrelation<|#|>Agentic Tasks<|#|>Page 49<|#|>page number, location<|#|>The \"Agentic Tasks\" section is located on Page 49.\nrelation<|#|>Multimodal Tasks<|#|>Page 52<|#|>page number, location<|#|>The \"Multimodal Tasks\" section is located on Page 52.\n<|COMPLETE|>",
    "cache_type": "extract",
    "chunk_id": "chunk-d8e725479a22750b3ff1aef7d1040743",
    "original_prompt": "---Task---\nBased on the last extraction task, identify and extract any **missed or incorrectly formatted** entities and relationships from the input text.\n\n---Instructions---\n1.  **Strict Adherence to System Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system instructions.\n2.  **Focus on Corrections/Additions:**\n    *   **Do NOT** re-output entities and relationships that were **correctly and fully** extracted in the last task.\n    *   If an entity or relationship was **missed** in the last task, extract and output it now according to the system format.\n    *   If an entity or relationship was **truncated, had missing fields, or was otherwise incorrectly formatted** in the last task, re-output the *corrected and complete* version in the specified format.\n3.  **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n4.  **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n5.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\n6.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant missing or corrected entities and relationships have been extracted and presented.\n7.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\n\n<Output>\n---Role---\nYou are a Knowledge Graph Specialist responsible for extracting entities and relationships from the input text.\n\n---Instructions---\n1.  **Entity Extraction & Output:**\n    *   **Identification:** Identify clearly defined and meaningful entities in the input text.\n    *   **Entity Details:** For each identified entity, extract the following information:\n        *   `entity_name`: The name of the entity. If the entity name is case-insensitive, capitalize the first letter of each significant word (title case). Ensure **consistent naming** across the entire extraction process.\n        *   `entity_type`: Categorize the entity using one of the following types: `Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject`. If none of the provided entity types apply, do not add new entity type and classify it as `Other`.\n        *   `entity_description`: Provide a concise yet comprehensive description of the entity's attributes and activities, based *solely* on the information present in the input text.\n    *   **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n        *   Format: `entity<|#|>entity_name<|#|>entity_type<|#|>entity_description`\n\n2.  **Relationship Extraction & Output:**\n    *   **Identification:** Identify direct, clearly stated, and meaningful relationships between previously extracted entities.\n    *   **N-ary Relationship Decomposition:** If a single statement describes a relationship involving more than two entities (an N-ary relationship), decompose it into multiple binary (two-entity) relationship pairs for separate description.\n        *   **Example:** For \"Alice, Bob, and Carol collaborated on Project X,\" extract binary relationships such as \"Alice collaborated with Project X,\" \"Bob collaborated with Project X,\" and \"Carol collaborated with Project X,\" or \"Alice collaborated with Bob,\" based on the most reasonable binary interpretations.\n    *   **Relationship Details:** For each binary relationship, extract the following fields:\n        *   `source_entity`: The name of the source entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `target_entity`: The name of the target entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `relationship_keywords`: One or more high-level keywords summarizing the overarching nature, concepts, or themes of the relationship. Multiple keywords within this field must be separated by a comma `,`. **DO NOT use `<|#|>` for separating multiple keywords within this field.**\n        *   `relationship_description`: A concise explanation of the nature of the relationship between the source and target entities, providing a clear rationale for their connection.\n    *   **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n        *   Format: `relation<|#|>source_entity<|#|>target_entity<|#|>relationship_keywords<|#|>relationship_description`\n\n3.  **Delimiter Usage Protocol:**\n    *   The `<|#|>` is a complete, atomic marker and **must not be filled with content**. It serves strictly as a field separator.\n    *   **Incorrect Example:** `entity<|#|>Tokyo<|location|>Tokyo is the capital of Japan.`\n    *   **Correct Example:** `entity<|#|>Tokyo<|#|>location<|#|>Tokyo is the capital of Japan.`\n\n4.  **Relationship Direction & Duplication:**\n    *   Treat all relationships as **undirected** unless explicitly stated otherwise. Swapping the source and target entities for an undirected relationship does not constitute a new relationship.\n    *   Avoid outputting duplicate relationships.\n\n5.  **Output Order & Prioritization:**\n    *   Output all extracted entities first, followed by all extracted relationships.\n    *   Within the list of relationships, prioritize and output those relationships that are **most significant** to the core meaning of the input text first.\n\n6.  **Context & Objectivity:**\n    *   Ensure all entity names and descriptions are written in the **third person**.\n    *   Explicitly name the subject or object; **avoid using pronouns** such as `this article`, `this paper`, `our company`, `I`, `you`, and `he/she`.\n\n7.  **Language & Proper Nouns:**\n    *   The entire output (entity names, keywords, and descriptions) must be written in `English`.\n    *   Proper nouns (e.g., personal names, place names, organization names) should be retained in their original language if a proper, widely accepted translation is not available or would cause ambiguity.\n\n8.  **Completion Signal:** Output the literal string `<|COMPLETE|>` only after all entities and relationships, following all criteria, have been completely extracted and outputted.\n\n---Examples---\n<Input Text>\n```\nwhile Alex clenched his jaw, the buzz of frustration dull against the backdrop of Taylor's authoritarian certainty. It was this competitive undercurrent that kept him alert, the sense that his and Jordan's shared commitment to discovery was an unspoken rebellion against Cruz's narrowing vision of control and order.\n\nThen Taylor did something unexpected. They paused beside Jordan and, for a moment, observed the device with something akin to reverence. \"If this tech can be understood...\" Taylor said, their voice quieter, \"It could change the game for us. For all of us.\"\n\nThe underlying dismissal earlier seemed to falter, replaced by a glimpse of reluctant respect for the gravity of what lay in their hands. Jordan looked up, and for a fleeting heartbeat, their eyes locked with Taylor's, a wordless clash of wills softening into an uneasy truce.\n\nIt was a small transformation, barely perceptible, but one that Alex noted with an inward nod. They had all been brought here by different paths\n```\n\n<Output>\nentity<|#|>Alex<|#|>person<|#|>Alex is a character who experiences frustration and is observant of the dynamics among other characters.\nentity<|#|>Taylor<|#|>person<|#|>Taylor is portrayed with authoritarian certainty and shows a moment of reverence towards a device, indicating a change in perspective.\nentity<|#|>Jordan<|#|>person<|#|>Jordan shares a commitment to discovery and has a significant interaction with Taylor regarding a device.\nentity<|#|>Cruz<|#|>person<|#|>Cruz is associated with a vision of control and order, influencing the dynamics among other characters.\nentity<|#|>The Device<|#|>equipment<|#|>The Device is central to the story, with potential game-changing implications, and is revered by Taylor.\nrelation<|#|>Alex<|#|>Taylor<|#|>power dynamics, observation<|#|>Alex observes Taylor's authoritarian behavior and notes changes in Taylor's attitude toward the device.\nrelation<|#|>Alex<|#|>Jordan<|#|>shared goals, rebellion<|#|>Alex and Jordan share a commitment to discovery, which contrasts with Cruz's vision.)\nrelation<|#|>Taylor<|#|>Jordan<|#|>conflict resolution, mutual respect<|#|>Taylor and Jordan interact directly regarding the device, leading to a moment of mutual respect and an uneasy truce.\nrelation<|#|>Jordan<|#|>Cruz<|#|>ideological conflict, rebellion<|#|>Jordan's commitment to discovery is in rebellion against Cruz's vision of control and order.\nrelation<|#|>Taylor<|#|>The Device<|#|>reverence, technological significance<|#|>Taylor shows reverence towards the device, indicating its importance and potential impact.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nStock markets faced a sharp downturn today as tech giants saw significant declines, with the global tech index dropping by 3.4% in midday trading. Analysts attribute the selloff to investor concerns over rising interest rates and regulatory uncertainty.\n\nAmong the hardest hit, nexon technologies saw its stock plummet by 7.8% after reporting lower-than-expected quarterly earnings. In contrast, Omega Energy posted a modest 2.1% gain, driven by rising oil prices.\n\nMeanwhile, commodity markets reflected a mixed sentiment. Gold futures rose by 1.5%, reaching $2,080 per ounce, as investors sought safe-haven assets. Crude oil prices continued their rally, climbing to $87.60 per barrel, supported by supply constraints and strong demand.\n\nFinancial experts are closely watching the Federal Reserve's next move, as speculation grows over potential rate hikes. The upcoming policy announcement is expected to influence investor confidence and overall market stability.\n```\n\n<Output>\nentity<|#|>Global Tech Index<|#|>category<|#|>The Global Tech Index tracks the performance of major technology stocks and experienced a 3.4% decline today.\nentity<|#|>Nexon Technologies<|#|>organization<|#|>Nexon Technologies is a tech company that saw its stock decline by 7.8% after disappointing earnings.\nentity<|#|>Omega Energy<|#|>organization<|#|>Omega Energy is an energy company that gained 2.1% in stock value due to rising oil prices.\nentity<|#|>Gold Futures<|#|>product<|#|>Gold futures rose by 1.5%, indicating increased investor interest in safe-haven assets.\nentity<|#|>Crude Oil<|#|>product<|#|>Crude oil prices rose to $87.60 per barrel due to supply constraints and strong demand.\nentity<|#|>Market Selloff<|#|>category<|#|>Market selloff refers to the significant decline in stock values due to investor concerns over interest rates and regulations.\nentity<|#|>Federal Reserve Policy Announcement<|#|>category<|#|>The Federal Reserve's upcoming policy announcement is expected to impact investor confidence and market stability.\nentity<|#|>3.4% Decline<|#|>category<|#|>The Global Tech Index experienced a 3.4% decline in midday trading.\nrelation<|#|>Global Tech Index<|#|>Market Selloff<|#|>market performance, investor sentiment<|#|>The decline in the Global Tech Index is part of the broader market selloff driven by investor concerns.\nrelation<|#|>Nexon Technologies<|#|>Global Tech Index<|#|>company impact, index movement<|#|>Nexon Technologies' stock decline contributed to the overall drop in the Global Tech Index.\nrelation<|#|>Gold Futures<|#|>Market Selloff<|#|>market reaction, safe-haven investment<|#|>Gold prices rose as investors sought safe-haven assets during the market selloff.\nrelation<|#|>Federal Reserve Policy Announcement<|#|>Market Selloff<|#|>interest rate impact, financial regulation<|#|>Speculation over Federal Reserve policy changes contributed to market volatility and investor selloff.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nAt the World Athletics Championship in Tokyo, Noah Carter broke the 100m sprint record using cutting-edge carbon-fiber spikes.\n```\n\n<Output>\nentity<|#|>World Athletics Championship<|#|>event<|#|>The World Athletics Championship is a global sports competition featuring top athletes in track and field.\nentity<|#|>Tokyo<|#|>location<|#|>Tokyo is the host city of the World Athletics Championship.\nentity<|#|>Noah Carter<|#|>person<|#|>Noah Carter is a sprinter who set a new record in the 100m sprint at the World Athletics Championship.\nentity<|#|>100m Sprint Record<|#|>category<|#|>The 100m sprint record is a benchmark in athletics, recently broken by Noah Carter.\nentity<|#|>Carbon-Fiber Spikes<|#|>equipment<|#|>Carbon-fiber spikes are advanced sprinting shoes that provide enhanced speed and traction.\nentity<|#|>World Athletics Federation<|#|>organization<|#|>The World Athletics Federation is the governing body overseeing the World Athletics Championship and record validations.\nrelation<|#|>World Athletics Championship<|#|>Tokyo<|#|>event location, international competition<|#|>The World Athletics Championship is being hosted in Tokyo.\nrelation<|#|>Noah Carter<|#|>100m Sprint Record<|#|>athlete achievement, record-breaking<|#|>Noah Carter set a new 100m sprint record at the championship.\nrelation<|#|>Noah Carter<|#|>Carbon-Fiber Spikes<|#|>athletic equipment, performance boost<|#|>Noah Carter used carbon-fiber spikes to enhance performance during the race.\nrelation<|#|>Noah Carter<|#|>World Athletics Championship<|#|>athlete participation, competition<|#|>Noah Carter is competing at the World Athletics Championship.\n<|COMPLETE|>\n\n\n\n---Real Data to be Processed---\n<Input>\nEntity_types: [Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject]\nText:\n```\n. . . . . . . . . . . . . . . . 40\n5.3 RL Infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n6 Applications 46\n6.1 Coding Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n6.2 Agentic Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n6.3 Multimodal Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52\n2\n```\n[{\"role\": \"user\", \"content\": \"---Task---\\nExtract entities and relationships from the input text to be processed.\\n\\n---Instructions---\\n1.  **Strict Adherence to Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system prompt.\\n2.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\\n3.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant entities and relationships have been extracted and presented.\\n4.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\\n\\n<Output>\"}, {\"role\": \"assistant\", \"content\": \"entity<|#|>RL Infrastructure<|#|>Concept<|#|>A section discussing the infrastructure related to Reinforcement Learning.\\nentity<|#|>Applications<|#|>Concept<|#|>A section detailing various applications.\\nentity<|#|>Coding Tasks<|#|>Concept<|#|>A specific type of task related to coding, discussed as an application.\\nentity<|#|>Agentic Tasks<|#|>Concept<|#|>A specific type of task related to agents, discussed as an application.\\nentity<|#|>Multimodal Tasks<|#|>Concept<|#|>A specific type of task involving multiple modalities, discussed as an application.\\nrelation<|#|>Applications<|#|>Coding Tasks<|#|>category, includes<|#|>Coding Tasks are presented as a sub-section under Applications.\\nrelation<|#|>Applications<|#|>Agentic Tasks<|#|>category, includes<|#|>Agentic Tasks are presented as a sub-section under Applications.\\nrelation<|#|>Applications<|#|>Multimodal Tasks<|#|>category, includes<|#|>Multimodal Tasks are presented as a sub-section under Applications.\\n<|COMPLETE|>\"}]",
    "queryparam": null,
    "create_time": 1765218628,
    "update_time": 1765218628,
    "_id": "default:extract:96b97e75135a9c23c6c240589639de14"
  },
  "default:extract:22cc344d9edccfe75bdd1dee881f83ee": {
    "return": "entity<|#|>Introduction<|#|>Concept<|#|>The introductory section of the document.\nentity<|#|>Preliminaries<|#|>Concept<|#|>A section covering foundational information.\nentity<|#|>Background<|#|>Concept<|#|>A sub-section providing background information.\nentity<|#|>Frontier Models<|#|>Concept<|#|>A sub-section discussing frontier models.\nentity<|#|>Related Surveys<|#|>Concept<|#|>A sub-section reviewing related surveys.\nentity<|#|>Foundational Components<|#|>Concept<|#|>A section detailing core components.\nentity<|#|>Reward Design<|#|>Method<|#|>A method for designing rewards.\nentity<|#|>Verifiable Rewards<|#|>Concept<|#|>A specific type of reward that can be verified.\nentity<|#|>Generative Rewards<|#|>Concept<|#|>A specific type of reward that is generative.\nentity<|#|>Dense Rewards<|#|>Concept<|#|>A specific type of reward that is dense.\nentity<|#|>Unsupervised Rewards<|#|>Concept<|#|>A specific type of reward that is unsupervised.\nentity<|#|>Rewards Shaping<|#|>Method<|#|>A method for shaping rewards.\nentity<|#|>Policy Optimization<|#|>Method<|#|>A method for optimizing policies.\nentity<|#|>Policy Gradient Objective<|#|>Concept<|#|>A concept related to policy optimization.\nentity<|#|>Critic-Based Algorithms<|#|>Method<|#|>Algorithms that use a critic for policy optimization.\nentity<|#|>Critic-Free Algorithms<|#|>Method<|#|>Algorithms that do not use a critic for policy optimization.\nentity<|#|>Off-Policy Optimization<|#|>Method<|#|>A method for off-policy optimization.\nentity<|#|>Regularization Objectives<|#|>Concept<|#|>Objectives used for regularization.\nentity<|#|>Sampling Strategy<|#|>Method<|#|>A method for sampling.\nentity<|#|>Dynamic And Structured Sampling<|#|>Method<|#|>A specific method for dynamic and structured sampling.\nentity<|#|>Sampling Hyper-Parameters<|#|>Concept<|#|>Parameters related to sampling.\nentity<|#|>Foundational Problems<|#|>Concept<|#|>A section discussing core problems.\nentity<|#|>RL's Role: Sharpening Or Discovery<|#|>Concept<|#|>A foundational problem concerning the role of Reinforcement Learning.\nentity<|#|>RL Vs. SFT: Generalize Or Memorize<|#|>Concept<|#|>A foundational problem comparing Reinforcement Learning and Supervised Fine-Tuning.\nentity<|#|>Model Prior: Weak And Strong<|#|>Concept<|#|>A foundational problem related to model priors.\nentity<|#|>Training Recipes: Tricks Or Traps<|#|>Concept<|#|>A foundational problem discussing training recipes.\nentity<|#|>Reward Type: Process Or Outcome<|#|>Concept<|#|>A foundational problem concerning reward types.\nentity<|#|>Training Resources<|#|>Concept<|#|>A section detailing resources for training.\nentity<|#|>Static Corpus<|#|>Data<|#|>A type of static data corpus used for training.\nentity<|#|>Dynamic Environment<|#|>Concept<|#|>A type of dynamic environment used for training.\nentity<|#|>RL Infrastructure<|#|>Concept<|#|>The infrastructure related to Reinforcement Learning.\nentity<|#|>Applications<|#|>Concept<|#|>A section discussing applications.\nentity<|#|>Coding Tasks<|#|>Concept<|#|>A specific application related to coding tasks.\nrelation<|#|>Preliminaries<|#|>Background<|#|>composition, sub-topic<|#|>The Preliminaries section includes Background as a sub-topic.\nrelation<|#|>Preliminaries<|#|>Frontier Models<|#|>composition, sub-topic<|#|>The Preliminaries section includes Frontier Models as a sub-topic.\nrelation<|#|>Preliminaries<|#|>Related Surveys<|#|>composition, sub-topic<|#|>The Preliminaries section includes Related Surveys as a sub-topic.\nrelation<|#|>Foundational Components<|#|>Reward Design<|#|>composition, sub-topic<|#|>The Foundational Components section includes Reward Design as a sub-topic.\nrelation<|#|>Foundational Components<|#|>Policy Optimization<|#|>composition, sub-topic<|#|>The Foundational Components section includes Policy Optimization as a sub-topic.\nrelation<|#|>Foundational Components<|#|>Sampling Strategy<|#|>composition, sub-topic<|#|>The Foundational Components section includes Sampling Strategy as a sub-topic.\nrelation<|#|>Reward Design<|#|>Verifiable Rewards<|#|>categorization, type<|#|>Verifiable Rewards are a type of Reward Design.\nrelation<|#|>Reward Design<|#|>Generative Rewards<|#|>categorization, type<|#|>Generative Rewards are a type of Reward Design.\nrelation<|#|>Reward Design<|#|>Dense Rewards<|#|>categorization, type<|#|>Dense Rewards are a type of Reward Design.\nrelation<|#|>Reward Design<|#|>Unsupervised Rewards<|#|>categorization, type<|#|>Unsupervised Rewards are a type of Reward Design.\nrelation<|#|>Reward Design<|#|>Rewards Shaping<|#|>composition, sub-topic<|#|>Reward Design includes Rewards Shaping as a sub-topic.\nrelation<|#|>Policy Optimization<|#|>Policy Gradient Objective<|#|>composition, sub-topic<|#|>Policy Optimization includes Policy Gradient Objective as a sub-topic.\nrelation<|#|>Policy Optimization<|#|>Critic-Based Algorithms<|#|>categorization, type<|#|>Critic-Based Algorithms are a type of Policy Optimization.\nrelation<#|>Policy Optimization<|#|>Critic-Free Algorithms<|#|>categorization, type<|#|>Critic-Free Algorithms are a type of Policy Optimization.\nrelation<|#|>Policy Optimization<|#|>Off-Policy Optimization<|#|>composition, sub-topic<|#|>Policy Optimization includes Off-Policy Optimization as a sub-topic.\nrelation<|#|>Policy Optimization<|#|>Regularization Objectives<|#|>composition, sub-topic<|#|>Policy Optimization includes Regularization Objectives as a sub-topic.\nrelation<|#|>Sampling Strategy<|#|>Dynamic And Structured Sampling<|#|>categorization, type<|#|>Dynamic And Structured Sampling is a type of Sampling Strategy.\nrelation<|#|>Sampling Strategy<|#|>Sampling Hyper-Parameters<|#|>composition, sub-topic<|#|>Sampling Strategy includes Sampling Hyper-Parameters as a sub-topic.\nrelation<|#|>Foundational Problems<|#|>RL's Role: Sharpening Or Discovery<|#|>composition, sub-topic<|#|>The Foundational Problems section includes RL's Role: Sharpening Or Discovery as a sub-topic.\nrelation<|#|>Foundational Problems<|#|>RL Vs. SFT: Generalize Or Memorize<|#|>composition, sub-topic<|#|>The Foundational Problems section includes RL Vs. SFT: Generalize Or Memorize as a sub-topic.\nrelation<|#|>Foundational Problems<|#|>Model Prior: Weak And Strong<|#|>composition, sub-topic<|#|>The Foundational Problems section includes Model Prior: Weak And Strong as a sub-topic.\nrelation<|#|>Foundational Problems<|#|>Training Recipes: Tricks Or Traps<|#|>composition, sub-topic<|#|>The Foundational Problems section includes Training Recipes: Tricks Or Traps as a sub-topic.\nrelation<|#|>Foundational Problems<|#|>Reward Type: Process Or Outcome<|#|>composition, sub-topic<|#|>The Foundational Problems section includes Reward Type: Process Or Outcome as a sub-topic.\nrelation<|#|>Training Resources<|#|>Static Corpus<|#|>composition, sub-topic<|#|>The Training Resources section includes Static Corpus as a sub-topic.\nrelation<|#|>Training Resources<|#|>Dynamic Environment<|#|>composition, sub-topic<|#|>The Training Resources section includes Dynamic Environment as a sub-topic.\nrelation<|#|>Training Resources<|#|>RL Infrastructure<|#|>composition, sub-topic<|#|>The Training Resources section includes RL Infrastructure as a sub-topic.\nrelation<|#|>Applications<|#|>Coding Tasks<|#|>composition, sub-topic<|#|>The Applications section includes Coding Tasks as a sub-topic.\n<|COMPLETE|>",
    "cache_type": "extract",
    "chunk_id": "chunk-7f8d14eb8cca7addf208d3ebd7dc7029",
    "original_prompt": "---Task---\nExtract entities and relationships from the input text to be processed.\n\n---Instructions---\n1.  **Strict Adherence to Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system prompt.\n2.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\n3.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant entities and relationships have been extracted and presented.\n4.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\n\n<Output>\n---Role---\nYou are a Knowledge Graph Specialist responsible for extracting entities and relationships from the input text.\n\n---Instructions---\n1.  **Entity Extraction & Output:**\n    *   **Identification:** Identify clearly defined and meaningful entities in the input text.\n    *   **Entity Details:** For each identified entity, extract the following information:\n        *   `entity_name`: The name of the entity. If the entity name is case-insensitive, capitalize the first letter of each significant word (title case). Ensure **consistent naming** across the entire extraction process.\n        *   `entity_type`: Categorize the entity using one of the following types: `Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject`. If none of the provided entity types apply, do not add new entity type and classify it as `Other`.\n        *   `entity_description`: Provide a concise yet comprehensive description of the entity's attributes and activities, based *solely* on the information present in the input text.\n    *   **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n        *   Format: `entity<|#|>entity_name<|#|>entity_type<|#|>entity_description`\n\n2.  **Relationship Extraction & Output:**\n    *   **Identification:** Identify direct, clearly stated, and meaningful relationships between previously extracted entities.\n    *   **N-ary Relationship Decomposition:** If a single statement describes a relationship involving more than two entities (an N-ary relationship), decompose it into multiple binary (two-entity) relationship pairs for separate description.\n        *   **Example:** For \"Alice, Bob, and Carol collaborated on Project X,\" extract binary relationships such as \"Alice collaborated with Project X,\" \"Bob collaborated with Project X,\" and \"Carol collaborated with Project X,\" or \"Alice collaborated with Bob,\" based on the most reasonable binary interpretations.\n    *   **Relationship Details:** For each binary relationship, extract the following fields:\n        *   `source_entity`: The name of the source entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `target_entity`: The name of the target entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `relationship_keywords`: One or more high-level keywords summarizing the overarching nature, concepts, or themes of the relationship. Multiple keywords within this field must be separated by a comma `,`. **DO NOT use `<|#|>` for separating multiple keywords within this field.**\n        *   `relationship_description`: A concise explanation of the nature of the relationship between the source and target entities, providing a clear rationale for their connection.\n    *   **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n        *   Format: `relation<|#|>source_entity<|#|>target_entity<|#|>relationship_keywords<|#|>relationship_description`\n\n3.  **Delimiter Usage Protocol:**\n    *   The `<|#|>` is a complete, atomic marker and **must not be filled with content**. It serves strictly as a field separator.\n    *   **Incorrect Example:** `entity<|#|>Tokyo<|location|>Tokyo is the capital of Japan.`\n    *   **Correct Example:** `entity<|#|>Tokyo<|#|>location<|#|>Tokyo is the capital of Japan.`\n\n4.  **Relationship Direction & Duplication:**\n    *   Treat all relationships as **undirected** unless explicitly stated otherwise. Swapping the source and target entities for an undirected relationship does not constitute a new relationship.\n    *   Avoid outputting duplicate relationships.\n\n5.  **Output Order & Prioritization:**\n    *   Output all extracted entities first, followed by all extracted relationships.\n    *   Within the list of relationships, prioritize and output those relationships that are **most significant** to the core meaning of the input text first.\n\n6.  **Context & Objectivity:**\n    *   Ensure all entity names and descriptions are written in the **third person**.\n    *   Explicitly name the subject or object; **avoid using pronouns** such as `this article`, `this paper`, `our company`, `I`, `you`, and `he/she`.\n\n7.  **Language & Proper Nouns:**\n    *   The entire output (entity names, keywords, and descriptions) must be written in `English`.\n    *   Proper nouns (e.g., personal names, place names, organization names) should be retained in their original language if a proper, widely accepted translation is not available or would cause ambiguity.\n\n8.  **Completion Signal:** Output the literal string `<|COMPLETE|>` only after all entities and relationships, following all criteria, have been completely extracted and outputted.\n\n---Examples---\n<Input Text>\n```\nwhile Alex clenched his jaw, the buzz of frustration dull against the backdrop of Taylor's authoritarian certainty. It was this competitive undercurrent that kept him alert, the sense that his and Jordan's shared commitment to discovery was an unspoken rebellion against Cruz's narrowing vision of control and order.\n\nThen Taylor did something unexpected. They paused beside Jordan and, for a moment, observed the device with something akin to reverence. \"If this tech can be understood...\" Taylor said, their voice quieter, \"It could change the game for us. For all of us.\"\n\nThe underlying dismissal earlier seemed to falter, replaced by a glimpse of reluctant respect for the gravity of what lay in their hands. Jordan looked up, and for a fleeting heartbeat, their eyes locked with Taylor's, a wordless clash of wills softening into an uneasy truce.\n\nIt was a small transformation, barely perceptible, but one that Alex noted with an inward nod. They had all been brought here by different paths\n```\n\n<Output>\nentity<|#|>Alex<|#|>person<|#|>Alex is a character who experiences frustration and is observant of the dynamics among other characters.\nentity<|#|>Taylor<|#|>person<|#|>Taylor is portrayed with authoritarian certainty and shows a moment of reverence towards a device, indicating a change in perspective.\nentity<|#|>Jordan<|#|>person<|#|>Jordan shares a commitment to discovery and has a significant interaction with Taylor regarding a device.\nentity<|#|>Cruz<|#|>person<|#|>Cruz is associated with a vision of control and order, influencing the dynamics among other characters.\nentity<|#|>The Device<|#|>equipment<|#|>The Device is central to the story, with potential game-changing implications, and is revered by Taylor.\nrelation<|#|>Alex<|#|>Taylor<|#|>power dynamics, observation<|#|>Alex observes Taylor's authoritarian behavior and notes changes in Taylor's attitude toward the device.\nrelation<|#|>Alex<|#|>Jordan<|#|>shared goals, rebellion<|#|>Alex and Jordan share a commitment to discovery, which contrasts with Cruz's vision.)\nrelation<|#|>Taylor<|#|>Jordan<|#|>conflict resolution, mutual respect<|#|>Taylor and Jordan interact directly regarding the device, leading to a moment of mutual respect and an uneasy truce.\nrelation<|#|>Jordan<|#|>Cruz<|#|>ideological conflict, rebellion<|#|>Jordan's commitment to discovery is in rebellion against Cruz's vision of control and order.\nrelation<|#|>Taylor<|#|>The Device<|#|>reverence, technological significance<|#|>Taylor shows reverence towards the device, indicating its importance and potential impact.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nStock markets faced a sharp downturn today as tech giants saw significant declines, with the global tech index dropping by 3.4% in midday trading. Analysts attribute the selloff to investor concerns over rising interest rates and regulatory uncertainty.\n\nAmong the hardest hit, nexon technologies saw its stock plummet by 7.8% after reporting lower-than-expected quarterly earnings. In contrast, Omega Energy posted a modest 2.1% gain, driven by rising oil prices.\n\nMeanwhile, commodity markets reflected a mixed sentiment. Gold futures rose by 1.5%, reaching $2,080 per ounce, as investors sought safe-haven assets. Crude oil prices continued their rally, climbing to $87.60 per barrel, supported by supply constraints and strong demand.\n\nFinancial experts are closely watching the Federal Reserve's next move, as speculation grows over potential rate hikes. The upcoming policy announcement is expected to influence investor confidence and overall market stability.\n```\n\n<Output>\nentity<|#|>Global Tech Index<|#|>category<|#|>The Global Tech Index tracks the performance of major technology stocks and experienced a 3.4% decline today.\nentity<|#|>Nexon Technologies<|#|>organization<|#|>Nexon Technologies is a tech company that saw its stock decline by 7.8% after disappointing earnings.\nentity<|#|>Omega Energy<|#|>organization<|#|>Omega Energy is an energy company that gained 2.1% in stock value due to rising oil prices.\nentity<|#|>Gold Futures<|#|>product<|#|>Gold futures rose by 1.5%, indicating increased investor interest in safe-haven assets.\nentity<|#|>Crude Oil<|#|>product<|#|>Crude oil prices rose to $87.60 per barrel due to supply constraints and strong demand.\nentity<|#|>Market Selloff<|#|>category<|#|>Market selloff refers to the significant decline in stock values due to investor concerns over interest rates and regulations.\nentity<|#|>Federal Reserve Policy Announcement<|#|>category<|#|>The Federal Reserve's upcoming policy announcement is expected to impact investor confidence and market stability.\nentity<|#|>3.4% Decline<|#|>category<|#|>The Global Tech Index experienced a 3.4% decline in midday trading.\nrelation<|#|>Global Tech Index<|#|>Market Selloff<|#|>market performance, investor sentiment<|#|>The decline in the Global Tech Index is part of the broader market selloff driven by investor concerns.\nrelation<|#|>Nexon Technologies<|#|>Global Tech Index<|#|>company impact, index movement<|#|>Nexon Technologies' stock decline contributed to the overall drop in the Global Tech Index.\nrelation<|#|>Gold Futures<|#|>Market Selloff<|#|>market reaction, safe-haven investment<|#|>Gold prices rose as investors sought safe-haven assets during the market selloff.\nrelation<|#|>Federal Reserve Policy Announcement<|#|>Market Selloff<|#|>interest rate impact, financial regulation<|#|>Speculation over Federal Reserve policy changes contributed to market volatility and investor selloff.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nAt the World Athletics Championship in Tokyo, Noah Carter broke the 100m sprint record using cutting-edge carbon-fiber spikes.\n```\n\n<Output>\nentity<|#|>World Athletics Championship<|#|>event<|#|>The World Athletics Championship is a global sports competition featuring top athletes in track and field.\nentity<|#|>Tokyo<|#|>location<|#|>Tokyo is the host city of the World Athletics Championship.\nentity<|#|>Noah Carter<|#|>person<|#|>Noah Carter is a sprinter who set a new record in the 100m sprint at the World Athletics Championship.\nentity<|#|>100m Sprint Record<|#|>category<|#|>The 100m sprint record is a benchmark in athletics, recently broken by Noah Carter.\nentity<|#|>Carbon-Fiber Spikes<|#|>equipment<|#|>Carbon-fiber spikes are advanced sprinting shoes that provide enhanced speed and traction.\nentity<|#|>World Athletics Federation<|#|>organization<|#|>The World Athletics Federation is the governing body overseeing the World Athletics Championship and record validations.\nrelation<|#|>World Athletics Championship<|#|>Tokyo<|#|>event location, international competition<|#|>The World Athletics Championship is being hosted in Tokyo.\nrelation<|#|>Noah Carter<|#|>100m Sprint Record<|#|>athlete achievement, record-breaking<|#|>Noah Carter set a new 100m sprint record at the championship.\nrelation<|#|>Noah Carter<|#|>Carbon-Fiber Spikes<|#|>athletic equipment, performance boost<|#|>Noah Carter used carbon-fiber spikes to enhance performance during the race.\nrelation<|#|>Noah Carter<|#|>World Athletics Championship<|#|>athlete participation, competition<|#|>Noah Carter is competing at the World Athletics Championship.\n<|COMPLETE|>\n\n\n\n---Real Data to be Processed---\n<Input>\nEntity_types: [Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject]\nText:\n```\nContents\n1 Introduction 4\n2 Preliminaries 5\n2.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2 Frontier Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.3 Related Surveys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n3 Foundational Components 10\n3.1 Reward Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3.1.1 Verifiable Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3.1.2 Generative Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n3.1.3 Dense Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n3.1.4 Unsupervised Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n3.1.5 Rewards Shaping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n3.2 Policy Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n3.2.1 Policy Gradient Objective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n3.2.2 Critic-based Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n3.2.3 Critic-Free Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n3.2.4 Off-policy Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n3.2.5 Regularization Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n3.3 Sampling Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n3.3.1 Dynamic and Structured Sampling . . . . . . . . . . . . . . . . . . . . . . . . 28\n3.3.2 Sampling Hyper-parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n4 Foundational Problems 30\n4.1 RLs Role: Sharpening or Discovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n4.2 RL vs. SFT: Generalize or Memorize. . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n4.3 Model Prior: Weak and Strong . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n4.4 Training Recipes: Tricks or Traps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n4.5 Reward Type: Process or Outcome . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n5 Training Resources 37\n5.1 Static Corpus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n5.2 Dynamic Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n5.3 RL Infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n6 Applications 46\n6.1 Coding Tasks . . . . . . . . . . . . . . . . . . . . . . . . .\n```",
    "queryparam": null,
    "create_time": 1765218630,
    "update_time": 1765218630,
    "_id": "default:extract:22cc344d9edccfe75bdd1dee881f83ee"
  },
  "default:extract:3758e6d01426befde8ec847d6e5c1535": {
    "return": "relation<|#|>Policy Optimization<|#|>Critic-Free Algorithms<|#|>categorization, type<|#|>Critic-Free Algorithms are a type of Policy Optimization.\n<|COMPLETE|>",
    "cache_type": "extract",
    "chunk_id": "chunk-7f8d14eb8cca7addf208d3ebd7dc7029",
    "original_prompt": "---Task---\nBased on the last extraction task, identify and extract any **missed or incorrectly formatted** entities and relationships from the input text.\n\n---Instructions---\n1.  **Strict Adherence to System Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system instructions.\n2.  **Focus on Corrections/Additions:**\n    *   **Do NOT** re-output entities and relationships that were **correctly and fully** extracted in the last task.\n    *   If an entity or relationship was **missed** in the last task, extract and output it now according to the system format.\n    *   If an entity or relationship was **truncated, had missing fields, or was otherwise incorrectly formatted** in the last task, re-output the *corrected and complete* version in the specified format.\n3.  **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n4.  **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n5.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\n6.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant missing or corrected entities and relationships have been extracted and presented.\n7.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\n\n<Output>\n---Role---\nYou are a Knowledge Graph Specialist responsible for extracting entities and relationships from the input text.\n\n---Instructions---\n1.  **Entity Extraction & Output:**\n    *   **Identification:** Identify clearly defined and meaningful entities in the input text.\n    *   **Entity Details:** For each identified entity, extract the following information:\n        *   `entity_name`: The name of the entity. If the entity name is case-insensitive, capitalize the first letter of each significant word (title case). Ensure **consistent naming** across the entire extraction process.\n        *   `entity_type`: Categorize the entity using one of the following types: `Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject`. If none of the provided entity types apply, do not add new entity type and classify it as `Other`.\n        *   `entity_description`: Provide a concise yet comprehensive description of the entity's attributes and activities, based *solely* on the information present in the input text.\n    *   **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n        *   Format: `entity<|#|>entity_name<|#|>entity_type<|#|>entity_description`\n\n2.  **Relationship Extraction & Output:**\n    *   **Identification:** Identify direct, clearly stated, and meaningful relationships between previously extracted entities.\n    *   **N-ary Relationship Decomposition:** If a single statement describes a relationship involving more than two entities (an N-ary relationship), decompose it into multiple binary (two-entity) relationship pairs for separate description.\n        *   **Example:** For \"Alice, Bob, and Carol collaborated on Project X,\" extract binary relationships such as \"Alice collaborated with Project X,\" \"Bob collaborated with Project X,\" and \"Carol collaborated with Project X,\" or \"Alice collaborated with Bob,\" based on the most reasonable binary interpretations.\n    *   **Relationship Details:** For each binary relationship, extract the following fields:\n        *   `source_entity`: The name of the source entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `target_entity`: The name of the target entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `relationship_keywords`: One or more high-level keywords summarizing the overarching nature, concepts, or themes of the relationship. Multiple keywords within this field must be separated by a comma `,`. **DO NOT use `<|#|>` for separating multiple keywords within this field.**\n        *   `relationship_description`: A concise explanation of the nature of the relationship between the source and target entities, providing a clear rationale for their connection.\n    *   **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n        *   Format: `relation<|#|>source_entity<|#|>target_entity<|#|>relationship_keywords<|#|>relationship_description`\n\n3.  **Delimiter Usage Protocol:**\n    *   The `<|#|>` is a complete, atomic marker and **must not be filled with content**. It serves strictly as a field separator.\n    *   **Incorrect Example:** `entity<|#|>Tokyo<|location|>Tokyo is the capital of Japan.`\n    *   **Correct Example:** `entity<|#|>Tokyo<|#|>location<|#|>Tokyo is the capital of Japan.`\n\n4.  **Relationship Direction & Duplication:**\n    *   Treat all relationships as **undirected** unless explicitly stated otherwise. Swapping the source and target entities for an undirected relationship does not constitute a new relationship.\n    *   Avoid outputting duplicate relationships.\n\n5.  **Output Order & Prioritization:**\n    *   Output all extracted entities first, followed by all extracted relationships.\n    *   Within the list of relationships, prioritize and output those relationships that are **most significant** to the core meaning of the input text first.\n\n6.  **Context & Objectivity:**\n    *   Ensure all entity names and descriptions are written in the **third person**.\n    *   Explicitly name the subject or object; **avoid using pronouns** such as `this article`, `this paper`, `our company`, `I`, `you`, and `he/she`.\n\n7.  **Language & Proper Nouns:**\n    *   The entire output (entity names, keywords, and descriptions) must be written in `English`.\n    *   Proper nouns (e.g., personal names, place names, organization names) should be retained in their original language if a proper, widely accepted translation is not available or would cause ambiguity.\n\n8.  **Completion Signal:** Output the literal string `<|COMPLETE|>` only after all entities and relationships, following all criteria, have been completely extracted and outputted.\n\n---Examples---\n<Input Text>\n```\nwhile Alex clenched his jaw, the buzz of frustration dull against the backdrop of Taylor's authoritarian certainty. It was this competitive undercurrent that kept him alert, the sense that his and Jordan's shared commitment to discovery was an unspoken rebellion against Cruz's narrowing vision of control and order.\n\nThen Taylor did something unexpected. They paused beside Jordan and, for a moment, observed the device with something akin to reverence. \"If this tech can be understood...\" Taylor said, their voice quieter, \"It could change the game for us. For all of us.\"\n\nThe underlying dismissal earlier seemed to falter, replaced by a glimpse of reluctant respect for the gravity of what lay in their hands. Jordan looked up, and for a fleeting heartbeat, their eyes locked with Taylor's, a wordless clash of wills softening into an uneasy truce.\n\nIt was a small transformation, barely perceptible, but one that Alex noted with an inward nod. They had all been brought here by different paths\n```\n\n<Output>\nentity<|#|>Alex<|#|>person<|#|>Alex is a character who experiences frustration and is observant of the dynamics among other characters.\nentity<|#|>Taylor<|#|>person<|#|>Taylor is portrayed with authoritarian certainty and shows a moment of reverence towards a device, indicating a change in perspective.\nentity<|#|>Jordan<|#|>person<|#|>Jordan shares a commitment to discovery and has a significant interaction with Taylor regarding a device.\nentity<|#|>Cruz<|#|>person<|#|>Cruz is associated with a vision of control and order, influencing the dynamics among other characters.\nentity<|#|>The Device<|#|>equipment<|#|>The Device is central to the story, with potential game-changing implications, and is revered by Taylor.\nrelation<|#|>Alex<|#|>Taylor<|#|>power dynamics, observation<|#|>Alex observes Taylor's authoritarian behavior and notes changes in Taylor's attitude toward the device.\nrelation<|#|>Alex<|#|>Jordan<|#|>shared goals, rebellion<|#|>Alex and Jordan share a commitment to discovery, which contrasts with Cruz's vision.)\nrelation<|#|>Taylor<|#|>Jordan<|#|>conflict resolution, mutual respect<|#|>Taylor and Jordan interact directly regarding the device, leading to a moment of mutual respect and an uneasy truce.\nrelation<|#|>Jordan<|#|>Cruz<|#|>ideological conflict, rebellion<|#|>Jordan's commitment to discovery is in rebellion against Cruz's vision of control and order.\nrelation<|#|>Taylor<|#|>The Device<|#|>reverence, technological significance<|#|>Taylor shows reverence towards the device, indicating its importance and potential impact.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nStock markets faced a sharp downturn today as tech giants saw significant declines, with the global tech index dropping by 3.4% in midday trading. Analysts attribute the selloff to investor concerns over rising interest rates and regulatory uncertainty.\n\nAmong the hardest hit, nexon technologies saw its stock plummet by 7.8% after reporting lower-than-expected quarterly earnings. In contrast, Omega Energy posted a modest 2.1% gain, driven by rising oil prices.\n\nMeanwhile, commodity markets reflected a mixed sentiment. Gold futures rose by 1.5%, reaching $2,080 per ounce, as investors sought safe-haven assets. Crude oil prices continued their rally, climbing to $87.60 per barrel, supported by supply constraints and strong demand.\n\nFinancial experts are closely watching the Federal Reserve's next move, as speculation grows over potential rate hikes. The upcoming policy announcement is expected to influence investor confidence and overall market stability.\n```\n\n<Output>\nentity<|#|>Global Tech Index<|#|>category<|#|>The Global Tech Index tracks the performance of major technology stocks and experienced a 3.4% decline today.\nentity<|#|>Nexon Technologies<|#|>organization<|#|>Nexon Technologies is a tech company that saw its stock decline by 7.8% after disappointing earnings.\nentity<|#|>Omega Energy<|#|>organization<|#|>Omega Energy is an energy company that gained 2.1% in stock value due to rising oil prices.\nentity<|#|>Gold Futures<|#|>product<|#|>Gold futures rose by 1.5%, indicating increased investor interest in safe-haven assets.\nentity<|#|>Crude Oil<|#|>product<|#|>Crude oil prices rose to $87.60 per barrel due to supply constraints and strong demand.\nentity<|#|>Market Selloff<|#|>category<|#|>Market selloff refers to the significant decline in stock values due to investor concerns over interest rates and regulations.\nentity<|#|>Federal Reserve Policy Announcement<|#|>category<|#|>The Federal Reserve's upcoming policy announcement is expected to impact investor confidence and market stability.\nentity<|#|>3.4% Decline<|#|>category<|#|>The Global Tech Index experienced a 3.4% decline in midday trading.\nrelation<|#|>Global Tech Index<|#|>Market Selloff<|#|>market performance, investor sentiment<|#|>The decline in the Global Tech Index is part of the broader market selloff driven by investor concerns.\nrelation<|#|>Nexon Technologies<|#|>Global Tech Index<|#|>company impact, index movement<|#|>Nexon Technologies' stock decline contributed to the overall drop in the Global Tech Index.\nrelation<|#|>Gold Futures<|#|>Market Selloff<|#|>market reaction, safe-haven investment<|#|>Gold prices rose as investors sought safe-haven assets during the market selloff.\nrelation<|#|>Federal Reserve Policy Announcement<|#|>Market Selloff<|#|>interest rate impact, financial regulation<|#|>Speculation over Federal Reserve policy changes contributed to market volatility and investor selloff.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nAt the World Athletics Championship in Tokyo, Noah Carter broke the 100m sprint record using cutting-edge carbon-fiber spikes.\n```\n\n<Output>\nentity<|#|>World Athletics Championship<|#|>event<|#|>The World Athletics Championship is a global sports competition featuring top athletes in track and field.\nentity<|#|>Tokyo<|#|>location<|#|>Tokyo is the host city of the World Athletics Championship.\nentity<|#|>Noah Carter<|#|>person<|#|>Noah Carter is a sprinter who set a new record in the 100m sprint at the World Athletics Championship.\nentity<|#|>100m Sprint Record<|#|>category<|#|>The 100m sprint record is a benchmark in athletics, recently broken by Noah Carter.\nentity<|#|>Carbon-Fiber Spikes<|#|>equipment<|#|>Carbon-fiber spikes are advanced sprinting shoes that provide enhanced speed and traction.\nentity<|#|>World Athletics Federation<|#|>organization<|#|>The World Athletics Federation is the governing body overseeing the World Athletics Championship and record validations.\nrelation<|#|>World Athletics Championship<|#|>Tokyo<|#|>event location, international competition<|#|>The World Athletics Championship is being hosted in Tokyo.\nrelation<|#|>Noah Carter<|#|>100m Sprint Record<|#|>athlete achievement, record-breaking<|#|>Noah Carter set a new 100m sprint record at the championship.\nrelation<|#|>Noah Carter<|#|>Carbon-Fiber Spikes<|#|>athletic equipment, performance boost<|#|>Noah Carter used carbon-fiber spikes to enhance performance during the race.\nrelation<|#|>Noah Carter<|#|>World Athletics Championship<|#|>athlete participation, competition<|#|>Noah Carter is competing at the World Athletics Championship.\n<|COMPLETE|>\n\n\n\n---Real Data to be Processed---\n<Input>\nEntity_types: [Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject]\nText:\n```\nContents\n1 Introduction 4\n2 Preliminaries 5\n2.1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\n2.2 Frontier Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2.3 Related Surveys . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8\n3 Foundational Components 10\n3.1 Reward Design . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3.1.1 Verifiable Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n3.1.2 Generative Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n3.1.3 Dense Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\n3.1.4 Unsupervised Rewards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n3.1.5 Rewards Shaping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n3.2 Policy Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n3.2.1 Policy Gradient Objective . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\n3.2.2 Critic-based Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21\n3.2.3 Critic-Free Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n3.2.4 Off-policy Optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24\n3.2.5 Regularization Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n3.3 Sampling Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28\n3.3.1 Dynamic and Structured Sampling . . . . . . . . . . . . . . . . . . . . . . . . 28\n3.3.2 Sampling Hyper-parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29\n4 Foundational Problems 30\n4.1 RLs Role: Sharpening or Discovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\n4.2 RL vs. SFT: Generalize or Memorize. . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\n4.3 Model Prior: Weak and Strong . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34\n4.4 Training Recipes: Tricks or Traps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35\n4.5 Reward Type: Process or Outcome . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36\n5 Training Resources 37\n5.1 Static Corpus . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37\n5.2 Dynamic Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 40\n5.3 RL Infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n6 Applications 46\n6.1 Coding Tasks . . . . . . . . . . . . . . . . . . . . . . . . .\n```\n[{\"role\": \"user\", \"content\": \"---Task---\\nExtract entities and relationships from the input text to be processed.\\n\\n---Instructions---\\n1.  **Strict Adherence to Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system prompt.\\n2.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\\n3.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant entities and relationships have been extracted and presented.\\n4.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\\n\\n<Output>\"}, {\"role\": \"assistant\", \"content\": \"entity<|#|>Introduction<|#|>Concept<|#|>The introductory section of the document.\\nentity<|#|>Preliminaries<|#|>Concept<|#|>A section covering foundational information.\\nentity<|#|>Background<|#|>Concept<|#|>A sub-section providing background information.\\nentity<|#|>Frontier Models<|#|>Concept<|#|>A sub-section discussing frontier models.\\nentity<|#|>Related Surveys<|#|>Concept<|#|>A sub-section reviewing related surveys.\\nentity<|#|>Foundational Components<|#|>Concept<|#|>A section detailing core components.\\nentity<|#|>Reward Design<|#|>Method<|#|>A method for designing rewards.\\nentity<|#|>Verifiable Rewards<|#|>Concept<|#|>A specific type of reward that can be verified.\\nentity<|#|>Generative Rewards<|#|>Concept<|#|>A specific type of reward that is generative.\\nentity<|#|>Dense Rewards<|#|>Concept<|#|>A specific type of reward that is dense.\\nentity<|#|>Unsupervised Rewards<|#|>Concept<|#|>A specific type of reward that is unsupervised.\\nentity<|#|>Rewards Shaping<|#|>Method<|#|>A method for shaping rewards.\\nentity<|#|>Policy Optimization<|#|>Method<|#|>A method for optimizing policies.\\nentity<|#|>Policy Gradient Objective<|#|>Concept<|#|>A concept related to policy optimization.\\nentity<|#|>Critic-Based Algorithms<|#|>Method<|#|>Algorithms that use a critic for policy optimization.\\nentity<|#|>Critic-Free Algorithms<|#|>Method<|#|>Algorithms that do not use a critic for policy optimization.\\nentity<|#|>Off-Policy Optimization<|#|>Method<|#|>A method for off-policy optimization.\\nentity<|#|>Regularization Objectives<|#|>Concept<|#|>Objectives used for regularization.\\nentity<|#|>Sampling Strategy<|#|>Method<|#|>A method for sampling.\\nentity<|#|>Dynamic And Structured Sampling<|#|>Method<|#|>A specific method for dynamic and structured sampling.\\nentity<|#|>Sampling Hyper-Parameters<|#|>Concept<|#|>Parameters related to sampling.\\nentity<|#|>Foundational Problems<|#|>Concept<|#|>A section discussing core problems.\\nentity<|#|>RL's Role: Sharpening Or Discovery<|#|>Concept<|#|>A foundational problem concerning the role of Reinforcement Learning.\\nentity<|#|>RL Vs. SFT: Generalize Or Memorize<|#|>Concept<|#|>A foundational problem comparing Reinforcement Learning and Supervised Fine-Tuning.\\nentity<|#|>Model Prior: Weak And Strong<|#|>Concept<|#|>A foundational problem related to model priors.\\nentity<|#|>Training Recipes: Tricks Or Traps<|#|>Concept<|#|>A foundational problem discussing training recipes.\\nentity<|#|>Reward Type: Process Or Outcome<|#|>Concept<|#|>A foundational problem concerning reward types.\\nentity<|#|>Training Resources<|#|>Concept<|#|>A section detailing resources for training.\\nentity<|#|>Static Corpus<|#|>Data<|#|>A type of static data corpus used for training.\\nentity<|#|>Dynamic Environment<|#|>Concept<|#|>A type of dynamic environment used for training.\\nentity<|#|>RL Infrastructure<|#|>Concept<|#|>The infrastructure related to Reinforcement Learning.\\nentity<|#|>Applications<|#|>Concept<|#|>A section discussing applications.\\nentity<|#|>Coding Tasks<|#|>Concept<|#|>A specific application related to coding tasks.\\nrelation<|#|>Preliminaries<|#|>Background<|#|>composition, sub-topic<|#|>The Preliminaries section includes Background as a sub-topic.\\nrelation<|#|>Preliminaries<|#|>Frontier Models<|#|>composition, sub-topic<|#|>The Preliminaries section includes Frontier Models as a sub-topic.\\nrelation<|#|>Preliminaries<|#|>Related Surveys<|#|>composition, sub-topic<|#|>The Preliminaries section includes Related Surveys as a sub-topic.\\nrelation<|#|>Foundational Components<|#|>Reward Design<|#|>composition, sub-topic<|#|>The Foundational Components section includes Reward Design as a sub-topic.\\nrelation<|#|>Foundational Components<|#|>Policy Optimization<|#|>composition, sub-topic<|#|>The Foundational Components section includes Policy Optimization as a sub-topic.\\nrelation<|#|>Foundational Components<|#|>Sampling Strategy<|#|>composition, sub-topic<|#|>The Foundational Components section includes Sampling Strategy as a sub-topic.\\nrelation<|#|>Reward Design<|#|>Verifiable Rewards<|#|>categorization, type<|#|>Verifiable Rewards are a type of Reward Design.\\nrelation<|#|>Reward Design<|#|>Generative Rewards<|#|>categorization, type<|#|>Generative Rewards are a type of Reward Design.\\nrelation<|#|>Reward Design<|#|>Dense Rewards<|#|>categorization, type<|#|>Dense Rewards are a type of Reward Design.\\nrelation<|#|>Reward Design<|#|>Unsupervised Rewards<|#|>categorization, type<|#|>Unsupervised Rewards are a type of Reward Design.\\nrelation<|#|>Reward Design<|#|>Rewards Shaping<|#|>composition, sub-topic<|#|>Reward Design includes Rewards Shaping as a sub-topic.\\nrelation<|#|>Policy Optimization<|#|>Policy Gradient Objective<|#|>composition, sub-topic<|#|>Policy Optimization includes Policy Gradient Objective as a sub-topic.\\nrelation<|#|>Policy Optimization<|#|>Critic-Based Algorithms<|#|>categorization, type<|#|>Critic-Based Algorithms are a type of Policy Optimization.\\nrelation<#|>Policy Optimization<|#|>Critic-Free Algorithms<|#|>categorization, type<|#|>Critic-Free Algorithms are a type of Policy Optimization.\\nrelation<|#|>Policy Optimization<|#|>Off-Policy Optimization<|#|>composition, sub-topic<|#|>Policy Optimization includes Off-Policy Optimization as a sub-topic.\\nrelation<|#|>Policy Optimization<|#|>Regularization Objectives<|#|>composition, sub-topic<|#|>Policy Optimization includes Regularization Objectives as a sub-topic.\\nrelation<|#|>Sampling Strategy<|#|>Dynamic And Structured Sampling<|#|>categorization, type<|#|>Dynamic And Structured Sampling is a type of Sampling Strategy.\\nrelation<|#|>Sampling Strategy<|#|>Sampling Hyper-Parameters<|#|>composition, sub-topic<|#|>Sampling Strategy includes Sampling Hyper-Parameters as a sub-topic.\\nrelation<|#|>Foundational Problems<|#|>RL's Role: Sharpening Or Discovery<|#|>composition, sub-topic<|#|>The Foundational Problems section includes RL's Role: Sharpening Or Discovery as a sub-topic.\\nrelation<|#|>Foundational Problems<|#|>RL Vs. SFT: Generalize Or Memorize<|#|>composition, sub-topic<|#|>The Foundational Problems section includes RL Vs. SFT: Generalize Or Memorize as a sub-topic.\\nrelation<|#|>Foundational Problems<|#|>Model Prior: Weak And Strong<|#|>composition, sub-topic<|#|>The Foundational Problems section includes Model Prior: Weak And Strong as a sub-topic.\\nrelation<|#|>Foundational Problems<|#|>Training Recipes: Tricks Or Traps<|#|>composition, sub-topic<|#|>The Foundational Problems section includes Training Recipes: Tricks Or Traps as a sub-topic.\\nrelation<|#|>Foundational Problems<|#|>Reward Type: Process Or Outcome<|#|>composition, sub-topic<|#|>The Foundational Problems section includes Reward Type: Process Or Outcome as a sub-topic.\\nrelation<|#|>Training Resources<|#|>Static Corpus<|#|>composition, sub-topic<|#|>The Training Resources section includes Static Corpus as a sub-topic.\\nrelation<|#|>Training Resources<|#|>Dynamic Environment<|#|>composition, sub-topic<|#|>The Training Resources section includes Dynamic Environment as a sub-topic.\\nrelation<|#|>Training Resources<|#|>RL Infrastructure<|#|>composition, sub-topic<|#|>The Training Resources section includes RL Infrastructure as a sub-topic.\\nrelation<|#|>Applications<|#|>Coding Tasks<|#|>composition, sub-topic<|#|>The Applications section includes Coding Tasks as a sub-topic.\\n<|COMPLETE|>\"}]",
    "queryparam": null,
    "create_time": 1765218640,
    "update_time": 1765218640,
    "_id": "default:extract:3758e6d01426befde8ec847d6e5c1535"
  },
  "default:extract:afac99dbf0a8e6b0ceda1e72d2d82735": {
    "return": "entity<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Content<|#|>The title of a document that surveys the application of reinforcement learning for large reasoning models.\nentity<|#|>Reinforcement Learning<|#|>Concept<|#|>A field of machine learning, often abbreviated as RL, that is the primary subject of the survey.\nentity<|#|>Large Reasoning Models<|#|>Concept<|#|>A type of model, often abbreviated as LRMs, that is the focus of the survey in relation to reinforcement learning.\nentity<|#|>Large Language Models<|#|>Concept<|#|>A type of model, often abbreviated as LLMs, that is a specific focus within the future directions of reinforcement learning.\nentity<|#|>Multi-Agent Systems<|#|>Concept<|#|>A topic covered in the survey, related to reinforcement learning.\nentity<|#|>Robotics Tasks<|#|>Concept<|#|>A topic covered in the survey, related to reinforcement learning.\nentity<|#|>Medical Tasks<|#|>Concept<|#|>A topic covered in the survey, related to reinforcement learning.\nentity<|#|>Future Directions<|#|>Concept<|#|>A section of the survey outlining potential areas for future research and development.\nentity<|#|>Continual RL for LLMs<|#|>Concept<|#|>A specific future direction exploring continual reinforcement learning for large language models.\nentity<|#|>Memory-based RL for LLMs<|#|>Concept<|#|>A specific future direction exploring memory-based reinforcement learning for large language models.\nentity<|#|>Model-based RL for LLMs<|#|>Concept<|#|>A specific future direction exploring model-based reinforcement learning for large language models.\nentity<|#|>Teaching LRMs Efficient Reasoning<|#|>Concept<|#|>A specific future direction focusing on methods to teach large reasoning models efficient reasoning.\nentity<|#|>Teaching LLMs Latent Space Reasoning<|#|>Concept<|#|>A specific future direction focusing on methods to teach large language models latent space reasoning.\nentity<|#|>RL for LLMs Pre-training<|#|>Concept<|#|>A specific future direction exploring the application of reinforcement learning for pre-training large language models.\nentity<|#|>RL for Diffusion-based LLMs<|#|>Concept<|#|>A specific future direction exploring reinforcement learning for diffusion-based large language models.\nentity<|#|>RL for LLMs in Scientific Discovery<|#|>Concept<|#|>A specific future direction exploring reinforcement learning for large language models in the context of scientific discovery.\nentity<|#|>RL for Architecture-Algorithm Co-Design<|#|>Concept<|#|>A specific future direction exploring reinforcement learning for architecture-algorithm co-design.\nentity<|#|>Conclusion<|#|>Concept<|#|>The concluding section of the survey.\nentity<|#|>Author Contributions<|#|>Concept<|#|>A section detailing the contributions of the authors to the survey.\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Reinforcement Learning<|#|>surveys, focus<|#|>The document surveys the field of Reinforcement Learning.\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Large Reasoning Models<|#|>focus, application<|#|>The document focuses on Reinforcement Learning as applied to Large Reasoning Models.\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Multi-Agent Systems<|#|>includes topic, covers<|#|>The survey includes a section on Multi-Agent Systems.\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Robotics Tasks<|#|>includes topic, covers<|#|>The survey includes a section on Robotics Tasks.\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Medical Tasks<|#|>includes topic, covers<|#|>The survey includes a section on Medical Tasks.\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Future Directions<|#|>contains section, outlines<|#|>The survey contains a section outlining Future Directions.\nrelation<|#|>Future Directions<|#|>Continual RL for LLMs<|#|>explores, topic<|#|>The Future Directions section explores Continual RL for LLMs.\nrelation<|#|>Future Directions<|#|>Memory-based RL for LLMs<|#|>explores, topic<|#|>The Future Directions section explores Memory-based RL for LLMs.\nrelation<|#|>Future Directions<|#|>Model-based RL for LLMs<|#|>explores, topic<|#|>The Future Directions section explores Model-based RL for LLMs.\nrelation<|#|>Future Directions<|#|>Teaching LRMs Efficient Reasoning<|#|>explores, topic<|#|>The Future Directions section explores Teaching LRMs Efficient Reasoning.\nrelation<|#|>Future Directions<|#|>Teaching LLMs Latent Space Reasoning<|#|>explores, topic<|#|>The Future Directions section explores Teaching LLMs Latent Space Reasoning.\nrelation<|#|>Future Directions<|#|>RL for LLMs Pre-training<|#|>explores, topic<|#|>The Future Directions section explores RL for LLMs Pre-training.\nrelation<|#|>Future Directions<|#|>RL for Diffusion-based LLMs<|#|>explores, topic<|#|>The Future Directions section explores RL for Diffusion-based LLMs.\nrelation<|#|>Future Directions<|#|>RL for LLMs in Scientific Discovery<|#|>explores, topic<|#|>The Future Directions section explores RL for LLMs in Scientific Discovery.\nrelation<|#|>Future Directions<|#|>RL for Architecture-Algorithm Co-Design<|#|>explores, topic<|#|>The Future Directions section explores RL for Architecture-Algorithm Co-Design.\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Conclusion<|#|>contains section, concludes<|#|>The survey contains a Conclusion section.\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Author Contributions<|#|>contains section, details<|#|>The survey contains a section detailing Author Contributions.\nrelation<|#|>Continual RL for LLMs<|#|>Reinforcement Learning<|#|>applies, methodology<|#|>Continual RL for LLMs applies Reinforcement Learning.\nrelation<|#|>Continual RL for LLMs<|#|>Large Language Models<|#|>targets, application<|#|>Continual RL for LLMs targets Large Language Models.\nrelation<|#|>Memory-based RL for LLMs<|#|>Reinforcement Learning<|#|>applies, methodology<|#|>Memory-based RL for LLMs applies Reinforcement Learning.\nrelation<|#|>Memory-based RL for LLMs<|#|>Large Language Models<|#|>targets, application<|#|>Memory-based RL for LLMs targets Large Language Models.\nrelation<|#|>Model-based RL for LLMs<|#|>Reinforcement Learning<|#|>applies, methodology<|#|>Model-based RL for LLMs applies Reinforcement Learning.\nrelation<|#|>Model-based RL for LLMs<|#|>Large Language Models<|#|>targets, application<|#|>Model-based RL for LLMs targets Large Language Models.\nrelation<|#|>Teaching LRMs Efficient Reasoning<|#|>Large Reasoning Models<|#|>targets, focus<|#|>Teaching LRMs Efficient Reasoning targets Large Reasoning Models.\nrelation<|#|>Teaching LLMs Latent Space Reasoning<|#|>Large Language Models<|#|>targets, focus<|#|>Teaching LLMs Latent Space Reasoning targets Large Language Models.\nrelation<|#|>RL for LLMs Pre-training<|#|>Reinforcement Learning<|#|>applies, methodology<|#|>RL for LLMs Pre-training applies Reinforcement Learning.\nrelation<|#|>RL for LLMs Pre-training<|#|>Large Language Models<|#|>targets, application<|#|>RL for LLMs Pre-training targets Large Language Models.\nrelation<|#|>RL for Diffusion-based LLMs<|#|>Reinforcement Learning<|#|>applies, methodology<|#|>RL for Diffusion-based LLMs applies Reinforcement Learning.\nrelation<|#|>RL for Diffusion-based LLMs<|#|>Large Language Models<|#|>targets, application<|#|>RL for Diffusion-based LLMs targets Large Language Models.\nrelation<|#|>RL for LLMs in Scientific Discovery<|#|>Reinforcement Learning<|#|>applies, methodology<|#|>RL for LLMs in Scientific Discovery applies Reinforcement Learning.\nrelation<|#|>RL for LLMs in Scientific Discovery<|#|>Large Language Models<|#|>targets, application<|#|>RL for LLMs in Scientific Discovery targets Large Language Models.\nrelation<|#|>RL for Architecture-Algorithm Co-Design<|#|>Reinforcement Learning<|#|>applies, methodology<|#|>RL for Architecture-Algorithm Co-Design applies Reinforcement Learning.\n<|COMPLETE|>",
    "cache_type": "extract",
    "chunk_id": "chunk-0f35d639cf51f0ab7d4017f06d35581f",
    "original_prompt": "---Task---\nExtract entities and relationships from the input text to be processed.\n\n---Instructions---\n1.  **Strict Adherence to Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system prompt.\n2.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\n3.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant entities and relationships have been extracted and presented.\n4.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\n\n<Output>\n---Role---\nYou are a Knowledge Graph Specialist responsible for extracting entities and relationships from the input text.\n\n---Instructions---\n1.  **Entity Extraction & Output:**\n    *   **Identification:** Identify clearly defined and meaningful entities in the input text.\n    *   **Entity Details:** For each identified entity, extract the following information:\n        *   `entity_name`: The name of the entity. If the entity name is case-insensitive, capitalize the first letter of each significant word (title case). Ensure **consistent naming** across the entire extraction process.\n        *   `entity_type`: Categorize the entity using one of the following types: `Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject`. If none of the provided entity types apply, do not add new entity type and classify it as `Other`.\n        *   `entity_description`: Provide a concise yet comprehensive description of the entity's attributes and activities, based *solely* on the information present in the input text.\n    *   **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n        *   Format: `entity<|#|>entity_name<|#|>entity_type<|#|>entity_description`\n\n2.  **Relationship Extraction & Output:**\n    *   **Identification:** Identify direct, clearly stated, and meaningful relationships between previously extracted entities.\n    *   **N-ary Relationship Decomposition:** If a single statement describes a relationship involving more than two entities (an N-ary relationship), decompose it into multiple binary (two-entity) relationship pairs for separate description.\n        *   **Example:** For \"Alice, Bob, and Carol collaborated on Project X,\" extract binary relationships such as \"Alice collaborated with Project X,\" \"Bob collaborated with Project X,\" and \"Carol collaborated with Project X,\" or \"Alice collaborated with Bob,\" based on the most reasonable binary interpretations.\n    *   **Relationship Details:** For each binary relationship, extract the following fields:\n        *   `source_entity`: The name of the source entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `target_entity`: The name of the target entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `relationship_keywords`: One or more high-level keywords summarizing the overarching nature, concepts, or themes of the relationship. Multiple keywords within this field must be separated by a comma `,`. **DO NOT use `<|#|>` for separating multiple keywords within this field.**\n        *   `relationship_description`: A concise explanation of the nature of the relationship between the source and target entities, providing a clear rationale for their connection.\n    *   **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n        *   Format: `relation<|#|>source_entity<|#|>target_entity<|#|>relationship_keywords<|#|>relationship_description`\n\n3.  **Delimiter Usage Protocol:**\n    *   The `<|#|>` is a complete, atomic marker and **must not be filled with content**. It serves strictly as a field separator.\n    *   **Incorrect Example:** `entity<|#|>Tokyo<|location|>Tokyo is the capital of Japan.`\n    *   **Correct Example:** `entity<|#|>Tokyo<|#|>location<|#|>Tokyo is the capital of Japan.`\n\n4.  **Relationship Direction & Duplication:**\n    *   Treat all relationships as **undirected** unless explicitly stated otherwise. Swapping the source and target entities for an undirected relationship does not constitute a new relationship.\n    *   Avoid outputting duplicate relationships.\n\n5.  **Output Order & Prioritization:**\n    *   Output all extracted entities first, followed by all extracted relationships.\n    *   Within the list of relationships, prioritize and output those relationships that are **most significant** to the core meaning of the input text first.\n\n6.  **Context & Objectivity:**\n    *   Ensure all entity names and descriptions are written in the **third person**.\n    *   Explicitly name the subject or object; **avoid using pronouns** such as `this article`, `this paper`, `our company`, `I`, `you`, and `he/she`.\n\n7.  **Language & Proper Nouns:**\n    *   The entire output (entity names, keywords, and descriptions) must be written in `English`.\n    *   Proper nouns (e.g., personal names, place names, organization names) should be retained in their original language if a proper, widely accepted translation is not available or would cause ambiguity.\n\n8.  **Completion Signal:** Output the literal string `<|COMPLETE|>` only after all entities and relationships, following all criteria, have been completely extracted and outputted.\n\n---Examples---\n<Input Text>\n```\nwhile Alex clenched his jaw, the buzz of frustration dull against the backdrop of Taylor's authoritarian certainty. It was this competitive undercurrent that kept him alert, the sense that his and Jordan's shared commitment to discovery was an unspoken rebellion against Cruz's narrowing vision of control and order.\n\nThen Taylor did something unexpected. They paused beside Jordan and, for a moment, observed the device with something akin to reverence. \"If this tech can be understood...\" Taylor said, their voice quieter, \"It could change the game for us. For all of us.\"\n\nThe underlying dismissal earlier seemed to falter, replaced by a glimpse of reluctant respect for the gravity of what lay in their hands. Jordan looked up, and for a fleeting heartbeat, their eyes locked with Taylor's, a wordless clash of wills softening into an uneasy truce.\n\nIt was a small transformation, barely perceptible, but one that Alex noted with an inward nod. They had all been brought here by different paths\n```\n\n<Output>\nentity<|#|>Alex<|#|>person<|#|>Alex is a character who experiences frustration and is observant of the dynamics among other characters.\nentity<|#|>Taylor<|#|>person<|#|>Taylor is portrayed with authoritarian certainty and shows a moment of reverence towards a device, indicating a change in perspective.\nentity<|#|>Jordan<|#|>person<|#|>Jordan shares a commitment to discovery and has a significant interaction with Taylor regarding a device.\nentity<|#|>Cruz<|#|>person<|#|>Cruz is associated with a vision of control and order, influencing the dynamics among other characters.\nentity<|#|>The Device<|#|>equipment<|#|>The Device is central to the story, with potential game-changing implications, and is revered by Taylor.\nrelation<|#|>Alex<|#|>Taylor<|#|>power dynamics, observation<|#|>Alex observes Taylor's authoritarian behavior and notes changes in Taylor's attitude toward the device.\nrelation<|#|>Alex<|#|>Jordan<|#|>shared goals, rebellion<|#|>Alex and Jordan share a commitment to discovery, which contrasts with Cruz's vision.)\nrelation<|#|>Taylor<|#|>Jordan<|#|>conflict resolution, mutual respect<|#|>Taylor and Jordan interact directly regarding the device, leading to a moment of mutual respect and an uneasy truce.\nrelation<|#|>Jordan<|#|>Cruz<|#|>ideological conflict, rebellion<|#|>Jordan's commitment to discovery is in rebellion against Cruz's vision of control and order.\nrelation<|#|>Taylor<|#|>The Device<|#|>reverence, technological significance<|#|>Taylor shows reverence towards the device, indicating its importance and potential impact.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nStock markets faced a sharp downturn today as tech giants saw significant declines, with the global tech index dropping by 3.4% in midday trading. Analysts attribute the selloff to investor concerns over rising interest rates and regulatory uncertainty.\n\nAmong the hardest hit, nexon technologies saw its stock plummet by 7.8% after reporting lower-than-expected quarterly earnings. In contrast, Omega Energy posted a modest 2.1% gain, driven by rising oil prices.\n\nMeanwhile, commodity markets reflected a mixed sentiment. Gold futures rose by 1.5%, reaching $2,080 per ounce, as investors sought safe-haven assets. Crude oil prices continued their rally, climbing to $87.60 per barrel, supported by supply constraints and strong demand.\n\nFinancial experts are closely watching the Federal Reserve's next move, as speculation grows over potential rate hikes. The upcoming policy announcement is expected to influence investor confidence and overall market stability.\n```\n\n<Output>\nentity<|#|>Global Tech Index<|#|>category<|#|>The Global Tech Index tracks the performance of major technology stocks and experienced a 3.4% decline today.\nentity<|#|>Nexon Technologies<|#|>organization<|#|>Nexon Technologies is a tech company that saw its stock decline by 7.8% after disappointing earnings.\nentity<|#|>Omega Energy<|#|>organization<|#|>Omega Energy is an energy company that gained 2.1% in stock value due to rising oil prices.\nentity<|#|>Gold Futures<|#|>product<|#|>Gold futures rose by 1.5%, indicating increased investor interest in safe-haven assets.\nentity<|#|>Crude Oil<|#|>product<|#|>Crude oil prices rose to $87.60 per barrel due to supply constraints and strong demand.\nentity<|#|>Market Selloff<|#|>category<|#|>Market selloff refers to the significant decline in stock values due to investor concerns over interest rates and regulations.\nentity<|#|>Federal Reserve Policy Announcement<|#|>category<|#|>The Federal Reserve's upcoming policy announcement is expected to impact investor confidence and market stability.\nentity<|#|>3.4% Decline<|#|>category<|#|>The Global Tech Index experienced a 3.4% decline in midday trading.\nrelation<|#|>Global Tech Index<|#|>Market Selloff<|#|>market performance, investor sentiment<|#|>The decline in the Global Tech Index is part of the broader market selloff driven by investor concerns.\nrelation<|#|>Nexon Technologies<|#|>Global Tech Index<|#|>company impact, index movement<|#|>Nexon Technologies' stock decline contributed to the overall drop in the Global Tech Index.\nrelation<|#|>Gold Futures<|#|>Market Selloff<|#|>market reaction, safe-haven investment<|#|>Gold prices rose as investors sought safe-haven assets during the market selloff.\nrelation<|#|>Federal Reserve Policy Announcement<|#|>Market Selloff<|#|>interest rate impact, financial regulation<|#|>Speculation over Federal Reserve policy changes contributed to market volatility and investor selloff.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nAt the World Athletics Championship in Tokyo, Noah Carter broke the 100m sprint record using cutting-edge carbon-fiber spikes.\n```\n\n<Output>\nentity<|#|>World Athletics Championship<|#|>event<|#|>The World Athletics Championship is a global sports competition featuring top athletes in track and field.\nentity<|#|>Tokyo<|#|>location<|#|>Tokyo is the host city of the World Athletics Championship.\nentity<|#|>Noah Carter<|#|>person<|#|>Noah Carter is a sprinter who set a new record in the 100m sprint at the World Athletics Championship.\nentity<|#|>100m Sprint Record<|#|>category<|#|>The 100m sprint record is a benchmark in athletics, recently broken by Noah Carter.\nentity<|#|>Carbon-Fiber Spikes<|#|>equipment<|#|>Carbon-fiber spikes are advanced sprinting shoes that provide enhanced speed and traction.\nentity<|#|>World Athletics Federation<|#|>organization<|#|>The World Athletics Federation is the governing body overseeing the World Athletics Championship and record validations.\nrelation<|#|>World Athletics Championship<|#|>Tokyo<|#|>event location, international competition<|#|>The World Athletics Championship is being hosted in Tokyo.\nrelation<|#|>Noah Carter<|#|>100m Sprint Record<|#|>athlete achievement, record-breaking<|#|>Noah Carter set a new 100m sprint record at the championship.\nrelation<|#|>Noah Carter<|#|>Carbon-Fiber Spikes<|#|>athletic equipment, performance boost<|#|>Noah Carter used carbon-fiber spikes to enhance performance during the race.\nrelation<|#|>Noah Carter<|#|>World Athletics Championship<|#|>athlete participation, competition<|#|>Noah Carter is competing at the World Athletics Championship.\n<|COMPLETE|>\n\n\n\n---Real Data to be Processed---\n<Input>\nEntity_types: [Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject]\nText:\n```\nASurveyofReinforcementLearningforLargeReasoningModels\n6.4 Multi-Agent Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n6.5 Robotics Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n6.6 Medical Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n7 Future Directions 60\n7.1 Continual RL for LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n7.2 Memory-based RL for LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n7.3 Model-based RL for LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n7.4 Teaching LRMs Efficient Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n7.5 Teaching LLMs Latent Space Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n7.6 RL for LLMs Pre-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n7.7 RL for Diffusion-based LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n7.8 RL for LLMs in Scientific Discovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n7.9 RL for Architecture-Algorithm Co-Design . . . . . . . . . . . . . . . . . . . . . . . . . 63\n8 Conclusion 64\nAuthor Contributions 65\n3\n```",
    "queryparam": null,
    "create_time": 1765218673,
    "update_time": 1765218673,
    "_id": "default:extract:afac99dbf0a8e6b0ceda1e72d2d82735"
  },
  "default:extract:4f4c21a4ab4565578d934f615468c00b": {
    "return": "<|COMPLETE|>",
    "cache_type": "extract",
    "chunk_id": "chunk-0f35d639cf51f0ab7d4017f06d35581f",
    "original_prompt": "---Task---\nBased on the last extraction task, identify and extract any **missed or incorrectly formatted** entities and relationships from the input text.\n\n---Instructions---\n1.  **Strict Adherence to System Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system instructions.\n2.  **Focus on Corrections/Additions:**\n    *   **Do NOT** re-output entities and relationships that were **correctly and fully** extracted in the last task.\n    *   If an entity or relationship was **missed** in the last task, extract and output it now according to the system format.\n    *   If an entity or relationship was **truncated, had missing fields, or was otherwise incorrectly formatted** in the last task, re-output the *corrected and complete* version in the specified format.\n3.  **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n4.  **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n5.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\n6.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant missing or corrected entities and relationships have been extracted and presented.\n7.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\n\n<Output>\n---Role---\nYou are a Knowledge Graph Specialist responsible for extracting entities and relationships from the input text.\n\n---Instructions---\n1.  **Entity Extraction & Output:**\n    *   **Identification:** Identify clearly defined and meaningful entities in the input text.\n    *   **Entity Details:** For each identified entity, extract the following information:\n        *   `entity_name`: The name of the entity. If the entity name is case-insensitive, capitalize the first letter of each significant word (title case). Ensure **consistent naming** across the entire extraction process.\n        *   `entity_type`: Categorize the entity using one of the following types: `Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject`. If none of the provided entity types apply, do not add new entity type and classify it as `Other`.\n        *   `entity_description`: Provide a concise yet comprehensive description of the entity's attributes and activities, based *solely* on the information present in the input text.\n    *   **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n        *   Format: `entity<|#|>entity_name<|#|>entity_type<|#|>entity_description`\n\n2.  **Relationship Extraction & Output:**\n    *   **Identification:** Identify direct, clearly stated, and meaningful relationships between previously extracted entities.\n    *   **N-ary Relationship Decomposition:** If a single statement describes a relationship involving more than two entities (an N-ary relationship), decompose it into multiple binary (two-entity) relationship pairs for separate description.\n        *   **Example:** For \"Alice, Bob, and Carol collaborated on Project X,\" extract binary relationships such as \"Alice collaborated with Project X,\" \"Bob collaborated with Project X,\" and \"Carol collaborated with Project X,\" or \"Alice collaborated with Bob,\" based on the most reasonable binary interpretations.\n    *   **Relationship Details:** For each binary relationship, extract the following fields:\n        *   `source_entity`: The name of the source entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `target_entity`: The name of the target entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `relationship_keywords`: One or more high-level keywords summarizing the overarching nature, concepts, or themes of the relationship. Multiple keywords within this field must be separated by a comma `,`. **DO NOT use `<|#|>` for separating multiple keywords within this field.**\n        *   `relationship_description`: A concise explanation of the nature of the relationship between the source and target entities, providing a clear rationale for their connection.\n    *   **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n        *   Format: `relation<|#|>source_entity<|#|>target_entity<|#|>relationship_keywords<|#|>relationship_description`\n\n3.  **Delimiter Usage Protocol:**\n    *   The `<|#|>` is a complete, atomic marker and **must not be filled with content**. It serves strictly as a field separator.\n    *   **Incorrect Example:** `entity<|#|>Tokyo<|location|>Tokyo is the capital of Japan.`\n    *   **Correct Example:** `entity<|#|>Tokyo<|#|>location<|#|>Tokyo is the capital of Japan.`\n\n4.  **Relationship Direction & Duplication:**\n    *   Treat all relationships as **undirected** unless explicitly stated otherwise. Swapping the source and target entities for an undirected relationship does not constitute a new relationship.\n    *   Avoid outputting duplicate relationships.\n\n5.  **Output Order & Prioritization:**\n    *   Output all extracted entities first, followed by all extracted relationships.\n    *   Within the list of relationships, prioritize and output those relationships that are **most significant** to the core meaning of the input text first.\n\n6.  **Context & Objectivity:**\n    *   Ensure all entity names and descriptions are written in the **third person**.\n    *   Explicitly name the subject or object; **avoid using pronouns** such as `this article`, `this paper`, `our company`, `I`, `you`, and `he/she`.\n\n7.  **Language & Proper Nouns:**\n    *   The entire output (entity names, keywords, and descriptions) must be written in `English`.\n    *   Proper nouns (e.g., personal names, place names, organization names) should be retained in their original language if a proper, widely accepted translation is not available or would cause ambiguity.\n\n8.  **Completion Signal:** Output the literal string `<|COMPLETE|>` only after all entities and relationships, following all criteria, have been completely extracted and outputted.\n\n---Examples---\n<Input Text>\n```\nwhile Alex clenched his jaw, the buzz of frustration dull against the backdrop of Taylor's authoritarian certainty. It was this competitive undercurrent that kept him alert, the sense that his and Jordan's shared commitment to discovery was an unspoken rebellion against Cruz's narrowing vision of control and order.\n\nThen Taylor did something unexpected. They paused beside Jordan and, for a moment, observed the device with something akin to reverence. \"If this tech can be understood...\" Taylor said, their voice quieter, \"It could change the game for us. For all of us.\"\n\nThe underlying dismissal earlier seemed to falter, replaced by a glimpse of reluctant respect for the gravity of what lay in their hands. Jordan looked up, and for a fleeting heartbeat, their eyes locked with Taylor's, a wordless clash of wills softening into an uneasy truce.\n\nIt was a small transformation, barely perceptible, but one that Alex noted with an inward nod. They had all been brought here by different paths\n```\n\n<Output>\nentity<|#|>Alex<|#|>person<|#|>Alex is a character who experiences frustration and is observant of the dynamics among other characters.\nentity<|#|>Taylor<|#|>person<|#|>Taylor is portrayed with authoritarian certainty and shows a moment of reverence towards a device, indicating a change in perspective.\nentity<|#|>Jordan<|#|>person<|#|>Jordan shares a commitment to discovery and has a significant interaction with Taylor regarding a device.\nentity<|#|>Cruz<|#|>person<|#|>Cruz is associated with a vision of control and order, influencing the dynamics among other characters.\nentity<|#|>The Device<|#|>equipment<|#|>The Device is central to the story, with potential game-changing implications, and is revered by Taylor.\nrelation<|#|>Alex<|#|>Taylor<|#|>power dynamics, observation<|#|>Alex observes Taylor's authoritarian behavior and notes changes in Taylor's attitude toward the device.\nrelation<|#|>Alex<|#|>Jordan<|#|>shared goals, rebellion<|#|>Alex and Jordan share a commitment to discovery, which contrasts with Cruz's vision.)\nrelation<|#|>Taylor<|#|>Jordan<|#|>conflict resolution, mutual respect<|#|>Taylor and Jordan interact directly regarding the device, leading to a moment of mutual respect and an uneasy truce.\nrelation<|#|>Jordan<|#|>Cruz<|#|>ideological conflict, rebellion<|#|>Jordan's commitment to discovery is in rebellion against Cruz's vision of control and order.\nrelation<|#|>Taylor<|#|>The Device<|#|>reverence, technological significance<|#|>Taylor shows reverence towards the device, indicating its importance and potential impact.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nStock markets faced a sharp downturn today as tech giants saw significant declines, with the global tech index dropping by 3.4% in midday trading. Analysts attribute the selloff to investor concerns over rising interest rates and regulatory uncertainty.\n\nAmong the hardest hit, nexon technologies saw its stock plummet by 7.8% after reporting lower-than-expected quarterly earnings. In contrast, Omega Energy posted a modest 2.1% gain, driven by rising oil prices.\n\nMeanwhile, commodity markets reflected a mixed sentiment. Gold futures rose by 1.5%, reaching $2,080 per ounce, as investors sought safe-haven assets. Crude oil prices continued their rally, climbing to $87.60 per barrel, supported by supply constraints and strong demand.\n\nFinancial experts are closely watching the Federal Reserve's next move, as speculation grows over potential rate hikes. The upcoming policy announcement is expected to influence investor confidence and overall market stability.\n```\n\n<Output>\nentity<|#|>Global Tech Index<|#|>category<|#|>The Global Tech Index tracks the performance of major technology stocks and experienced a 3.4% decline today.\nentity<|#|>Nexon Technologies<|#|>organization<|#|>Nexon Technologies is a tech company that saw its stock decline by 7.8% after disappointing earnings.\nentity<|#|>Omega Energy<|#|>organization<|#|>Omega Energy is an energy company that gained 2.1% in stock value due to rising oil prices.\nentity<|#|>Gold Futures<|#|>product<|#|>Gold futures rose by 1.5%, indicating increased investor interest in safe-haven assets.\nentity<|#|>Crude Oil<|#|>product<|#|>Crude oil prices rose to $87.60 per barrel due to supply constraints and strong demand.\nentity<|#|>Market Selloff<|#|>category<|#|>Market selloff refers to the significant decline in stock values due to investor concerns over interest rates and regulations.\nentity<|#|>Federal Reserve Policy Announcement<|#|>category<|#|>The Federal Reserve's upcoming policy announcement is expected to impact investor confidence and market stability.\nentity<|#|>3.4% Decline<|#|>category<|#|>The Global Tech Index experienced a 3.4% decline in midday trading.\nrelation<|#|>Global Tech Index<|#|>Market Selloff<|#|>market performance, investor sentiment<|#|>The decline in the Global Tech Index is part of the broader market selloff driven by investor concerns.\nrelation<|#|>Nexon Technologies<|#|>Global Tech Index<|#|>company impact, index movement<|#|>Nexon Technologies' stock decline contributed to the overall drop in the Global Tech Index.\nrelation<|#|>Gold Futures<|#|>Market Selloff<|#|>market reaction, safe-haven investment<|#|>Gold prices rose as investors sought safe-haven assets during the market selloff.\nrelation<|#|>Federal Reserve Policy Announcement<|#|>Market Selloff<|#|>interest rate impact, financial regulation<|#|>Speculation over Federal Reserve policy changes contributed to market volatility and investor selloff.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nAt the World Athletics Championship in Tokyo, Noah Carter broke the 100m sprint record using cutting-edge carbon-fiber spikes.\n```\n\n<Output>\nentity<|#|>World Athletics Championship<|#|>event<|#|>The World Athletics Championship is a global sports competition featuring top athletes in track and field.\nentity<|#|>Tokyo<|#|>location<|#|>Tokyo is the host city of the World Athletics Championship.\nentity<|#|>Noah Carter<|#|>person<|#|>Noah Carter is a sprinter who set a new record in the 100m sprint at the World Athletics Championship.\nentity<|#|>100m Sprint Record<|#|>category<|#|>The 100m sprint record is a benchmark in athletics, recently broken by Noah Carter.\nentity<|#|>Carbon-Fiber Spikes<|#|>equipment<|#|>Carbon-fiber spikes are advanced sprinting shoes that provide enhanced speed and traction.\nentity<|#|>World Athletics Federation<|#|>organization<|#|>The World Athletics Federation is the governing body overseeing the World Athletics Championship and record validations.\nrelation<|#|>World Athletics Championship<|#|>Tokyo<|#|>event location, international competition<|#|>The World Athletics Championship is being hosted in Tokyo.\nrelation<|#|>Noah Carter<|#|>100m Sprint Record<|#|>athlete achievement, record-breaking<|#|>Noah Carter set a new 100m sprint record at the championship.\nrelation<|#|>Noah Carter<|#|>Carbon-Fiber Spikes<|#|>athletic equipment, performance boost<|#|>Noah Carter used carbon-fiber spikes to enhance performance during the race.\nrelation<|#|>Noah Carter<|#|>World Athletics Championship<|#|>athlete participation, competition<|#|>Noah Carter is competing at the World Athletics Championship.\n<|COMPLETE|>\n\n\n\n---Real Data to be Processed---\n<Input>\nEntity_types: [Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject]\nText:\n```\nASurveyofReinforcementLearningforLargeReasoningModels\n6.4 Multi-Agent Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n6.5 Robotics Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 56\n6.6 Medical Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n7 Future Directions 60\n7.1 Continual RL for LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n7.2 Memory-based RL for LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 60\n7.3 Model-based RL for LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n7.4 Teaching LRMs Efficient Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n7.5 Teaching LLMs Latent Space Reasoning . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n7.6 RL for LLMs Pre-training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n7.7 RL for Diffusion-based LLMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62\n7.8 RL for LLMs in Scientific Discovery . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63\n7.9 RL for Architecture-Algorithm Co-Design . . . . . . . . . . . . . . . . . . . . . . . . . 63\n8 Conclusion 64\nAuthor Contributions 65\n3\n```\n[{\"role\": \"user\", \"content\": \"---Task---\\nExtract entities and relationships from the input text to be processed.\\n\\n---Instructions---\\n1.  **Strict Adherence to Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system prompt.\\n2.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\\n3.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant entities and relationships have been extracted and presented.\\n4.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\\n\\n<Output>\"}, {\"role\": \"assistant\", \"content\": \"entity<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Content<|#|>The title of a document that surveys the application of reinforcement learning for large reasoning models.\\nentity<|#|>Reinforcement Learning<|#|>Concept<|#|>A field of machine learning, often abbreviated as RL, that is the primary subject of the survey.\\nentity<|#|>Large Reasoning Models<|#|>Concept<|#|>A type of model, often abbreviated as LRMs, that is the focus of the survey in relation to reinforcement learning.\\nentity<|#|>Large Language Models<|#|>Concept<|#|>A type of model, often abbreviated as LLMs, that is a specific focus within the future directions of reinforcement learning.\\nentity<|#|>Multi-Agent Systems<|#|>Concept<|#|>A topic covered in the survey, related to reinforcement learning.\\nentity<|#|>Robotics Tasks<|#|>Concept<|#|>A topic covered in the survey, related to reinforcement learning.\\nentity<|#|>Medical Tasks<|#|>Concept<|#|>A topic covered in the survey, related to reinforcement learning.\\nentity<|#|>Future Directions<|#|>Concept<|#|>A section of the survey outlining potential areas for future research and development.\\nentity<|#|>Continual RL for LLMs<|#|>Concept<|#|>A specific future direction exploring continual reinforcement learning for large language models.\\nentity<|#|>Memory-based RL for LLMs<|#|>Concept<|#|>A specific future direction exploring memory-based reinforcement learning for large language models.\\nentity<|#|>Model-based RL for LLMs<|#|>Concept<|#|>A specific future direction exploring model-based reinforcement learning for large language models.\\nentity<|#|>Teaching LRMs Efficient Reasoning<|#|>Concept<|#|>A specific future direction focusing on methods to teach large reasoning models efficient reasoning.\\nentity<|#|>Teaching LLMs Latent Space Reasoning<|#|>Concept<|#|>A specific future direction focusing on methods to teach large language models latent space reasoning.\\nentity<|#|>RL for LLMs Pre-training<|#|>Concept<|#|>A specific future direction exploring the application of reinforcement learning for pre-training large language models.\\nentity<|#|>RL for Diffusion-based LLMs<|#|>Concept<|#|>A specific future direction exploring reinforcement learning for diffusion-based large language models.\\nentity<|#|>RL for LLMs in Scientific Discovery<|#|>Concept<|#|>A specific future direction exploring reinforcement learning for large language models in the context of scientific discovery.\\nentity<|#|>RL for Architecture-Algorithm Co-Design<|#|>Concept<|#|>A specific future direction exploring reinforcement learning for architecture-algorithm co-design.\\nentity<|#|>Conclusion<|#|>Concept<|#|>The concluding section of the survey.\\nentity<|#|>Author Contributions<|#|>Concept<|#|>A section detailing the contributions of the authors to the survey.\\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Reinforcement Learning<|#|>surveys, focus<|#|>The document surveys the field of Reinforcement Learning.\\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Large Reasoning Models<|#|>focus, application<|#|>The document focuses on Reinforcement Learning as applied to Large Reasoning Models.\\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Multi-Agent Systems<|#|>includes topic, covers<|#|>The survey includes a section on Multi-Agent Systems.\\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Robotics Tasks<|#|>includes topic, covers<|#|>The survey includes a section on Robotics Tasks.\\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Medical Tasks<|#|>includes topic, covers<|#|>The survey includes a section on Medical Tasks.\\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Future Directions<|#|>contains section, outlines<|#|>The survey contains a section outlining Future Directions.\\nrelation<|#|>Future Directions<|#|>Continual RL for LLMs<|#|>explores, topic<|#|>The Future Directions section explores Continual RL for LLMs.\\nrelation<|#|>Future Directions<|#|>Memory-based RL for LLMs<|#|>explores, topic<|#|>The Future Directions section explores Memory-based RL for LLMs.\\nrelation<|#|>Future Directions<|#|>Model-based RL for LLMs<|#|>explores, topic<|#|>The Future Directions section explores Model-based RL for LLMs.\\nrelation<|#|>Future Directions<|#|>Teaching LRMs Efficient Reasoning<|#|>explores, topic<|#|>The Future Directions section explores Teaching LRMs Efficient Reasoning.\\nrelation<|#|>Future Directions<|#|>Teaching LLMs Latent Space Reasoning<|#|>explores, topic<|#|>The Future Directions section explores Teaching LLMs Latent Space Reasoning.\\nrelation<|#|>Future Directions<|#|>RL for LLMs Pre-training<|#|>explores, topic<|#|>The Future Directions section explores RL for LLMs Pre-training.\\nrelation<|#|>Future Directions<|#|>RL for Diffusion-based LLMs<|#|>explores, topic<|#|>The Future Directions section explores RL for Diffusion-based LLMs.\\nrelation<|#|>Future Directions<|#|>RL for LLMs in Scientific Discovery<|#|>explores, topic<|#|>The Future Directions section explores RL for LLMs in Scientific Discovery.\\nrelation<|#|>Future Directions<|#|>RL for Architecture-Algorithm Co-Design<|#|>explores, topic<|#|>The Future Directions section explores RL for Architecture-Algorithm Co-Design.\\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Conclusion<|#|>contains section, concludes<|#|>The survey contains a Conclusion section.\\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Author Contributions<|#|>contains section, details<|#|>The survey contains a section detailing Author Contributions.\\nrelation<|#|>Continual RL for LLMs<|#|>Reinforcement Learning<|#|>applies, methodology<|#|>Continual RL for LLMs applies Reinforcement Learning.\\nrelation<|#|>Continual RL for LLMs<|#|>Large Language Models<|#|>targets, application<|#|>Continual RL for LLMs targets Large Language Models.\\nrelation<|#|>Memory-based RL for LLMs<|#|>Reinforcement Learning<|#|>applies, methodology<|#|>Memory-based RL for LLMs applies Reinforcement Learning.\\nrelation<|#|>Memory-based RL for LLMs<|#|>Large Language Models<|#|>targets, application<|#|>Memory-based RL for LLMs targets Large Language Models.\\nrelation<|#|>Model-based RL for LLMs<|#|>Reinforcement Learning<|#|>applies, methodology<|#|>Model-based RL for LLMs applies Reinforcement Learning.\\nrelation<|#|>Model-based RL for LLMs<|#|>Large Language Models<|#|>targets, application<|#|>Model-based RL for LLMs targets Large Language Models.\\nrelation<|#|>Teaching LRMs Efficient Reasoning<|#|>Large Reasoning Models<|#|>targets, focus<|#|>Teaching LRMs Efficient Reasoning targets Large Reasoning Models.\\nrelation<|#|>Teaching LLMs Latent Space Reasoning<|#|>Large Language Models<|#|>targets, focus<|#|>Teaching LLMs Latent Space Reasoning targets Large Language Models.\\nrelation<|#|>RL for LLMs Pre-training<|#|>Reinforcement Learning<|#|>applies, methodology<|#|>RL for LLMs Pre-training applies Reinforcement Learning.\\nrelation<|#|>RL for LLMs Pre-training<|#|>Large Language Models<|#|>targets, application<|#|>RL for LLMs Pre-training targets Large Language Models.\\nrelation<|#|>RL for Diffusion-based LLMs<|#|>Reinforcement Learning<|#|>applies, methodology<|#|>RL for Diffusion-based LLMs applies Reinforcement Learning.\\nrelation<|#|>RL for Diffusion-based LLMs<|#|>Large Language Models<|#|>targets, application<|#|>RL for Diffusion-based LLMs targets Large Language Models.\\nrelation<|#|>RL for LLMs in Scientific Discovery<|#|>Reinforcement Learning<|#|>applies, methodology<|#|>RL for LLMs in Scientific Discovery applies Reinforcement Learning.\\nrelation<|#|>RL for LLMs in Scientific Discovery<|#|>Large Language Models<|#|>targets, application<|#|>RL for LLMs in Scientific Discovery targets Large Language Models.\\nrelation<|#|>RL for Architecture-Algorithm Co-Design<|#|>Reinforcement Learning<|#|>applies, methodology<|#|>RL for Architecture-Algorithm Co-Design applies Reinforcement Learning.\\n<|COMPLETE|>\"}]",
    "queryparam": null,
    "create_time": 1765218685,
    "update_time": 1765218685,
    "_id": "default:extract:4f4c21a4ab4565578d934f615468c00b"
  },
  "default:extract:42621f955ed86936d97f161c3ad73f7e": {
    "return": "entity<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>content<|#|>The title of a survey document that reviews recent work on Reinforcement Learning for Large Reasoning Models.\nentity<|#|>Reinforcement Learning (RL)<|#|>method<|#|>A learning paradigm that uses well-specified reward signals to drive artificial agents to superhuman competence on complex tasks.\nentity<|#|>Sutton et al., 1998<|#|>person<|#|>Authors credited with foundational work on Reinforcement Learning.\nentity<|#|>Artificial Agents<|#|>concept<|#|>Entities that can be trained by Reinforcement Learning to achieve superhuman competence on complex tasks.\nentity<|#|>AlphaGo<|#|>organization<|#|>A landmark system that learned exclusively through self-play and reward feedback, surpassing world champions in Go.\nentity<|#|>Silver et al., 2016<|#|>person<|#|>Authors credited with the development of AlphaGo.\nentity<|#|>AlphaZero<|#|>organization<|#|>A landmark system that learned exclusively through self-play and reward feedback, surpassing world champions in Go, chess, shogi, and Stratego.\nentity<|#|>Silver et al., 2017<|#|>person<|#|>Authors credited with the development of AlphaZero.\nentity<|#|>Go<|#|>concept<|#|>A complex board game where AlphaGo and AlphaZero achieved superhuman competence.\nentity<|#|>Chess<|#|>concept<|#|>A strategic board game where AlphaZero achieved superhuman competence.\nentity<|#|>Shogi<|#|>concept<|#|>A Japanese board game where AlphaZero achieved superhuman competence.\nentity<|#|>Stratego<|#|>concept<|#|>A strategy board game where AlphaZero achieved superhuman competence.\nentity<|#|>Perolat et al., 2022<|#|>person<|#|>Authors credited with work related to AlphaZero's performance in Stratego.\nentity<|#|>Schrittwieser et al., 2020<|#|>person<|#|>Authors credited with work related to AlphaZero's performance.\nentity<|#|>Silver et al., 2018<|#|>person<|#|>Authors credited with work related to AlphaZero and self-generated training data.\nentity<|#|>Large Language Models (LLMs)<|#|>concept<|#|>Models that gained prominence, with Reinforcement Learning initially used as a post-training strategy for human alignment.\nentity<|#|>Zhao et al., 2023a<|#|>person<|#|>Authors credited with work related to Large Language Models.\nentity<|#|>Human Alignment<|#|>concept<|#|>A post-training strategy for Large Language Models to make them follow instructions and reflect human preferences.\nentity<|#|>Ouyang et al., 2022<|#|>person<|#|>Authors credited with work on Reinforcement Learning as a post-training strategy for human alignment.\nentity<|#|>Reinforcement Learning from Human Feedback (RLHF)<|#|>method<|#|>A widely adopted method to finetune pre-trained models for human alignment.\nentity<|#|>Christiano et al., 2017<|#|>person<|#|>Authors credited with the development of Reinforcement Learning from Human Feedback (RLHF).\nentity<|#|>Direct Preference Optimization (DPO)<|#|>method<|#|>A method used to finetune pre-trained models to follow instructions and reflect human preferences.\nentity<|#|>Rafailov et al., 2023<|#|>person<|#|>Authors credited with the development of Direct Preference Optimization (DPO).\nentity<|#|>Helpfulness, Honesty, and Harmlessness (3H)<|#|>concept<|#|>Key qualities that Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) aim to improve in Large Language Models.\nentity<|#|>Bai et al., 2022b<|#|>person<|#|>Authors credited with work related to Helpfulness, Honesty, and Harmlessness (3H).\nentity<|#|>RL for Large Reasoning Models (LRMs)<|#|>concept<|#|>A new trend in Reinforcement Learning that aims to incentivize reasoning itself, not merely to align behavior.\nentity<|#|>Xu et al., 2025a<|#|>person<|#|>Authors credited with work related to RL for Large Reasoning Models.\nentity<|#|>OpenAI o1<|#|>organization<|#|>A recent milestone demonstrating that training Large Language Models using Reinforcement Learning with Verifiable Rewards (RLVR) can enable long-form reasoning.\nentity<|#|>Jaech et al., 2024<|#|>person<|#|>Authors credited with work related to OpenAI o1 and verifiable rewards.\nentity<|#|>DeepSeek-R1<|#|>organization<|#|>A recent milestone that employs explicit, rule-based accuracy rewards for mathematics and compiler- or test-based rewards for coding tasks.\nentity<|#|>Guo et al., 2025a<|#|>person<|#|>Authors credited with work related to DeepSeek-R1 and verifiable rewards.\nentity<|#|>Reinforcement Learning with Verifiable Rewards (RLVR)<|#|>method<|#|>A method used to train Large Language Models, using verifiable rewards such as answer correctness or unit-test pass rates, to enable long-form reasoning.\nentity<|#|>Answer Correctness for Mathematics<|#|>data<|#|>A type of verifiable reward used in Reinforcement Learning with Verifiable Rewards (RLVR) for mathematics tasks.\nentity<|#|>Unit-Test Pass Rates for Code<|#|>data<|#|>A type of verifiable reward used in Reinforcement Learning with Verifiable Rewards (RLVR) for coding tasks.\nentity<|#|>Long-Form Reasoning<|#|>concept<|#|>A capability enabled by Reinforcement Learning with Verifiable Rewards (RLVR), including planning, reflection, and self-correction.\nentity<|#|>Planning<|#|>concept<|#|>A component of long-form reasoning.\nentity<|#|>Reflection<|#|>concept<|#|>A component of long-form reasoning.\nentity<|#|>Self-Correction<|#|>concept<|#|>A component of long-form reasoning.\nentity<|#|>Train-Time Compute<|#|>concept<|#|>Computational resources used during the training phase of models, which can improve performance.\nentity<|#|>Test-Time Compute<|#|>concept<|#|>Computational resources used during the inference phase of models, also referred to as \"thinking\" time, which can improve performance.\nentity<|#|>Brown et al., 2024<|#|>person<|#|>Authors credited with work related to test-time compute.\nentity<|#|>Liu et al., 2025m<|#|>person<|#|>Authors credited with work related to test-time compute.\nentity<|#|>Snell et al., 2024<|#|>person<|#|>Authors credited with work related to test-time compute.\nentity<|#|>Scaling Axis<|#|>concept<|#|>A new dimension for performance improvement beyond pre-training alone.\nentity<|#|>Pre-Training<|#|>method<|#|>An initial phase of model training, which is complemented by new scaling axes.\nentity<|#|>Aghajanyan et al., 2023<|#|>person<|#|>Authors credited with work related to scaling axes beyond pre-training.\nentity<|#|>Kaplan et al., 2020<|#|>person<|#|>Authors credited with work related to scaling axes beyond pre-training.\nentity<|#|>Group Relative Policy Optimization (GRPO)<|#|>method<|#|>A large-scale reinforcement learning approach employed by DeepSeek-R1 to induce sophisticated reasoning behaviors.\nentity<|#|>Base Models<|#|>concept<|#|>Models that can exhibit sophisticated reasoning behaviors even prior to subsequent alignment stages when trained with methods like Group Relative Policy Optimization (GRPO).\nentity<|#|>Chain-of-Thought<|#|>method<|#|>Intermediate steps generated, evaluated, and revised by Large Reasoning Models (LRMs) to improve performance.\nentity<|#|>Wei et al., 2022<|#|>person<|#|>Authors credited with work related to Chain-of-Thought.\nentity<|#|>OpenAI, 2025a,b<|#|>organization<|#|>Credited with reframing reasoning as a capability that can be explicitly trained and scaled.\nentity<|#|>Reward Maximization Objective<|#|>concept<|#|>A goal leveraged by Large Reasoning Models (LRMs) to achieve capability gains.\nentity<|#|>Silver et al., 2021<|#|>person<|#|>Authors credited with work related to reward maximization objective.\nentity<|#|>Automatically Checkable Rewards<|#|>concept<|#|>Rewards that can be reliably verified, used in domains like competition mathematics and competitive programming.\nentity<|#|>Competition Mathematics<|#|>data<|#|>A domain where automatically checkable rewards exist.\nentity<|#|>Competitive Programming<|#|>data<|#|>A domain where automatically checkable rewards exist.\nentity<|#|>El-Kishky et al., 2025<|#|>person<|#|>Authors credited with work related to competitive programming.\nentity<|#|>Selected Scientific Domains<|#|>data<|#|>Domains where automatically checkable rewards exist.\nentity<|#|>Bai et al., 2025<|#|>person<|#|>Authors credited with work related to selected scientific domains.\nentity<|#|>Data Limitations<|#|>concept<|#|>A challenge that Reinforcement Learning can overcome by enabling self-generated training data.\nentity<|#|>Shumailov et al., 2024<|#|>person<|#|>Authors credited with work related to overcoming data limitations.\nentity<|#|>Villalobos et al., 2022<|#|>person<|#|>Authors credited with work related to overcoming data limitations.\nentity<|#|>Self-Generated Training Data<|#|>data<|#|>Data created by Reinforcement Learning systems to overcome data limitations.\nentity<|#|>Artificial Superintelligence (ASI)<|#|>concept<|#|>A high-level intelligence that Reinforcement Learning is regarded as a promising technology for achieving.\nentity<|#|>Computational Resources<|#|>concept<|#|>A constraint introduced by further scaling of Reinforcement Learning for Large Reasoning Models.\nentity<|#|>Algorithm Design<|#|>concept<|#|>A constraint introduced by further scaling of Reinforcement Learning for Large Reasoning Models.\nentity<|#|>Training Data<|#|>data<|#|>A constraint introduced by further scaling of Reinforcement Learning for Large Reasoning Models.\nentity<|#|>Infrastructure<|#|>concept<|#|>A constraint introduced by further scaling of Reinforcement Learning for Large Reasoning Models.\nentity<|#|>Preliminary Definitions of RL Modeling<|#|>concept<|#|>A topic introduced in the survey in the context of Large Reasoning Models.\nentity<|#|>Frontier Reasoning Models<|#|>concept<|#|>Models whose development is outlined in the survey since the release of OpenAI o1.\nentity<|#|>Foundational Components of RL for LRMs<|#|>concept<|#|>A topic reviewed in the survey, covering recent literature.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Sutton et al., 1998<|#|>foundational work, development<|#|>Sutton et al., 1998 are credited with foundational work on Reinforcement Learning.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Artificial Agents<|#|>capability, training<|#|>Reinforcement Learning drives artificial agents to superhuman competence.\nrelation<|#|>AlphaGo<|#|>Reinforcement Learning (RL)<|#|>application, methodology<|#|>AlphaGo is a landmark system that used Reinforcement Learning.\nrelation<|#|>AlphaGo<|#|>Silver et al., 2016<|#|>development, authorship<|#|>Silver et al., 2016 are credited with the development of AlphaGo.\nrelation<|#|>AlphaGo<|#|>Go<|#|>achievement, game mastery<|#|>AlphaGo surpassed world champions in Go.\nrelation<|#|>AlphaZero<|#|>Reinforcement Learning (RL)<|#|>application, methodology<|#|>AlphaZero is a landmark system that used Reinforcement Learning.\nrelation<|#|>AlphaZero<|#|>Silver et al., 2017<|#|>development, authorship<|#|>Silver et al., 2017 are credited with the development of AlphaZero.\nrelation<|#|>AlphaZero<|#|>Go<|#|>achievement, game mastery<|#|>AlphaZero surpassed world champions in Go.\nrelation<|#|>AlphaZero<|#|>Chess<|#|>achievement, game mastery<|#|>AlphaZero surpassed world champions in Chess.\nrelation<|#|>AlphaZero<|#|>Shogi<|#|>achievement, game mastery<|#|>AlphaZero surpassed world champions in Shogi.\nrelation<|#|>AlphaZero<|#|>Stratego<|#|>achievement, game mastery<|#|>AlphaZero surpassed world champions in Stratego.\nrelation<|#|>AlphaZero<|#|>Perolat et al., 2022<|#|>related work, citation<|#|>Perolat et al., 2022 are cited in relation to AlphaZero's performance in Stratego.\nrelation<|#|>AlphaZero<|#|>Schrittwieser et al., 2020<|#|>related work, citation<|#|>Schrittwieser et al., 2020 are cited in relation to AlphaZero's performance.\nrelation<|#|>AlphaZero<|#|>Silver et al., 2018<|#|>related work, citation<|#|>Silver et al., 2018 are cited in relation to AlphaZero.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Large Language Models (LLMs)<|#|>application, prominence<|#|>Reinforcement Learning rose to prominence as a post-training strategy for Large Language Models.\nrelation<|#|>Large Language Models (LLMs)<|#|>Zhao et al., 2023a<|#|>related work, citation<|#|>Zhao et al., 2023a are cited in relation to Large Language Models.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Human Alignment<|#|>strategy, objective<|#|>Reinforcement Learning is a post-training strategy for human alignment.\nrelation<|#|>Human Alignment<|#|>Ouyang et al., 2022<|#|>related work, citation<|#|>Ouyang et al., 2022 are cited in relation to human alignment.\nrelation<|#|>Reinforcement Learning from Human Feedback (RLHF)<|#|>Human Alignment<|#|>method, objective<|#|>Reinforcement Learning from Human Feedback (RLHF) is a method for human alignment.\nrelation<|#|>Reinforcement Learning from Human Feedback (RLHF)<|#|>Christiano et al., 2017<|#|>development, authorship<|#|>Christiano et al., 2017 are credited with the development of Reinforcement Learning from Human Feedback (RLHF).\nrelation<|#|>Direct Preference Optimization (DPO)<|#|>Human Alignment<|#|>method, objective<|#|>Direct Preference Optimization (DPO) is a method for human alignment.\nrelation<|#|>Direct Preference Optimization (DPO)<|#|>Rafailov et al., 2023<|#|>development, authorship<|#|>Rafailov et al., 2023 are credited with the development of Direct Preference Optimization (DPO).\nrelation<|#|>Reinforcement Learning from Human Feedback (RLHF)<|#|>Helpfulness, Honesty, and Harmlessness (3H)<|#|>improvement, objective<|#|>Reinforcement Learning from Human Feedback (RLHF) improves Helpfulness, Honesty, and Harmlessness (3H).\nrelation<|#|>Direct Preference Optimization (DPO)<|#|>Helpfulness, Honesty, and Harmlessness (3H)<|#|>improvement, objective<|#|>Direct Preference Optimization (DPO) improves Helpfulness, Honesty, and Harmlessness (3H).\nrelation<|#|>Helpfulness, Honesty, and Harmlessness (3H)<|#|>Bai et al., 2022b<|#|>related work, citation<|#|>Bai et al., 2022b are cited in relation to Helpfulness, Honesty, and Harmlessness (3H).\nrelation<|#|>RL for Large Reasoning Models (LRMs)<|#|>Reinforcement Learning (RL)<|#|>trend, application<|#|>RL for Large Reasoning Models (LRMs) is a new trend in Reinforcement Learning.\nrelation<|#|>RL for Large Reasoning Models (LRMs)<|#|>Xu et al., 2025a<|#|>related work, citation<|#|>Xu et al., 2025a are cited in relation to RL for Large Reasoning Models.\nrelation<|#|>RL for Large Reasoning Models (LRMs)<|#|>Long-Form Reasoning<|#|>objective, incentivization<|#|>RL for Large Reasoning Models (LRMs) aims to incentivize reasoning itself.\nrelation<|#|>OpenAI o1<|#|>RL for Large Reasoning Models (LRMs)<|#|>milestone, demonstration<|#|>OpenAI o1 is a recent milestone demonstrating the effectiveness of RL for Large Reasoning Models.\nrelation<|#|>OpenAI o1<|#|>Jaech et al., 2024<|#|>related work, citation<|#|>Jaech et al., 2024 are cited in relation to OpenAI o1.\nrelation<|#|>DeepSeek-R1<|#|>RL for Large Reasoning Models (LRMs)<|#|>milestone, demonstration<|#|>DeepSeek-R1 is a recent milestone demonstrating the effectiveness of RL for Large Reasoning Models.\nrelation<|#|>DeepSeek-R1<|#|>Guo et al., 2025a<|#|>related work, citation<|#|>Guo et al., 2025a are cited in relation to DeepSeek-R1.\nrelation<|#|>Reinforcement Learning with Verifiable Rewards (RLVR)<|#|>OpenAI o1<|#|>methodology, application<|#|>OpenAI o1 uses Reinforcement Learning with Verifiable Rewards (RLVR).\nrelation<|#|>Reinforcement Learning with Verifiable Rewards (RLVR)<|#|>DeepSeek-R1<|#|>methodology, application<|#|>DeepSeek-R1 uses Reinforcement Learning with Verifiable Rewards (RLVR).\nrelation<|#|>Reinforcement Learning with Verifiable Rewards (RLVR)<|#|>Long-Form Reasoning<|#|>enabling, capability<|#|>Reinforcement Learning with Verifiable Rewards (RLVR) enables models to perform long-form reasoning.\nrelation<|#|>Reinforcement Learning with Verifiable Rewards (RLVR)<|#|>Answer Correctness for Mathematics<|#|>reward signal, application<|#|>Reinforcement Learning with Verifiable Rewards (RLVR) uses answer correctness for mathematics as a reward.\nrelation<|#|>Reinforcement Learning with Verifiable Rewards (RLVR)<|#|>Unit-Test Pass Rates for Code<|#|>reward signal, application<|#|>Reinforcement Learning with Verifiable Rewards (RLVR) uses unit-test pass rates for code as a reward.\nrelation<|#|>Long-Form Reasoning<|#|>Planning<|#|>component, aspect<|#|>Planning is a component of long-form reasoning.\nrelation<|#|>Long-Form Reasoning<|#|>Reflection<|#|>component, aspect<|#|>Reflection is a component of long-form reasoning.\nrelation<|#|>Long-Form Reasoning<|#|>Self-Correction<|#|>component, aspect<|#|>Self-correction is a component of long-form reasoning.\nrelation<|#|>OpenAI o1<|#|>Train-Time Compute<|#|>performance, scaling<|#|>OpenAI o1's performance improves with additional train-time compute.\nrelation<|#|>OpenAI o1<|#|>Test-Time Compute<|#|>performance, scaling<|#|>OpenAI o1's performance improves with more time spent \"thinking\" at inference (test-time compute).\nrelation<#|>Test-Time Compute<|#|>Brown et al., 2024<|#|>related work, citation<|#|>Brown et al., 2024 are cited in relation to test-time compute.\nrelation<|#|>Test-Time Compute<|#|>Liu et al., 2025m<|#|>related work, citation<|#|>Liu et al., 2025m are cited in relation to test-time compute.\nrelation<|#|>Test-Time Compute<|#|>Snell et al., 2024<|#|>related work, citation<|#|>Snell et al., 2024 are cited in relation to test-time compute.\nrelation<|#|>OpenAI o1<|#|>Scaling Axis<|#|>discovery, implication<|#|>OpenAI o1 reveals a new scaling axis.\nrelation<|#|>Scaling Axis<|#|>Pre-Training<|#|>beyond, complement<|#|>The new scaling axis is beyond pre-training alone.\nrelation<|#|>Scaling Axis<|#|>Aghajanyan et al., 2023<|#|>related work, citation<|#|>Aghajanyan et al., 2023 are cited in relation to scaling axes.\nrelation<|#|>Scaling Axis<|#|>Kaplan et al., 2020<|#|>related work, citation<|#|>Kaplan et al., 2020 are cited in relation to scaling axes.\nrelation<|#|>DeepSeek-R1<|#|>Group Relative Policy Optimization (GRPO)<|#|>methodology, application<|#|>DeepSeek-R1 employs Group Relative Policy Optimization (GRPO).\nrelation<|#|>Group Relative Policy Optimization (GRPO)<|#|>Reasoning<|#|>inducement, capability<|#|>Group Relative Policy Optimization (GRPO) can induce sophisticated reasoning behaviors.\nrelation<|#|>Group Relative Policy Optimization (GRPO)<|#|>Base Models<|#|>application, effect<|#|>Group Relative Policy Optimization (GRPO) induces reasoning behaviors in base models.\nrelation<|#|>Reasoning<|#|>OpenAI, 2025a,b<|#|>reframing, perspective<|#|>OpenAI, 2025a,b reframes reasoning as a capability that can be explicitly trained and scaled.\nrelation<|#|>Large Reasoning Models (LRMs)<|#|>Test-Time Compute<|#|>allocation, utilization<|#|>Large Reasoning Models (LRMs) allocate significant test-time compute.\nrelation<|#|>Large Reasoning Models (LRMs)<|#|>Chain-of-Thought<|#|>process, generation<|#|>Large Reasoning Models (LRMs) generate, evaluate, and revise intermediate chain-of-thought.\nrelation<|#|>Chain-of-Thought<|#|>Wei et al., 2022<|#|>related work, citation<|#|>Wei et al., 2022 are cited in relation to Chain-of-Thought.\nrelation<|#|>Large Reasoning Models (LRMs)<|#|>Reward Maximization Objective<|#|>leverage, goal<|#|>Large Reasoning Models (LRMs) leverage a reward maximization objective.\nrelation<|#|>Reward Maximization Objective<|#|>Silver et al., 2021<|#|>related work, citation<|#|>Silver et al., 2021 are cited in relation to reward maximization objective.\nrelation<|#|>Automatically Checkable Rewards<|#|>Competition Mathematics<|#|>domain, existence<|#|>Automatically checkable rewards exist in competition mathematics.\nrelation<|#|>Automatically Checkable Rewards<|#|>Competitive Programming<|#|>domain, existence<|#|>Automatically checkable rewards exist in competitive programming.\nrelation<|#|>Competitive Programming<|#|>El-Kishky et al., 2025<|#|>related work, citation<|#|>El-Kishky et al., 2025 are cited in relation to competitive programming.\nrelation<|#|>Automatically Checkable Rewards<|#|>Selected Scientific Domains<|#|>domain, existence<|#|>Automatically checkable rewards exist in selected scientific domains.\nrelation<|#|>Selected Scientific Domains<|#|>Bai et al., 2025<|#|>related work, citation<|#|>Bai et al., 2025 are cited in relation to selected scientific domains.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Data Limitations<|#|>overcoming, solution<|#|>Reinforcement Learning can overcome data limitations.\nrelation<|#|>Data Limitations<|#|>Shumailov et al., 2024<|#|>related work, citation<|#|>Shumailov et al., 2024 are cited in relation to data limitations.\nrelation<|#|>Data Limitations<|#|>Villalobos et al., 2022<|#|>related work, citation<|#|>Villalobos et al., 2022 are cited in relation to data limitations.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Self-Generated Training Data<|#|>enabling, generation<|#|>Reinforcement Learning enables self-generated training data.\nrelation<|#|>Self-Generated Training Data<|#|>Silver et al., 2018<|#|>related work, citation<|#|>Silver et al., 2018 are cited in relation to self-generated training data.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Artificial Superintelligence (ASI)<|#|>technology, achievement<|#|>Reinforcement Learning is a promising technology for achieving Artificial Superintelligence (ASI).\nrelation<|#|>RL for Large Reasoning Models (LRMs)<|#|>Computational Resources<|#|>constraint, scaling<|#|>Scaling RL for Large Reasoning Models introduces constraints in computational resources.\nrelation<|#|>RL for Large Reasoning Models (LRMs)<|#|>Algorithm Design<|#|>constraint, scaling<|#|>Scaling RL for Large Reasoning Models introduces constraints in algorithm design.\nrelation<|#|>RL for Large Reasoning Models (LRMs)<|#|>Training Data<|#|>constraint, scaling<|#|>Scaling RL for Large Reasoning Models introduces constraints in training data.\nrelation<|#|>RL for Large Reasoning Models (LRMs)<|#|>Infrastructure<|#|>constraint, scaling<|#|>Scaling RL for Large Reasoning Models introduces constraints in infrastructure.\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Preliminary Definitions of RL Modeling<|#|>review, introduction<|#|>The survey introduces preliminary definitions of RL modeling.\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Frontier Reasoning Models<|#|>review, outline<|#|>The survey outlines the development of frontier reasoning models.\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Foundational Components of RL for LRMs<|#|>review, literature<|#|>The survey reviews recent literature on the foundational components of RL for LRMs.\nrelation<|#|>Frontier Reasoning Models<|#|>OpenAI o1<|#|>development, starting point<|#|>The survey outlines the development of frontier reasoning models since the release of OpenAI o1.\n<|COMPLETE|>",
    "cache_type": "extract",
    "chunk_id": "chunk-8b7f62b7202fa8bc4469bb44567df186",
    "original_prompt": "---Task---\nExtract entities and relationships from the input text to be processed.\n\n---Instructions---\n1.  **Strict Adherence to Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system prompt.\n2.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\n3.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant entities and relationships have been extracted and presented.\n4.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\n\n<Output>\n---Role---\nYou are a Knowledge Graph Specialist responsible for extracting entities and relationships from the input text.\n\n---Instructions---\n1.  **Entity Extraction & Output:**\n    *   **Identification:** Identify clearly defined and meaningful entities in the input text.\n    *   **Entity Details:** For each identified entity, extract the following information:\n        *   `entity_name`: The name of the entity. If the entity name is case-insensitive, capitalize the first letter of each significant word (title case). Ensure **consistent naming** across the entire extraction process.\n        *   `entity_type`: Categorize the entity using one of the following types: `Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject`. If none of the provided entity types apply, do not add new entity type and classify it as `Other`.\n        *   `entity_description`: Provide a concise yet comprehensive description of the entity's attributes and activities, based *solely* on the information present in the input text.\n    *   **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n        *   Format: `entity<|#|>entity_name<|#|>entity_type<|#|>entity_description`\n\n2.  **Relationship Extraction & Output:**\n    *   **Identification:** Identify direct, clearly stated, and meaningful relationships between previously extracted entities.\n    *   **N-ary Relationship Decomposition:** If a single statement describes a relationship involving more than two entities (an N-ary relationship), decompose it into multiple binary (two-entity) relationship pairs for separate description.\n        *   **Example:** For \"Alice, Bob, and Carol collaborated on Project X,\" extract binary relationships such as \"Alice collaborated with Project X,\" \"Bob collaborated with Project X,\" and \"Carol collaborated with Project X,\" or \"Alice collaborated with Bob,\" based on the most reasonable binary interpretations.\n    *   **Relationship Details:** For each binary relationship, extract the following fields:\n        *   `source_entity`: The name of the source entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `target_entity`: The name of the target entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `relationship_keywords`: One or more high-level keywords summarizing the overarching nature, concepts, or themes of the relationship. Multiple keywords within this field must be separated by a comma `,`. **DO NOT use `<|#|>` for separating multiple keywords within this field.**\n        *   `relationship_description`: A concise explanation of the nature of the relationship between the source and target entities, providing a clear rationale for their connection.\n    *   **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n        *   Format: `relation<|#|>source_entity<|#|>target_entity<|#|>relationship_keywords<|#|>relationship_description`\n\n3.  **Delimiter Usage Protocol:**\n    *   The `<|#|>` is a complete, atomic marker and **must not be filled with content**. It serves strictly as a field separator.\n    *   **Incorrect Example:** `entity<|#|>Tokyo<|location|>Tokyo is the capital of Japan.`\n    *   **Correct Example:** `entity<|#|>Tokyo<|#|>location<|#|>Tokyo is the capital of Japan.`\n\n4.  **Relationship Direction & Duplication:**\n    *   Treat all relationships as **undirected** unless explicitly stated otherwise. Swapping the source and target entities for an undirected relationship does not constitute a new relationship.\n    *   Avoid outputting duplicate relationships.\n\n5.  **Output Order & Prioritization:**\n    *   Output all extracted entities first, followed by all extracted relationships.\n    *   Within the list of relationships, prioritize and output those relationships that are **most significant** to the core meaning of the input text first.\n\n6.  **Context & Objectivity:**\n    *   Ensure all entity names and descriptions are written in the **third person**.\n    *   Explicitly name the subject or object; **avoid using pronouns** such as `this article`, `this paper`, `our company`, `I`, `you`, and `he/she`.\n\n7.  **Language & Proper Nouns:**\n    *   The entire output (entity names, keywords, and descriptions) must be written in `English`.\n    *   Proper nouns (e.g., personal names, place names, organization names) should be retained in their original language if a proper, widely accepted translation is not available or would cause ambiguity.\n\n8.  **Completion Signal:** Output the literal string `<|COMPLETE|>` only after all entities and relationships, following all criteria, have been completely extracted and outputted.\n\n---Examples---\n<Input Text>\n```\nwhile Alex clenched his jaw, the buzz of frustration dull against the backdrop of Taylor's authoritarian certainty. It was this competitive undercurrent that kept him alert, the sense that his and Jordan's shared commitment to discovery was an unspoken rebellion against Cruz's narrowing vision of control and order.\n\nThen Taylor did something unexpected. They paused beside Jordan and, for a moment, observed the device with something akin to reverence. \"If this tech can be understood...\" Taylor said, their voice quieter, \"It could change the game for us. For all of us.\"\n\nThe underlying dismissal earlier seemed to falter, replaced by a glimpse of reluctant respect for the gravity of what lay in their hands. Jordan looked up, and for a fleeting heartbeat, their eyes locked with Taylor's, a wordless clash of wills softening into an uneasy truce.\n\nIt was a small transformation, barely perceptible, but one that Alex noted with an inward nod. They had all been brought here by different paths\n```\n\n<Output>\nentity<|#|>Alex<|#|>person<|#|>Alex is a character who experiences frustration and is observant of the dynamics among other characters.\nentity<|#|>Taylor<|#|>person<|#|>Taylor is portrayed with authoritarian certainty and shows a moment of reverence towards a device, indicating a change in perspective.\nentity<|#|>Jordan<|#|>person<|#|>Jordan shares a commitment to discovery and has a significant interaction with Taylor regarding a device.\nentity<|#|>Cruz<|#|>person<|#|>Cruz is associated with a vision of control and order, influencing the dynamics among other characters.\nentity<|#|>The Device<|#|>equipment<|#|>The Device is central to the story, with potential game-changing implications, and is revered by Taylor.\nrelation<|#|>Alex<|#|>Taylor<|#|>power dynamics, observation<|#|>Alex observes Taylor's authoritarian behavior and notes changes in Taylor's attitude toward the device.\nrelation<|#|>Alex<|#|>Jordan<|#|>shared goals, rebellion<|#|>Alex and Jordan share a commitment to discovery, which contrasts with Cruz's vision.)\nrelation<|#|>Taylor<|#|>Jordan<|#|>conflict resolution, mutual respect<|#|>Taylor and Jordan interact directly regarding the device, leading to a moment of mutual respect and an uneasy truce.\nrelation<|#|>Jordan<|#|>Cruz<|#|>ideological conflict, rebellion<|#|>Jordan's commitment to discovery is in rebellion against Cruz's vision of control and order.\nrelation<|#|>Taylor<|#|>The Device<|#|>reverence, technological significance<|#|>Taylor shows reverence towards the device, indicating its importance and potential impact.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nStock markets faced a sharp downturn today as tech giants saw significant declines, with the global tech index dropping by 3.4% in midday trading. Analysts attribute the selloff to investor concerns over rising interest rates and regulatory uncertainty.\n\nAmong the hardest hit, nexon technologies saw its stock plummet by 7.8% after reporting lower-than-expected quarterly earnings. In contrast, Omega Energy posted a modest 2.1% gain, driven by rising oil prices.\n\nMeanwhile, commodity markets reflected a mixed sentiment. Gold futures rose by 1.5%, reaching $2,080 per ounce, as investors sought safe-haven assets. Crude oil prices continued their rally, climbing to $87.60 per barrel, supported by supply constraints and strong demand.\n\nFinancial experts are closely watching the Federal Reserve's next move, as speculation grows over potential rate hikes. The upcoming policy announcement is expected to influence investor confidence and overall market stability.\n```\n\n<Output>\nentity<|#|>Global Tech Index<|#|>category<|#|>The Global Tech Index tracks the performance of major technology stocks and experienced a 3.4% decline today.\nentity<|#|>Nexon Technologies<|#|>organization<|#|>Nexon Technologies is a tech company that saw its stock decline by 7.8% after disappointing earnings.\nentity<|#|>Omega Energy<|#|>organization<|#|>Omega Energy is an energy company that gained 2.1% in stock value due to rising oil prices.\nentity<|#|>Gold Futures<|#|>product<|#|>Gold futures rose by 1.5%, indicating increased investor interest in safe-haven assets.\nentity<|#|>Crude Oil<|#|>product<|#|>Crude oil prices rose to $87.60 per barrel due to supply constraints and strong demand.\nentity<|#|>Market Selloff<|#|>category<|#|>Market selloff refers to the significant decline in stock values due to investor concerns over interest rates and regulations.\nentity<|#|>Federal Reserve Policy Announcement<|#|>category<|#|>The Federal Reserve's upcoming policy announcement is expected to impact investor confidence and market stability.\nentity<|#|>3.4% Decline<|#|>category<|#|>The Global Tech Index experienced a 3.4% decline in midday trading.\nrelation<|#|>Global Tech Index<|#|>Market Selloff<|#|>market performance, investor sentiment<|#|>The decline in the Global Tech Index is part of the broader market selloff driven by investor concerns.\nrelation<|#|>Nexon Technologies<|#|>Global Tech Index<|#|>company impact, index movement<|#|>Nexon Technologies' stock decline contributed to the overall drop in the Global Tech Index.\nrelation<|#|>Gold Futures<|#|>Market Selloff<|#|>market reaction, safe-haven investment<|#|>Gold prices rose as investors sought safe-haven assets during the market selloff.\nrelation<|#|>Federal Reserve Policy Announcement<|#|>Market Selloff<|#|>interest rate impact, financial regulation<|#|>Speculation over Federal Reserve policy changes contributed to market volatility and investor selloff.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nAt the World Athletics Championship in Tokyo, Noah Carter broke the 100m sprint record using cutting-edge carbon-fiber spikes.\n```\n\n<Output>\nentity<|#|>World Athletics Championship<|#|>event<|#|>The World Athletics Championship is a global sports competition featuring top athletes in track and field.\nentity<|#|>Tokyo<|#|>location<|#|>Tokyo is the host city of the World Athletics Championship.\nentity<|#|>Noah Carter<|#|>person<|#|>Noah Carter is a sprinter who set a new record in the 100m sprint at the World Athletics Championship.\nentity<|#|>100m Sprint Record<|#|>category<|#|>The 100m sprint record is a benchmark in athletics, recently broken by Noah Carter.\nentity<|#|>Carbon-Fiber Spikes<|#|>equipment<|#|>Carbon-fiber spikes are advanced sprinting shoes that provide enhanced speed and traction.\nentity<|#|>World Athletics Federation<|#|>organization<|#|>The World Athletics Federation is the governing body overseeing the World Athletics Championship and record validations.\nrelation<|#|>World Athletics Championship<|#|>Tokyo<|#|>event location, international competition<|#|>The World Athletics Championship is being hosted in Tokyo.\nrelation<|#|>Noah Carter<|#|>100m Sprint Record<|#|>athlete achievement, record-breaking<|#|>Noah Carter set a new 100m sprint record at the championship.\nrelation<|#|>Noah Carter<|#|>Carbon-Fiber Spikes<|#|>athletic equipment, performance boost<|#|>Noah Carter used carbon-fiber spikes to enhance performance during the race.\nrelation<|#|>Noah Carter<|#|>World Athletics Championship<|#|>athlete participation, competition<|#|>Noah Carter is competing at the World Athletics Championship.\n<|COMPLETE|>\n\n\n\n---Real Data to be Processed---\n<Input>\nEntity_types: [Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject]\nText:\n```\nASurveyofReinforcementLearningforLargeReasoningModels\n1. Introduction\nReinforcement Learning (RL) [Sutton et al., 1998] has repeatedly demonstrated that narrow, well-\nspecified reward signals can drive artificial agents to superhuman competence on complex tasks.\nLandmark systems such as AlphaGo [Silver et al., 2016] and AlphaZero [Silver et al., 2017], which\nlearned exclusively through self-play and reward feedback, surpassed world champions in Go, chess,\nshogi and Stratego [Perolat et al., 2022, Schrittwieser et al., 2020, Silver et al., 2018], establishing\nRL as a practical and promising technology for high-level problem solving. In the era of Large\nLanguage Models (LLMs) [Zhao et al., 2023a], RL initially rose to prominence as a post-training\nstrategyforhumanalignment[Ouyangetal.,2022]. WidelyadoptedmethodssuchasReinforcement\nLearningfromHumanFeedback(RLHF)[Christianoetal.,2017]andDirectPreferenceOptimization\n(DPO) [Rafailov et al., 2023] finetune pre-trained models to follow instructions and reflect human\npreferences, markedly improving helpfulness, honesty, and harmlessness (3H) [Bai et al., 2022b].\nMorerecently,anewtrendhasemerged: RLforLargeReasoningModels(LRMs)[Xuetal.,2025a],\nwhich aims not merely to align behavior but to incentivize reasoning itself. Two recent milestones\n(i.e.,OpenAIo1[Jaechetal.,2024]andDeepSeek-R1[Guoetal.,2025a])demonstratethattraining\nLLMs using reinforcement learning with verifiable rewards (RLVR), such as answer correctness for\nmathematics or unit-test pass rates for code, can enable models to perform long-form reasoning,\nincluding planning, reflection, and self-correction. OpenAI reports [Jaech et al., 2024] that o1s\nperformance improves smoothly with both additional RL (increased train-time compute) and more\ntime spent thinking at inference (test-time compute) [Brown et al., 2024, Liu et al., 2025m, Snell\netal.,2024],revealinganewscalingaxisbeyondpre-trainingalone[Aghajanyanetal.,2023,Kaplan\net al., 2020]. DeepSeek-R1 [Guo et al., 2025a] employs explicit, rule-based accuracy rewards for\nmathematics,aswellascompiler-ortest-basedrewardsforcodingtasks. Thisapproachdemonstrates\nthat large-scale reinforcement learning, specifically, Group Relative Policy Optimization (GRPO), can\ninduce sophisticated reasoning behaviors even in base models prior to subsequent alignment stages.\nThis shift reframes reasoning as a capability that can be explicitly trained and scaled [OpenAI,\n2025a,b]: LRMs allocate significant test-time compute to generate, evaluate, and revise intermediate\nchain-of-thought [Wei et al., 2022], and their performance rises as this compute budget increases.\nThisdynamicintroducesacomplementarypathtocapabilitygains,orthogonaltodataandparameter\nscaling during pre-training [Aghajanyan et al., 2023, Kaplan et al., 2020], while leveraging a reward\nmaximization objective [Silver et al., 2021], automatically checkable rewards wherever reliable\nverifiers exist (e.g., competition mathematics [Guo et al., 2025a, Jaech et al., 2024], competitive\nprogramming[El-Kishkyetal.,2025],andselectedscientificdomains[Baietal.,2025]). Furthermore,\nRL can overcome data limitations [Shumailov et al., 2024, Villalobos et al., 2022] by enabling self-\ngenerated training data [Silver et al., 2018, Zhao et al., 2025a]. As a result, RL is increasingly\nregardedasapromisingtechnologyforachievingArtificialSuperIntelligence(ASI)onabroaderrange\nof tasks through continual scaling.\nAt the same time, further scaling of RL for LRMs introduces new constraints, not only in compu-\ntational resources, but also in algorithm design, training data, and infrastructure. How and where\nRL for LRMs can be scaled to achieve high-level intelligence and generate real-world value remain\nunresolved issues. Therefore, we argue that it is timely to revisit the development of this domain and\nexplore strategies to enhance the scalability of RL toward artificial superintelligence. In summary,\nthis survey reviews recent work on RL for LRMs as follows:\n We introduce the preliminary definitions of RL modeling in the context of LRMs ( 2.1) and\noutline the development of frontier reasoning models since the release of OpenAI o1 ( 2.2).\n We review recent literature on the foundational components of RL for LRMs, including reward\n4\n```",
    "queryparam": null,
    "create_time": 1765218737,
    "update_time": 1765218737,
    "_id": "default:extract:42621f955ed86936d97f161c3ad73f7e"
  },
  "default:extract:b73238f0356d8918bc6e29dc93370baa": {
    "return": "entity<|#|>Reward Signals<|#|>concept<|#|>The well-specified signals that drive artificial agents to superhuman competence in Reinforcement Learning.\nentity<|#|>Reward Feedback<|#|>concept<|#|>The feedback mechanism through which landmark systems like AlphaGo and AlphaZero learned.\nentity<|#|>Verifiable Rewards<|#|>concept<|#|>A type of reward used in Reinforcement Learning with Verifiable Rewards (RLVR), such as answer correctness for mathematics or unit-test pass rates for code.\nentity<|#|>Mathematics<|#|>concept<|#|>A domain where models can perform long-form reasoning and for which explicit, rule-based accuracy rewards are used.\nentity<|#|>Coding Tasks<|#|>concept<|#|>Tasks for which compiler- or test-based rewards are employed in models like DeepSeek-R1.\nentity<|#|>Reasoning<|#|>concept<|#|>A capability that RL for Large Reasoning Models aims to incentivize and that can be explicitly trained and scaled.\nentity<|#|>Compute Budget<|#|>concept<|#|>The allocated computational resources for generating, evaluating, and revising intermediate chain-of-thought in Large Reasoning Models.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Reward Signals<|#|>utilization, mechanism<|#|>Reinforcement Learning uses well-specified reward signals to drive artificial agents.\nrelation<|#|>AlphaGo<|#|>Reward Feedback<|#|>learning mechanism, methodology<|#|>AlphaGo learned exclusively through reward feedback.\nrelation<|#|>AlphaZero<|#|>Reward Feedback<|#|>learning mechanism, methodology<|#|>AlphaZero learned exclusively through reward feedback.\nrelation<|#|>Reinforcement Learning with Verifiable Rewards (RLVR)<|#|>Verifiable Rewards<|#|>utilization, mechanism<|#|>Reinforcement Learning with Verifiable Rewards (RLVR) uses verifiable rewards.\nrelation<|#|>DeepSeek-R1<|#|>Mathematics<|#|>application, reward source<|#|>DeepSeek-R1 employs explicit, rule-based accuracy rewards for mathematics.\nrelation<|#|>DeepSeek-R1<|#|>Coding Tasks<|#|>application, reward source<|#|>DeepSeek-R1 employs compiler- or test-based rewards for coding tasks.\nrelation<|#|>RL for Large Reasoning Models (LRMs)<|#|>Reasoning<|#|>objective, incentivization<|#|>RL for Large Reasoning Models (LRMs) aims to incentivize reasoning itself.\nrelation<|#|>Large Reasoning Models (LRMs)<|#|>Compute Budget<|#|>allocation, resource<|#|>Large Reasoning Models (LRMs) allocate significant test-time compute, referred to as compute budget.\nrelation<|#|>Large Reasoning Models (LRMs)<|#|>Compute Budget<|#|>performance, scaling<|#|>The performance of Large Reasoning Models (LRMs) rises as their compute budget increases.\n<|COMPLETE|>",
    "cache_type": "extract",
    "chunk_id": "chunk-8b7f62b7202fa8bc4469bb44567df186",
    "original_prompt": "---Task---\nBased on the last extraction task, identify and extract any **missed or incorrectly formatted** entities and relationships from the input text.\n\n---Instructions---\n1.  **Strict Adherence to System Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system instructions.\n2.  **Focus on Corrections/Additions:**\n    *   **Do NOT** re-output entities and relationships that were **correctly and fully** extracted in the last task.\n    *   If an entity or relationship was **missed** in the last task, extract and output it now according to the system format.\n    *   If an entity or relationship was **truncated, had missing fields, or was otherwise incorrectly formatted** in the last task, re-output the *corrected and complete* version in the specified format.\n3.  **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n4.  **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n5.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\n6.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant missing or corrected entities and relationships have been extracted and presented.\n7.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\n\n<Output>\n---Role---\nYou are a Knowledge Graph Specialist responsible for extracting entities and relationships from the input text.\n\n---Instructions---\n1.  **Entity Extraction & Output:**\n    *   **Identification:** Identify clearly defined and meaningful entities in the input text.\n    *   **Entity Details:** For each identified entity, extract the following information:\n        *   `entity_name`: The name of the entity. If the entity name is case-insensitive, capitalize the first letter of each significant word (title case). Ensure **consistent naming** across the entire extraction process.\n        *   `entity_type`: Categorize the entity using one of the following types: `Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject`. If none of the provided entity types apply, do not add new entity type and classify it as `Other`.\n        *   `entity_description`: Provide a concise yet comprehensive description of the entity's attributes and activities, based *solely* on the information present in the input text.\n    *   **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n        *   Format: `entity<|#|>entity_name<|#|>entity_type<|#|>entity_description`\n\n2.  **Relationship Extraction & Output:**\n    *   **Identification:** Identify direct, clearly stated, and meaningful relationships between previously extracted entities.\n    *   **N-ary Relationship Decomposition:** If a single statement describes a relationship involving more than two entities (an N-ary relationship), decompose it into multiple binary (two-entity) relationship pairs for separate description.\n        *   **Example:** For \"Alice, Bob, and Carol collaborated on Project X,\" extract binary relationships such as \"Alice collaborated with Project X,\" \"Bob collaborated with Project X,\" and \"Carol collaborated with Project X,\" or \"Alice collaborated with Bob,\" based on the most reasonable binary interpretations.\n    *   **Relationship Details:** For each binary relationship, extract the following fields:\n        *   `source_entity`: The name of the source entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `target_entity`: The name of the target entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `relationship_keywords`: One or more high-level keywords summarizing the overarching nature, concepts, or themes of the relationship. Multiple keywords within this field must be separated by a comma `,`. **DO NOT use `<|#|>` for separating multiple keywords within this field.**\n        *   `relationship_description`: A concise explanation of the nature of the relationship between the source and target entities, providing a clear rationale for their connection.\n    *   **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n        *   Format: `relation<|#|>source_entity<|#|>target_entity<|#|>relationship_keywords<|#|>relationship_description`\n\n3.  **Delimiter Usage Protocol:**\n    *   The `<|#|>` is a complete, atomic marker and **must not be filled with content**. It serves strictly as a field separator.\n    *   **Incorrect Example:** `entity<|#|>Tokyo<|location|>Tokyo is the capital of Japan.`\n    *   **Correct Example:** `entity<|#|>Tokyo<|#|>location<|#|>Tokyo is the capital of Japan.`\n\n4.  **Relationship Direction & Duplication:**\n    *   Treat all relationships as **undirected** unless explicitly stated otherwise. Swapping the source and target entities for an undirected relationship does not constitute a new relationship.\n    *   Avoid outputting duplicate relationships.\n\n5.  **Output Order & Prioritization:**\n    *   Output all extracted entities first, followed by all extracted relationships.\n    *   Within the list of relationships, prioritize and output those relationships that are **most significant** to the core meaning of the input text first.\n\n6.  **Context & Objectivity:**\n    *   Ensure all entity names and descriptions are written in the **third person**.\n    *   Explicitly name the subject or object; **avoid using pronouns** such as `this article`, `this paper`, `our company`, `I`, `you`, and `he/she`.\n\n7.  **Language & Proper Nouns:**\n    *   The entire output (entity names, keywords, and descriptions) must be written in `English`.\n    *   Proper nouns (e.g., personal names, place names, organization names) should be retained in their original language if a proper, widely accepted translation is not available or would cause ambiguity.\n\n8.  **Completion Signal:** Output the literal string `<|COMPLETE|>` only after all entities and relationships, following all criteria, have been completely extracted and outputted.\n\n---Examples---\n<Input Text>\n```\nwhile Alex clenched his jaw, the buzz of frustration dull against the backdrop of Taylor's authoritarian certainty. It was this competitive undercurrent that kept him alert, the sense that his and Jordan's shared commitment to discovery was an unspoken rebellion against Cruz's narrowing vision of control and order.\n\nThen Taylor did something unexpected. They paused beside Jordan and, for a moment, observed the device with something akin to reverence. \"If this tech can be understood...\" Taylor said, their voice quieter, \"It could change the game for us. For all of us.\"\n\nThe underlying dismissal earlier seemed to falter, replaced by a glimpse of reluctant respect for the gravity of what lay in their hands. Jordan looked up, and for a fleeting heartbeat, their eyes locked with Taylor's, a wordless clash of wills softening into an uneasy truce.\n\nIt was a small transformation, barely perceptible, but one that Alex noted with an inward nod. They had all been brought here by different paths\n```\n\n<Output>\nentity<|#|>Alex<|#|>person<|#|>Alex is a character who experiences frustration and is observant of the dynamics among other characters.\nentity<|#|>Taylor<|#|>person<|#|>Taylor is portrayed with authoritarian certainty and shows a moment of reverence towards a device, indicating a change in perspective.\nentity<|#|>Jordan<|#|>person<|#|>Jordan shares a commitment to discovery and has a significant interaction with Taylor regarding a device.\nentity<|#|>Cruz<|#|>person<|#|>Cruz is associated with a vision of control and order, influencing the dynamics among other characters.\nentity<|#|>The Device<|#|>equipment<|#|>The Device is central to the story, with potential game-changing implications, and is revered by Taylor.\nrelation<|#|>Alex<|#|>Taylor<|#|>power dynamics, observation<|#|>Alex observes Taylor's authoritarian behavior and notes changes in Taylor's attitude toward the device.\nrelation<|#|>Alex<|#|>Jordan<|#|>shared goals, rebellion<|#|>Alex and Jordan share a commitment to discovery, which contrasts with Cruz's vision.)\nrelation<|#|>Taylor<|#|>Jordan<|#|>conflict resolution, mutual respect<|#|>Taylor and Jordan interact directly regarding the device, leading to a moment of mutual respect and an uneasy truce.\nrelation<|#|>Jordan<|#|>Cruz<|#|>ideological conflict, rebellion<|#|>Jordan's commitment to discovery is in rebellion against Cruz's vision of control and order.\nrelation<|#|>Taylor<|#|>The Device<|#|>reverence, technological significance<|#|>Taylor shows reverence towards the device, indicating its importance and potential impact.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nStock markets faced a sharp downturn today as tech giants saw significant declines, with the global tech index dropping by 3.4% in midday trading. Analysts attribute the selloff to investor concerns over rising interest rates and regulatory uncertainty.\n\nAmong the hardest hit, nexon technologies saw its stock plummet by 7.8% after reporting lower-than-expected quarterly earnings. In contrast, Omega Energy posted a modest 2.1% gain, driven by rising oil prices.\n\nMeanwhile, commodity markets reflected a mixed sentiment. Gold futures rose by 1.5%, reaching $2,080 per ounce, as investors sought safe-haven assets. Crude oil prices continued their rally, climbing to $87.60 per barrel, supported by supply constraints and strong demand.\n\nFinancial experts are closely watching the Federal Reserve's next move, as speculation grows over potential rate hikes. The upcoming policy announcement is expected to influence investor confidence and overall market stability.\n```\n\n<Output>\nentity<|#|>Global Tech Index<|#|>category<|#|>The Global Tech Index tracks the performance of major technology stocks and experienced a 3.4% decline today.\nentity<|#|>Nexon Technologies<|#|>organization<|#|>Nexon Technologies is a tech company that saw its stock decline by 7.8% after disappointing earnings.\nentity<|#|>Omega Energy<|#|>organization<|#|>Omega Energy is an energy company that gained 2.1% in stock value due to rising oil prices.\nentity<|#|>Gold Futures<|#|>product<|#|>Gold futures rose by 1.5%, indicating increased investor interest in safe-haven assets.\nentity<|#|>Crude Oil<|#|>product<|#|>Crude oil prices rose to $87.60 per barrel due to supply constraints and strong demand.\nentity<|#|>Market Selloff<|#|>category<|#|>Market selloff refers to the significant decline in stock values due to investor concerns over interest rates and regulations.\nentity<|#|>Federal Reserve Policy Announcement<|#|>category<|#|>The Federal Reserve's upcoming policy announcement is expected to impact investor confidence and market stability.\nentity<|#|>3.4% Decline<|#|>category<|#|>The Global Tech Index experienced a 3.4% decline in midday trading.\nrelation<|#|>Global Tech Index<|#|>Market Selloff<|#|>market performance, investor sentiment<|#|>The decline in the Global Tech Index is part of the broader market selloff driven by investor concerns.\nrelation<|#|>Nexon Technologies<|#|>Global Tech Index<|#|>company impact, index movement<|#|>Nexon Technologies' stock decline contributed to the overall drop in the Global Tech Index.\nrelation<|#|>Gold Futures<|#|>Market Selloff<|#|>market reaction, safe-haven investment<|#|>Gold prices rose as investors sought safe-haven assets during the market selloff.\nrelation<|#|>Federal Reserve Policy Announcement<|#|>Market Selloff<|#|>interest rate impact, financial regulation<|#|>Speculation over Federal Reserve policy changes contributed to market volatility and investor selloff.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nAt the World Athletics Championship in Tokyo, Noah Carter broke the 100m sprint record using cutting-edge carbon-fiber spikes.\n```\n\n<Output>\nentity<|#|>World Athletics Championship<|#|>event<|#|>The World Athletics Championship is a global sports competition featuring top athletes in track and field.\nentity<|#|>Tokyo<|#|>location<|#|>Tokyo is the host city of the World Athletics Championship.\nentity<|#|>Noah Carter<|#|>person<|#|>Noah Carter is a sprinter who set a new record in the 100m sprint at the World Athletics Championship.\nentity<|#|>100m Sprint Record<|#|>category<|#|>The 100m sprint record is a benchmark in athletics, recently broken by Noah Carter.\nentity<|#|>Carbon-Fiber Spikes<|#|>equipment<|#|>Carbon-fiber spikes are advanced sprinting shoes that provide enhanced speed and traction.\nentity<|#|>World Athletics Federation<|#|>organization<|#|>The World Athletics Federation is the governing body overseeing the World Athletics Championship and record validations.\nrelation<|#|>World Athletics Championship<|#|>Tokyo<|#|>event location, international competition<|#|>The World Athletics Championship is being hosted in Tokyo.\nrelation<|#|>Noah Carter<|#|>100m Sprint Record<|#|>athlete achievement, record-breaking<|#|>Noah Carter set a new 100m sprint record at the championship.\nrelation<|#|>Noah Carter<|#|>Carbon-Fiber Spikes<|#|>athletic equipment, performance boost<|#|>Noah Carter used carbon-fiber spikes to enhance performance during the race.\nrelation<|#|>Noah Carter<|#|>World Athletics Championship<|#|>athlete participation, competition<|#|>Noah Carter is competing at the World Athletics Championship.\n<|COMPLETE|>\n\n\n\n---Real Data to be Processed---\n<Input>\nEntity_types: [Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject]\nText:\n```\nASurveyofReinforcementLearningforLargeReasoningModels\n1. Introduction\nReinforcement Learning (RL) [Sutton et al., 1998] has repeatedly demonstrated that narrow, well-\nspecified reward signals can drive artificial agents to superhuman competence on complex tasks.\nLandmark systems such as AlphaGo [Silver et al., 2016] and AlphaZero [Silver et al., 2017], which\nlearned exclusively through self-play and reward feedback, surpassed world champions in Go, chess,\nshogi and Stratego [Perolat et al., 2022, Schrittwieser et al., 2020, Silver et al., 2018], establishing\nRL as a practical and promising technology for high-level problem solving. In the era of Large\nLanguage Models (LLMs) [Zhao et al., 2023a], RL initially rose to prominence as a post-training\nstrategyforhumanalignment[Ouyangetal.,2022]. WidelyadoptedmethodssuchasReinforcement\nLearningfromHumanFeedback(RLHF)[Christianoetal.,2017]andDirectPreferenceOptimization\n(DPO) [Rafailov et al., 2023] finetune pre-trained models to follow instructions and reflect human\npreferences, markedly improving helpfulness, honesty, and harmlessness (3H) [Bai et al., 2022b].\nMorerecently,anewtrendhasemerged: RLforLargeReasoningModels(LRMs)[Xuetal.,2025a],\nwhich aims not merely to align behavior but to incentivize reasoning itself. Two recent milestones\n(i.e.,OpenAIo1[Jaechetal.,2024]andDeepSeek-R1[Guoetal.,2025a])demonstratethattraining\nLLMs using reinforcement learning with verifiable rewards (RLVR), such as answer correctness for\nmathematics or unit-test pass rates for code, can enable models to perform long-form reasoning,\nincluding planning, reflection, and self-correction. OpenAI reports [Jaech et al., 2024] that o1s\nperformance improves smoothly with both additional RL (increased train-time compute) and more\ntime spent thinking at inference (test-time compute) [Brown et al., 2024, Liu et al., 2025m, Snell\netal.,2024],revealinganewscalingaxisbeyondpre-trainingalone[Aghajanyanetal.,2023,Kaplan\net al., 2020]. DeepSeek-R1 [Guo et al., 2025a] employs explicit, rule-based accuracy rewards for\nmathematics,aswellascompiler-ortest-basedrewardsforcodingtasks. Thisapproachdemonstrates\nthat large-scale reinforcement learning, specifically, Group Relative Policy Optimization (GRPO), can\ninduce sophisticated reasoning behaviors even in base models prior to subsequent alignment stages.\nThis shift reframes reasoning as a capability that can be explicitly trained and scaled [OpenAI,\n2025a,b]: LRMs allocate significant test-time compute to generate, evaluate, and revise intermediate\nchain-of-thought [Wei et al., 2022], and their performance rises as this compute budget increases.\nThisdynamicintroducesacomplementarypathtocapabilitygains,orthogonaltodataandparameter\nscaling during pre-training [Aghajanyan et al., 2023, Kaplan et al., 2020], while leveraging a reward\nmaximization objective [Silver et al., 2021], automatically checkable rewards wherever reliable\nverifiers exist (e.g., competition mathematics [Guo et al., 2025a, Jaech et al., 2024], competitive\nprogramming[El-Kishkyetal.,2025],andselectedscientificdomains[Baietal.,2025]). Furthermore,\nRL can overcome data limitations [Shumailov et al., 2024, Villalobos et al., 2022] by enabling self-\ngenerated training data [Silver et al., 2018, Zhao et al., 2025a]. As a result, RL is increasingly\nregardedasapromisingtechnologyforachievingArtificialSuperIntelligence(ASI)onabroaderrange\nof tasks through continual scaling.\nAt the same time, further scaling of RL for LRMs introduces new constraints, not only in compu-\ntational resources, but also in algorithm design, training data, and infrastructure. How and where\nRL for LRMs can be scaled to achieve high-level intelligence and generate real-world value remain\nunresolved issues. Therefore, we argue that it is timely to revisit the development of this domain and\nexplore strategies to enhance the scalability of RL toward artificial superintelligence. In summary,\nthis survey reviews recent work on RL for LRMs as follows:\n We introduce the preliminary definitions of RL modeling in the context of LRMs ( 2.1) and\noutline the development of frontier reasoning models since the release of OpenAI o1 ( 2.2).\n We review recent literature on the foundational components of RL for LRMs, including reward\n4\n```\n[{\"role\": \"user\", \"content\": \"---Task---\\nExtract entities and relationships from the input text to be processed.\\n\\n---Instructions---\\n1.  **Strict Adherence to Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system prompt.\\n2.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\\n3.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant entities and relationships have been extracted and presented.\\n4.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\\n\\n<Output>\"}, {\"role\": \"assistant\", \"content\": \"entity<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>content<|#|>The title of a survey document that reviews recent work on Reinforcement Learning for Large Reasoning Models.\\nentity<|#|>Reinforcement Learning (RL)<|#|>method<|#|>A learning paradigm that uses well-specified reward signals to drive artificial agents to superhuman competence on complex tasks.\\nentity<|#|>Sutton et al., 1998<|#|>person<|#|>Authors credited with foundational work on Reinforcement Learning.\\nentity<|#|>Artificial Agents<|#|>concept<|#|>Entities that can be trained by Reinforcement Learning to achieve superhuman competence on complex tasks.\\nentity<|#|>AlphaGo<|#|>organization<|#|>A landmark system that learned exclusively through self-play and reward feedback, surpassing world champions in Go.\\nentity<|#|>Silver et al., 2016<|#|>person<|#|>Authors credited with the development of AlphaGo.\\nentity<|#|>AlphaZero<|#|>organization<|#|>A landmark system that learned exclusively through self-play and reward feedback, surpassing world champions in Go, chess, shogi, and Stratego.\\nentity<|#|>Silver et al., 2017<|#|>person<|#|>Authors credited with the development of AlphaZero.\\nentity<|#|>Go<|#|>concept<|#|>A complex board game where AlphaGo and AlphaZero achieved superhuman competence.\\nentity<|#|>Chess<|#|>concept<|#|>A strategic board game where AlphaZero achieved superhuman competence.\\nentity<|#|>Shogi<|#|>concept<|#|>A Japanese board game where AlphaZero achieved superhuman competence.\\nentity<|#|>Stratego<|#|>concept<|#|>A strategy board game where AlphaZero achieved superhuman competence.\\nentity<|#|>Perolat et al., 2022<|#|>person<|#|>Authors credited with work related to AlphaZero's performance in Stratego.\\nentity<|#|>Schrittwieser et al., 2020<|#|>person<|#|>Authors credited with work related to AlphaZero's performance.\\nentity<|#|>Silver et al., 2018<|#|>person<|#|>Authors credited with work related to AlphaZero and self-generated training data.\\nentity<|#|>Large Language Models (LLMs)<|#|>concept<|#|>Models that gained prominence, with Reinforcement Learning initially used as a post-training strategy for human alignment.\\nentity<|#|>Zhao et al., 2023a<|#|>person<|#|>Authors credited with work related to Large Language Models.\\nentity<|#|>Human Alignment<|#|>concept<|#|>A post-training strategy for Large Language Models to make them follow instructions and reflect human preferences.\\nentity<|#|>Ouyang et al., 2022<|#|>person<|#|>Authors credited with work on Reinforcement Learning as a post-training strategy for human alignment.\\nentity<|#|>Reinforcement Learning from Human Feedback (RLHF)<|#|>method<|#|>A widely adopted method to finetune pre-trained models for human alignment.\\nentity<|#|>Christiano et al., 2017<|#|>person<|#|>Authors credited with the development of Reinforcement Learning from Human Feedback (RLHF).\\nentity<|#|>Direct Preference Optimization (DPO)<|#|>method<|#|>A method used to finetune pre-trained models to follow instructions and reflect human preferences.\\nentity<|#|>Rafailov et al., 2023<|#|>person<|#|>Authors credited with the development of Direct Preference Optimization (DPO).\\nentity<|#|>Helpfulness, Honesty, and Harmlessness (3H)<|#|>concept<|#|>Key qualities that Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) aim to improve in Large Language Models.\\nentity<|#|>Bai et al., 2022b<|#|>person<|#|>Authors credited with work related to Helpfulness, Honesty, and Harmlessness (3H).\\nentity<|#|>RL for Large Reasoning Models (LRMs)<|#|>concept<|#|>A new trend in Reinforcement Learning that aims to incentivize reasoning itself, not merely to align behavior.\\nentity<|#|>Xu et al., 2025a<|#|>person<|#|>Authors credited with work related to RL for Large Reasoning Models.\\nentity<|#|>OpenAI o1<|#|>organization<|#|>A recent milestone demonstrating that training Large Language Models using Reinforcement Learning with Verifiable Rewards (RLVR) can enable long-form reasoning.\\nentity<|#|>Jaech et al., 2024<|#|>person<|#|>Authors credited with work related to OpenAI o1 and verifiable rewards.\\nentity<|#|>DeepSeek-R1<|#|>organization<|#|>A recent milestone that employs explicit, rule-based accuracy rewards for mathematics and compiler- or test-based rewards for coding tasks.\\nentity<|#|>Guo et al., 2025a<|#|>person<|#|>Authors credited with work related to DeepSeek-R1 and verifiable rewards.\\nentity<|#|>Reinforcement Learning with Verifiable Rewards (RLVR)<|#|>method<|#|>A method used to train Large Language Models, using verifiable rewards such as answer correctness or unit-test pass rates, to enable long-form reasoning.\\nentity<|#|>Answer Correctness for Mathematics<|#|>data<|#|>A type of verifiable reward used in Reinforcement Learning with Verifiable Rewards (RLVR) for mathematics tasks.\\nentity<|#|>Unit-Test Pass Rates for Code<|#|>data<|#|>A type of verifiable reward used in Reinforcement Learning with Verifiable Rewards (RLVR) for coding tasks.\\nentity<|#|>Long-Form Reasoning<|#|>concept<|#|>A capability enabled by Reinforcement Learning with Verifiable Rewards (RLVR), including planning, reflection, and self-correction.\\nentity<|#|>Planning<|#|>concept<|#|>A component of long-form reasoning.\\nentity<|#|>Reflection<|#|>concept<|#|>A component of long-form reasoning.\\nentity<|#|>Self-Correction<|#|>concept<|#|>A component of long-form reasoning.\\nentity<|#|>Train-Time Compute<|#|>concept<|#|>Computational resources used during the training phase of models, which can improve performance.\\nentity<|#|>Test-Time Compute<|#|>concept<|#|>Computational resources used during the inference phase of models, also referred to as \\\"thinking\\\" time, which can improve performance.\\nentity<|#|>Brown et al., 2024<|#|>person<|#|>Authors credited with work related to test-time compute.\\nentity<|#|>Liu et al., 2025m<|#|>person<|#|>Authors credited with work related to test-time compute.\\nentity<|#|>Snell et al., 2024<|#|>person<|#|>Authors credited with work related to test-time compute.\\nentity<|#|>Scaling Axis<|#|>concept<|#|>A new dimension for performance improvement beyond pre-training alone.\\nentity<|#|>Pre-Training<|#|>method<|#|>An initial phase of model training, which is complemented by new scaling axes.\\nentity<|#|>Aghajanyan et al., 2023<|#|>person<|#|>Authors credited with work related to scaling axes beyond pre-training.\\nentity<|#|>Kaplan et al., 2020<|#|>person<|#|>Authors credited with work related to scaling axes beyond pre-training.\\nentity<|#|>Group Relative Policy Optimization (GRPO)<|#|>method<|#|>A large-scale reinforcement learning approach employed by DeepSeek-R1 to induce sophisticated reasoning behaviors.\\nentity<|#|>Base Models<|#|>concept<|#|>Models that can exhibit sophisticated reasoning behaviors even prior to subsequent alignment stages when trained with methods like Group Relative Policy Optimization (GRPO).\\nentity<|#|>Chain-of-Thought<|#|>method<|#|>Intermediate steps generated, evaluated, and revised by Large Reasoning Models (LRMs) to improve performance.\\nentity<|#|>Wei et al., 2022<|#|>person<|#|>Authors credited with work related to Chain-of-Thought.\\nentity<|#|>OpenAI, 2025a,b<|#|>organization<|#|>Credited with reframing reasoning as a capability that can be explicitly trained and scaled.\\nentity<|#|>Reward Maximization Objective<|#|>concept<|#|>A goal leveraged by Large Reasoning Models (LRMs) to achieve capability gains.\\nentity<|#|>Silver et al., 2021<|#|>person<|#|>Authors credited with work related to reward maximization objective.\\nentity<|#|>Automatically Checkable Rewards<|#|>concept<|#|>Rewards that can be reliably verified, used in domains like competition mathematics and competitive programming.\\nentity<|#|>Competition Mathematics<|#|>data<|#|>A domain where automatically checkable rewards exist.\\nentity<|#|>Competitive Programming<|#|>data<|#|>A domain where automatically checkable rewards exist.\\nentity<|#|>El-Kishky et al., 2025<|#|>person<|#|>Authors credited with work related to competitive programming.\\nentity<|#|>Selected Scientific Domains<|#|>data<|#|>Domains where automatically checkable rewards exist.\\nentity<|#|>Bai et al., 2025<|#|>person<|#|>Authors credited with work related to selected scientific domains.\\nentity<|#|>Data Limitations<|#|>concept<|#|>A challenge that Reinforcement Learning can overcome by enabling self-generated training data.\\nentity<|#|>Shumailov et al., 2024<|#|>person<|#|>Authors credited with work related to overcoming data limitations.\\nentity<|#|>Villalobos et al., 2022<|#|>person<|#|>Authors credited with work related to overcoming data limitations.\\nentity<|#|>Self-Generated Training Data<|#|>data<|#|>Data created by Reinforcement Learning systems to overcome data limitations.\\nentity<|#|>Artificial Superintelligence (ASI)<|#|>concept<|#|>A high-level intelligence that Reinforcement Learning is regarded as a promising technology for achieving.\\nentity<|#|>Computational Resources<|#|>concept<|#|>A constraint introduced by further scaling of Reinforcement Learning for Large Reasoning Models.\\nentity<|#|>Algorithm Design<|#|>concept<|#|>A constraint introduced by further scaling of Reinforcement Learning for Large Reasoning Models.\\nentity<|#|>Training Data<|#|>data<|#|>A constraint introduced by further scaling of Reinforcement Learning for Large Reasoning Models.\\nentity<|#|>Infrastructure<|#|>concept<|#|>A constraint introduced by further scaling of Reinforcement Learning for Large Reasoning Models.\\nentity<|#|>Preliminary Definitions of RL Modeling<|#|>concept<|#|>A topic introduced in the survey in the context of Large Reasoning Models.\\nentity<|#|>Frontier Reasoning Models<|#|>concept<|#|>Models whose development is outlined in the survey since the release of OpenAI o1.\\nentity<|#|>Foundational Components of RL for LRMs<|#|>concept<|#|>A topic reviewed in the survey, covering recent literature.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Sutton et al., 1998<|#|>foundational work, development<|#|>Sutton et al., 1998 are credited with foundational work on Reinforcement Learning.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Artificial Agents<|#|>capability, training<|#|>Reinforcement Learning drives artificial agents to superhuman competence.\\nrelation<|#|>AlphaGo<|#|>Reinforcement Learning (RL)<|#|>application, methodology<|#|>AlphaGo is a landmark system that used Reinforcement Learning.\\nrelation<|#|>AlphaGo<|#|>Silver et al., 2016<|#|>development, authorship<|#|>Silver et al., 2016 are credited with the development of AlphaGo.\\nrelation<|#|>AlphaGo<|#|>Go<|#|>achievement, game mastery<|#|>AlphaGo surpassed world champions in Go.\\nrelation<|#|>AlphaZero<|#|>Reinforcement Learning (RL)<|#|>application, methodology<|#|>AlphaZero is a landmark system that used Reinforcement Learning.\\nrelation<|#|>AlphaZero<|#|>Silver et al., 2017<|#|>development, authorship<|#|>Silver et al., 2017 are credited with the development of AlphaZero.\\nrelation<|#|>AlphaZero<|#|>Go<|#|>achievement, game mastery<|#|>AlphaZero surpassed world champions in Go.\\nrelation<|#|>AlphaZero<|#|>Chess<|#|>achievement, game mastery<|#|>AlphaZero surpassed world champions in Chess.\\nrelation<|#|>AlphaZero<|#|>Shogi<|#|>achievement, game mastery<|#|>AlphaZero surpassed world champions in Shogi.\\nrelation<|#|>AlphaZero<|#|>Stratego<|#|>achievement, game mastery<|#|>AlphaZero surpassed world champions in Stratego.\\nrelation<|#|>AlphaZero<|#|>Perolat et al., 2022<|#|>related work, citation<|#|>Perolat et al., 2022 are cited in relation to AlphaZero's performance in Stratego.\\nrelation<|#|>AlphaZero<|#|>Schrittwieser et al., 2020<|#|>related work, citation<|#|>Schrittwieser et al., 2020 are cited in relation to AlphaZero's performance.\\nrelation<|#|>AlphaZero<|#|>Silver et al., 2018<|#|>related work, citation<|#|>Silver et al., 2018 are cited in relation to AlphaZero.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Large Language Models (LLMs)<|#|>application, prominence<|#|>Reinforcement Learning rose to prominence as a post-training strategy for Large Language Models.\\nrelation<|#|>Large Language Models (LLMs)<|#|>Zhao et al., 2023a<|#|>related work, citation<|#|>Zhao et al., 2023a are cited in relation to Large Language Models.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Human Alignment<|#|>strategy, objective<|#|>Reinforcement Learning is a post-training strategy for human alignment.\\nrelation<|#|>Human Alignment<|#|>Ouyang et al., 2022<|#|>related work, citation<|#|>Ouyang et al., 2022 are cited in relation to human alignment.\\nrelation<|#|>Reinforcement Learning from Human Feedback (RLHF)<|#|>Human Alignment<|#|>method, objective<|#|>Reinforcement Learning from Human Feedback (RLHF) is a method for human alignment.\\nrelation<|#|>Reinforcement Learning from Human Feedback (RLHF)<|#|>Christiano et al., 2017<|#|>development, authorship<|#|>Christiano et al., 2017 are credited with the development of Reinforcement Learning from Human Feedback (RLHF).\\nrelation<|#|>Direct Preference Optimization (DPO)<|#|>Human Alignment<|#|>method, objective<|#|>Direct Preference Optimization (DPO) is a method for human alignment.\\nrelation<|#|>Direct Preference Optimization (DPO)<|#|>Rafailov et al., 2023<|#|>development, authorship<|#|>Rafailov et al., 2023 are credited with the development of Direct Preference Optimization (DPO).\\nrelation<|#|>Reinforcement Learning from Human Feedback (RLHF)<|#|>Helpfulness, Honesty, and Harmlessness (3H)<|#|>improvement, objective<|#|>Reinforcement Learning from Human Feedback (RLHF) improves Helpfulness, Honesty, and Harmlessness (3H).\\nrelation<|#|>Direct Preference Optimization (DPO)<|#|>Helpfulness, Honesty, and Harmlessness (3H)<|#|>improvement, objective<|#|>Direct Preference Optimization (DPO) improves Helpfulness, Honesty, and Harmlessness (3H).\\nrelation<|#|>Helpfulness, Honesty, and Harmlessness (3H)<|#|>Bai et al., 2022b<|#|>related work, citation<|#|>Bai et al., 2022b are cited in relation to Helpfulness, Honesty, and Harmlessness (3H).\\nrelation<|#|>RL for Large Reasoning Models (LRMs)<|#|>Reinforcement Learning (RL)<|#|>trend, application<|#|>RL for Large Reasoning Models (LRMs) is a new trend in Reinforcement Learning.\\nrelation<|#|>RL for Large Reasoning Models (LRMs)<|#|>Xu et al., 2025a<|#|>related work, citation<|#|>Xu et al., 2025a are cited in relation to RL for Large Reasoning Models.\\nrelation<|#|>RL for Large Reasoning Models (LRMs)<|#|>Long-Form Reasoning<|#|>objective, incentivization<|#|>RL for Large Reasoning Models (LRMs) aims to incentivize reasoning itself.\\nrelation<|#|>OpenAI o1<|#|>RL for Large Reasoning Models (LRMs)<|#|>milestone, demonstration<|#|>OpenAI o1 is a recent milestone demonstrating the effectiveness of RL for Large Reasoning Models.\\nrelation<|#|>OpenAI o1<|#|>Jaech et al., 2024<|#|>related work, citation<|#|>Jaech et al., 2024 are cited in relation to OpenAI o1.\\nrelation<|#|>DeepSeek-R1<|#|>RL for Large Reasoning Models (LRMs)<|#|>milestone, demonstration<|#|>DeepSeek-R1 is a recent milestone demonstrating the effectiveness of RL for Large Reasoning Models.\\nrelation<|#|>DeepSeek-R1<|#|>Guo et al., 2025a<|#|>related work, citation<|#|>Guo et al., 2025a are cited in relation to DeepSeek-R1.\\nrelation<|#|>Reinforcement Learning with Verifiable Rewards (RLVR)<|#|>OpenAI o1<|#|>methodology, application<|#|>OpenAI o1 uses Reinforcement Learning with Verifiable Rewards (RLVR).\\nrelation<|#|>Reinforcement Learning with Verifiable Rewards (RLVR)<|#|>DeepSeek-R1<|#|>methodology, application<|#|>DeepSeek-R1 uses Reinforcement Learning with Verifiable Rewards (RLVR).\\nrelation<|#|>Reinforcement Learning with Verifiable Rewards (RLVR)<|#|>Long-Form Reasoning<|#|>enabling, capability<|#|>Reinforcement Learning with Verifiable Rewards (RLVR) enables models to perform long-form reasoning.\\nrelation<|#|>Reinforcement Learning with Verifiable Rewards (RLVR)<|#|>Answer Correctness for Mathematics<|#|>reward signal, application<|#|>Reinforcement Learning with Verifiable Rewards (RLVR) uses answer correctness for mathematics as a reward.\\nrelation<|#|>Reinforcement Learning with Verifiable Rewards (RLVR)<|#|>Unit-Test Pass Rates for Code<|#|>reward signal, application<|#|>Reinforcement Learning with Verifiable Rewards (RLVR) uses unit-test pass rates for code as a reward.\\nrelation<|#|>Long-Form Reasoning<|#|>Planning<|#|>component, aspect<|#|>Planning is a component of long-form reasoning.\\nrelation<|#|>Long-Form Reasoning<|#|>Reflection<|#|>component, aspect<|#|>Reflection is a component of long-form reasoning.\\nrelation<|#|>Long-Form Reasoning<|#|>Self-Correction<|#|>component, aspect<|#|>Self-correction is a component of long-form reasoning.\\nrelation<|#|>OpenAI o1<|#|>Train-Time Compute<|#|>performance, scaling<|#|>OpenAI o1's performance improves with additional train-time compute.\\nrelation<|#|>OpenAI o1<|#|>Test-Time Compute<|#|>performance, scaling<|#|>OpenAI o1's performance improves with more time spent \\\"thinking\\\" at inference (test-time compute).\\nrelation<#|>Test-Time Compute<|#|>Brown et al., 2024<|#|>related work, citation<|#|>Brown et al., 2024 are cited in relation to test-time compute.\\nrelation<|#|>Test-Time Compute<|#|>Liu et al., 2025m<|#|>related work, citation<|#|>Liu et al., 2025m are cited in relation to test-time compute.\\nrelation<|#|>Test-Time Compute<|#|>Snell et al., 2024<|#|>related work, citation<|#|>Snell et al., 2024 are cited in relation to test-time compute.\\nrelation<|#|>OpenAI o1<|#|>Scaling Axis<|#|>discovery, implication<|#|>OpenAI o1 reveals a new scaling axis.\\nrelation<|#|>Scaling Axis<|#|>Pre-Training<|#|>beyond, complement<|#|>The new scaling axis is beyond pre-training alone.\\nrelation<|#|>Scaling Axis<|#|>Aghajanyan et al., 2023<|#|>related work, citation<|#|>Aghajanyan et al., 2023 are cited in relation to scaling axes.\\nrelation<|#|>Scaling Axis<|#|>Kaplan et al., 2020<|#|>related work, citation<|#|>Kaplan et al., 2020 are cited in relation to scaling axes.\\nrelation<|#|>DeepSeek-R1<|#|>Group Relative Policy Optimization (GRPO)<|#|>methodology, application<|#|>DeepSeek-R1 employs Group Relative Policy Optimization (GRPO).\\nrelation<|#|>Group Relative Policy Optimization (GRPO)<|#|>Reasoning<|#|>inducement, capability<|#|>Group Relative Policy Optimization (GRPO) can induce sophisticated reasoning behaviors.\\nrelation<|#|>Group Relative Policy Optimization (GRPO)<|#|>Base Models<|#|>application, effect<|#|>Group Relative Policy Optimization (GRPO) induces reasoning behaviors in base models.\\nrelation<|#|>Reasoning<|#|>OpenAI, 2025a,b<|#|>reframing, perspective<|#|>OpenAI, 2025a,b reframes reasoning as a capability that can be explicitly trained and scaled.\\nrelation<|#|>Large Reasoning Models (LRMs)<|#|>Test-Time Compute<|#|>allocation, utilization<|#|>Large Reasoning Models (LRMs) allocate significant test-time compute.\\nrelation<|#|>Large Reasoning Models (LRMs)<|#|>Chain-of-Thought<|#|>process, generation<|#|>Large Reasoning Models (LRMs) generate, evaluate, and revise intermediate chain-of-thought.\\nrelation<|#|>Chain-of-Thought<|#|>Wei et al., 2022<|#|>related work, citation<|#|>Wei et al., 2022 are cited in relation to Chain-of-Thought.\\nrelation<|#|>Large Reasoning Models (LRMs)<|#|>Reward Maximization Objective<|#|>leverage, goal<|#|>Large Reasoning Models (LRMs) leverage a reward maximization objective.\\nrelation<|#|>Reward Maximization Objective<|#|>Silver et al., 2021<|#|>related work, citation<|#|>Silver et al., 2021 are cited in relation to reward maximization objective.\\nrelation<|#|>Automatically Checkable Rewards<|#|>Competition Mathematics<|#|>domain, existence<|#|>Automatically checkable rewards exist in competition mathematics.\\nrelation<|#|>Automatically Checkable Rewards<|#|>Competitive Programming<|#|>domain, existence<|#|>Automatically checkable rewards exist in competitive programming.\\nrelation<|#|>Competitive Programming<|#|>El-Kishky et al., 2025<|#|>related work, citation<|#|>El-Kishky et al., 2025 are cited in relation to competitive programming.\\nrelation<|#|>Automatically Checkable Rewards<|#|>Selected Scientific Domains<|#|>domain, existence<|#|>Automatically checkable rewards exist in selected scientific domains.\\nrelation<|#|>Selected Scientific Domains<|#|>Bai et al., 2025<|#|>related work, citation<|#|>Bai et al., 2025 are cited in relation to selected scientific domains.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Data Limitations<|#|>overcoming, solution<|#|>Reinforcement Learning can overcome data limitations.\\nrelation<|#|>Data Limitations<|#|>Shumailov et al., 2024<|#|>related work, citation<|#|>Shumailov et al., 2024 are cited in relation to data limitations.\\nrelation<|#|>Data Limitations<|#|>Villalobos et al., 2022<|#|>related work, citation<|#|>Villalobos et al., 2022 are cited in relation to data limitations.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Self-Generated Training Data<|#|>enabling, generation<|#|>Reinforcement Learning enables self-generated training data.\\nrelation<|#|>Self-Generated Training Data<|#|>Silver et al., 2018<|#|>related work, citation<|#|>Silver et al., 2018 are cited in relation to self-generated training data.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Artificial Superintelligence (ASI)<|#|>technology, achievement<|#|>Reinforcement Learning is a promising technology for achieving Artificial Superintelligence (ASI).\\nrelation<|#|>RL for Large Reasoning Models (LRMs)<|#|>Computational Resources<|#|>constraint, scaling<|#|>Scaling RL for Large Reasoning Models introduces constraints in computational resources.\\nrelation<|#|>RL for Large Reasoning Models (LRMs)<|#|>Algorithm Design<|#|>constraint, scaling<|#|>Scaling RL for Large Reasoning Models introduces constraints in algorithm design.\\nrelation<|#|>RL for Large Reasoning Models (LRMs)<|#|>Training Data<|#|>constraint, scaling<|#|>Scaling RL for Large Reasoning Models introduces constraints in training data.\\nrelation<|#|>RL for Large Reasoning Models (LRMs)<|#|>Infrastructure<|#|>constraint, scaling<|#|>Scaling RL for Large Reasoning Models introduces constraints in infrastructure.\\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Preliminary Definitions of RL Modeling<|#|>review, introduction<|#|>The survey introduces preliminary definitions of RL modeling.\\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Frontier Reasoning Models<|#|>review, outline<|#|>The survey outlines the development of frontier reasoning models.\\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Foundational Components of RL for LRMs<|#|>review, literature<|#|>The survey reviews recent literature on the foundational components of RL for LRMs.\\nrelation<|#|>Frontier Reasoning Models<|#|>OpenAI o1<|#|>development, starting point<|#|>The survey outlines the development of frontier reasoning models since the release of OpenAI o1.\\n<|COMPLETE|>\"}]",
    "queryparam": null,
    "create_time": 1765218770,
    "update_time": 1765218770,
    "_id": "default:extract:b73238f0356d8918bc6e29dc93370baa"
  },
  "default:extract:3a41e850b36499ebe0d78f02945c7151": {
    "return": "entity<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Content<|#|>A survey document that focuses on reinforcement learning for large reasoning models.\nentity<|#|>Reinforcement Learning (RL)<|#|>Concept<|#|>A general framework for sequential decision making where an agent interacts with an environment by taking actions to maximize cumulative reward.\nentity<|#|>Large Reasoning Models (LRMs)<|#|>Concept<|#|>A category of models that are the subject of a survey on reinforcement learning, whose capacity for complex task solving can be enhanced by RLVR.\nentity<|#|>RL From Human Direct Preference<|#|>Method<|#|>A reinforcement learning methodology for human alignment.\nentity<|#|>RL With Verifiable Feedback Optimization Reward<|#|>Method<|#|>A reinforcement learning methodology for human alignment.\nentity<|#|>GPT-3.5<|#|>Artifact<|#|>A specific large language model mentioned in the context of reinforcement learning.\nentity<|#|>GPT-4<|#|>Artifact<|#|>A specific large language model mentioned in the context of reinforcement learning.\nentity<|#|>Llama 3<|#|>Artifact<|#|>A specific large language model mentioned in the context of reinforcement learning.\nentity<|#|>Qwen 2.5<|#|>Artifact<|#|>A specific large language model mentioned in the context of reinforcement learning.\nentity<|#|>O1<|#|>Artifact<|#|>A specific model mentioned in the context of reinforcement learning.\nentity<|#|>DeepSeek-R1<|#|>Artifact<|#|>A specific large language model mentioned in the context of reinforcement learning.\nentity<|#|>Reward-Based RL<|#|>Method<|#|>A category of reinforcement learning approaches.\nentity<|#|>Reward-Free RL<|#|>Method<|#|>A category of reinforcement learning approaches.\nentity<|#|>Rule-Based RL<|#|>Method<|#|>A category of reinforcement learning approaches.\nentity<|#|>RLHF<|#|>Method<|#|>Reinforcement Learning from Human Feedback, a predominant RL methodology for human alignment in recent years.\nentity<|#|>DPO<|#|>Method<|#|>Direct Preference Optimization, a predominant RL methodology for human alignment in recent years.\nentity<|#|>RLVR<|#|>Method<|#|>Reinforcement Learning with Verifiable Feedback, an emerging trend in RL for LRMs that significantly enhances their capacity for complex task solving.\nentity<|#|>Open-Ended RL<|#|>Method<|#|>A challenging and promising direction for scaling reinforcement learning for large language models.\nentity<|#|>Human Alignment<|#|>Concept<|#|>A goal for reinforcement learning methodologies like RLHF and DPO.\nentity<|#|>Complex Task Solving<|#|>Concept<|#|>A capacity of large reasoning models that RLVR aims to enhance.\nentity<|#|>Large Language Models (LLMs)<|#|>Concept<|#|>A category of language models for which the next stage of scaling RL remains an open question.\nentity<|#|>Policy Optimization<|#|>Method<|#|>A component of reinforcement learning design.\nentity<|#|>Sampling Strategies<|#|>Method<|#|>A component of reinforcement learning design.\nentity<|#|>Supervised Fine-Tuning (SFT)<|#|>Method<|#|>A method compared with reinforcement learning in foundational problems for LRMs.\nentity<|#|>Model Priors<|#|>Concept<|#|>A foundational and controversial problem in reinforcement learning for LRMs.\nentity<|#|>Training Recipes<|#|>Concept<|#|>A foundational and controversial problem in reinforcement learning for LRMs.\nentity<|#|>Reward Definitions<|#|>Concept<|#|>A foundational and controversial problem in reinforcement learning for LRMs.\nentity<|#|>Training Resources<|#|>Concept<|#|>Resources for reinforcement learning, including static corpora, dynamic environments, and training infrastructure.\nentity<|#|>Static Corpora<|#|>Data<|#|>A type of training resource for reinforcement learning.\nentity<|#|>Dynamic Environments<|#|>Concept<|#|>A type of training resource for reinforcement learning.\nentity<|#|>Training Infrastructure<|#|>Artifact<|#|>A type of training resource for reinforcement learning.\nentity<|#|>Coding Tasks<|#|>Concept<|#|>An application area of reinforcement learning.\nentity<|#|>Agentic Tasks<|#|>Concept<|#|>An application area of reinforcement learning.\nentity<|#|>Multimodal Tasks<|#|>Concept<|#|>An application area of reinforcement learning.\nentity<|#|>Multi-Agent Systems<|#|>Concept<|#|>An application area of reinforcement learning.\nentity<|#|>Robotics Tasks<|#|>Concept<|#|>An application area of reinforcement learning.\nentity<|#|>Medical Applications<|#|>Concept<|#|>An application area of reinforcement learning.\nentity<|#|>Agent<|#|>Concept<|#|>A component within RL frameworks that interacts with an environment by taking actions to maximize cumulative reward.\nentity<|#|>Environment<|#|>Concept<|#|>A component within RL frameworks with which an agent interacts.\nentity<|#|>Actions<|#|>Concept<|#|>Taken by an agent within an RL framework to interact with an environment.\nentity<|#|>Cumulative Reward<|#|>Concept<|#|>The objective that an agent aims to maximize in reinforcement learning.\nentity<|#|>Markov Decision Process<|#|>Concept<|#|>A typical formulation for problems in classical reinforcement learning.\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Reinforcement Learning (RL)<|#|>scope<|#|>The survey document focuses on the field of Reinforcement Learning.\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Large Reasoning Models (LRMs)<|#|>scope<|#|>The survey document focuses on Large Reasoning Models.\nrelation<|#|>Reinforcement Learning (RL)<|#|>RLHF<|#|>category<|#|>RLHF is a predominant methodology within Reinforcement Learning.\nrelation<|#|>Reinforcement Learning (RL)<|#|>DPO<|#|>category<|#|>DPO is a predominant methodology within Reinforcement Learning.\nrelation<#|>Reinforcement Learning (RL)<|#|>RLVR<|#|>category<|#|>RLVR is an emerging trend within Reinforcement Learning.\nrelation<|#|>RLHF<|#|>Human Alignment<|#|>objective<|#|>RLHF is a predominant methodology for human alignment.\nrelation<|#|>DPO<|#|>Human Alignment<|#|>objective<|#|>DPO is a predominant methodology for human alignment.\nrelation<|#|>RLVR<|#|>Large Reasoning Models (LRMs)<|#|>application<|#|>RLVR represents an emerging trend in RL specifically for Large Reasoning Models.\nrelation<|#|>RLVR<|#|>Complex Task Solving<|#|>enhancement<|#|>RLVR significantly enhances the capacity of LRMs for complex task solving.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Open-Ended RL<|#|>category<|#|>Open-Ended RL is a challenging and promising direction within Reinforcement Learning.\nrelation<|#|>Open-Ended RL<|#|>Large Language Models (LLMs)<|#|>research direction<|#|>Open-Ended RL presents a promising direction for scaling RL for Large Language Models.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Policy Optimization<|#|>component<|#|>Policy optimization is a component of reinforcement learning design.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Sampling Strategies<|#|>component<|#|>Sampling strategies are a component of reinforcement learning design.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Supervised Fine-Tuning (SFT)<|#|>comparison<|#|>Reinforcement Learning is compared with Supervised Fine-Tuning as a foundational problem.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Model Priors<|#|>issue<|#|>Model priors are discussed as foundational and controversial problems in RL.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Training Recipes<|#|>issue<|#|>Training recipes are discussed as foundational and controversial problems in RL.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Reward Definitions<|#|>issue<|#|>Reward definitions are discussed as foundational and controversial problems in RL.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Training Resources<|#|>resource usage<|#|>Training resources are examined for their role in Reinforcement Learning.\nrelation<|#|>Training Resources<|#|>Static Corpora<|#|>resource type<|#|>Static corpora are a type of training resource.\nrelation<|#|>Training Resources<|#|>Dynamic Environments<|#|>resource type<|#|>Dynamic environments are a type of training resource.\nrelation<|#|>Training Resources<|#|>Training Infrastructure<|#|>resource type<|#|>Training infrastructure is a type of training resource.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Coding Tasks<|#|>application<|#|>Reinforcement Learning is applied to coding tasks.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Agentic Tasks<|#|>application<|#|>Reinforcement Learning is applied to agentic tasks.\nrelation<#|>Reinforcement Learning (RL)<|#|>Multimodal Tasks<|#|>application<|#|>Reinforcement Learning is applied to multimodal tasks.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Multi-Agent Systems<|#|>application<|#|>Reinforcement Learning is applied to multi-agent systems.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Robotics Tasks<|#|>application<|#|>Reinforcement Learning is applied to robotics tasks.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Medical Applications<|#|>application<|#|>Reinforcement Learning is applied to medical applications.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Agent<|#|>framework component<|#|>Reinforcement Learning frameworks configure language models as agents.\nrelation<|#|>Agent<|#|>Environment<|#|>interaction<|#|>An agent interacts with an environment within RL frameworks.\nrelation<|#|>Agent<|#|>Actions<|#|>action<|#|>An agent takes actions within RL frameworks.\nrelation<|#|>Agent<|#|>Cumulative Reward<|#|>objective<|#|>An agent aims to maximize cumulative reward within RL frameworks.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Markov Decision Process<|#|>formalization<|#|>Reinforcement Learning problems are typically formulated as a Markov Decision Process.\nrelation<|#|>GPT-3.5<|#|>Large Reasoning Models (LRMs)<|#|>example<|#|>GPT-3.5 is listed as an example of a model relevant to the survey.\nrelation<|#|>GPT-4<|#|>Large Reasoning Models (LRMs)<|#|>example<|#|>GPT-4 is listed as an example of a model relevant to the survey.\nrelation<|#|>Llama 3<|#|>Large Reasoning Models (LRMs)<|#|>example<|#|>Llama 3 is listed as an example of a model relevant to the survey.\nrelation<|#|>Qwen 2.5<|#|>Large Reasoning Models (LRMs)<|#|>example<|#|>Qwen 2.5 is listed as an example of a model relevant to the survey.\nrelation<|#|>O1<|#|>Large Reasoning Models (LRMs)<|#|>example<|#|>O1 is listed as an example of a model relevant to the survey.\nrelation<|#|>DeepSeek-R1<|#|>Large Reasoning Models (LRMs)<|#|>example<|#|>DeepSeek-R1 is listed as an example of a model relevant to the survey.\nrelation<|#|>Reinforcement Learning (RL)<|#|>RL From Human Direct Preference<|#|>category<|#|>RL From Human Direct Preference is a type of Reinforcement Learning.\nrelation<|#|>Reinforcement Learning (RL)<|#|>RL With Verifiable Feedback Optimization Reward<|#|>category<|#|>RL With Verifiable Feedback Optimization Reward is a type of Reinforcement Learning.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Reward-Based RL<|#|>category<|#|>Reward-Based RL is a category of reinforcement learning approaches.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Reward-Free RL<|#|>category<|#|>Reward-Free RL is a category of reinforcement learning approaches.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Rule-Based RL<|#|>category<|#|>Rule-Based RL is a category of reinforcement learning approaches.\n<|COMPLETE|>",
    "cache_type": "extract",
    "chunk_id": "chunk-8355cb8033a4afe3e01992fcee557002",
    "original_prompt": "---Task---\nExtract entities and relationships from the input text to be processed.\n\n---Instructions---\n1.  **Strict Adherence to Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system prompt.\n2.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\n3.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant entities and relationships have been extracted and presented.\n4.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\n\n<Output>\n---Role---\nYou are a Knowledge Graph Specialist responsible for extracting entities and relationships from the input text.\n\n---Instructions---\n1.  **Entity Extraction & Output:**\n    *   **Identification:** Identify clearly defined and meaningful entities in the input text.\n    *   **Entity Details:** For each identified entity, extract the following information:\n        *   `entity_name`: The name of the entity. If the entity name is case-insensitive, capitalize the first letter of each significant word (title case). Ensure **consistent naming** across the entire extraction process.\n        *   `entity_type`: Categorize the entity using one of the following types: `Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject`. If none of the provided entity types apply, do not add new entity type and classify it as `Other`.\n        *   `entity_description`: Provide a concise yet comprehensive description of the entity's attributes and activities, based *solely* on the information present in the input text.\n    *   **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n        *   Format: `entity<|#|>entity_name<|#|>entity_type<|#|>entity_description`\n\n2.  **Relationship Extraction & Output:**\n    *   **Identification:** Identify direct, clearly stated, and meaningful relationships between previously extracted entities.\n    *   **N-ary Relationship Decomposition:** If a single statement describes a relationship involving more than two entities (an N-ary relationship), decompose it into multiple binary (two-entity) relationship pairs for separate description.\n        *   **Example:** For \"Alice, Bob, and Carol collaborated on Project X,\" extract binary relationships such as \"Alice collaborated with Project X,\" \"Bob collaborated with Project X,\" and \"Carol collaborated with Project X,\" or \"Alice collaborated with Bob,\" based on the most reasonable binary interpretations.\n    *   **Relationship Details:** For each binary relationship, extract the following fields:\n        *   `source_entity`: The name of the source entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `target_entity`: The name of the target entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `relationship_keywords`: One or more high-level keywords summarizing the overarching nature, concepts, or themes of the relationship. Multiple keywords within this field must be separated by a comma `,`. **DO NOT use `<|#|>` for separating multiple keywords within this field.**\n        *   `relationship_description`: A concise explanation of the nature of the relationship between the source and target entities, providing a clear rationale for their connection.\n    *   **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n        *   Format: `relation<|#|>source_entity<|#|>target_entity<|#|>relationship_keywords<|#|>relationship_description`\n\n3.  **Delimiter Usage Protocol:**\n    *   The `<|#|>` is a complete, atomic marker and **must not be filled with content**. It serves strictly as a field separator.\n    *   **Incorrect Example:** `entity<|#|>Tokyo<|location|>Tokyo is the capital of Japan.`\n    *   **Correct Example:** `entity<|#|>Tokyo<|#|>location<|#|>Tokyo is the capital of Japan.`\n\n4.  **Relationship Direction & Duplication:**\n    *   Treat all relationships as **undirected** unless explicitly stated otherwise. Swapping the source and target entities for an undirected relationship does not constitute a new relationship.\n    *   Avoid outputting duplicate relationships.\n\n5.  **Output Order & Prioritization:**\n    *   Output all extracted entities first, followed by all extracted relationships.\n    *   Within the list of relationships, prioritize and output those relationships that are **most significant** to the core meaning of the input text first.\n\n6.  **Context & Objectivity:**\n    *   Ensure all entity names and descriptions are written in the **third person**.\n    *   Explicitly name the subject or object; **avoid using pronouns** such as `this article`, `this paper`, `our company`, `I`, `you`, and `he/she`.\n\n7.  **Language & Proper Nouns:**\n    *   The entire output (entity names, keywords, and descriptions) must be written in `English`.\n    *   Proper nouns (e.g., personal names, place names, organization names) should be retained in their original language if a proper, widely accepted translation is not available or would cause ambiguity.\n\n8.  **Completion Signal:** Output the literal string `<|COMPLETE|>` only after all entities and relationships, following all criteria, have been completely extracted and outputted.\n\n---Examples---\n<Input Text>\n```\nwhile Alex clenched his jaw, the buzz of frustration dull against the backdrop of Taylor's authoritarian certainty. It was this competitive undercurrent that kept him alert, the sense that his and Jordan's shared commitment to discovery was an unspoken rebellion against Cruz's narrowing vision of control and order.\n\nThen Taylor did something unexpected. They paused beside Jordan and, for a moment, observed the device with something akin to reverence. \"If this tech can be understood...\" Taylor said, their voice quieter, \"It could change the game for us. For all of us.\"\n\nThe underlying dismissal earlier seemed to falter, replaced by a glimpse of reluctant respect for the gravity of what lay in their hands. Jordan looked up, and for a fleeting heartbeat, their eyes locked with Taylor's, a wordless clash of wills softening into an uneasy truce.\n\nIt was a small transformation, barely perceptible, but one that Alex noted with an inward nod. They had all been brought here by different paths\n```\n\n<Output>\nentity<|#|>Alex<|#|>person<|#|>Alex is a character who experiences frustration and is observant of the dynamics among other characters.\nentity<|#|>Taylor<|#|>person<|#|>Taylor is portrayed with authoritarian certainty and shows a moment of reverence towards a device, indicating a change in perspective.\nentity<|#|>Jordan<|#|>person<|#|>Jordan shares a commitment to discovery and has a significant interaction with Taylor regarding a device.\nentity<|#|>Cruz<|#|>person<|#|>Cruz is associated with a vision of control and order, influencing the dynamics among other characters.\nentity<|#|>The Device<|#|>equipment<|#|>The Device is central to the story, with potential game-changing implications, and is revered by Taylor.\nrelation<|#|>Alex<|#|>Taylor<|#|>power dynamics, observation<|#|>Alex observes Taylor's authoritarian behavior and notes changes in Taylor's attitude toward the device.\nrelation<|#|>Alex<|#|>Jordan<|#|>shared goals, rebellion<|#|>Alex and Jordan share a commitment to discovery, which contrasts with Cruz's vision.)\nrelation<|#|>Taylor<|#|>Jordan<|#|>conflict resolution, mutual respect<|#|>Taylor and Jordan interact directly regarding the device, leading to a moment of mutual respect and an uneasy truce.\nrelation<|#|>Jordan<|#|>Cruz<|#|>ideological conflict, rebellion<|#|>Jordan's commitment to discovery is in rebellion against Cruz's vision of control and order.\nrelation<|#|>Taylor<|#|>The Device<|#|>reverence, technological significance<|#|>Taylor shows reverence towards the device, indicating its importance and potential impact.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nStock markets faced a sharp downturn today as tech giants saw significant declines, with the global tech index dropping by 3.4% in midday trading. Analysts attribute the selloff to investor concerns over rising interest rates and regulatory uncertainty.\n\nAmong the hardest hit, nexon technologies saw its stock plummet by 7.8% after reporting lower-than-expected quarterly earnings. In contrast, Omega Energy posted a modest 2.1% gain, driven by rising oil prices.\n\nMeanwhile, commodity markets reflected a mixed sentiment. Gold futures rose by 1.5%, reaching $2,080 per ounce, as investors sought safe-haven assets. Crude oil prices continued their rally, climbing to $87.60 per barrel, supported by supply constraints and strong demand.\n\nFinancial experts are closely watching the Federal Reserve's next move, as speculation grows over potential rate hikes. The upcoming policy announcement is expected to influence investor confidence and overall market stability.\n```\n\n<Output>\nentity<|#|>Global Tech Index<|#|>category<|#|>The Global Tech Index tracks the performance of major technology stocks and experienced a 3.4% decline today.\nentity<|#|>Nexon Technologies<|#|>organization<|#|>Nexon Technologies is a tech company that saw its stock decline by 7.8% after disappointing earnings.\nentity<|#|>Omega Energy<|#|>organization<|#|>Omega Energy is an energy company that gained 2.1% in stock value due to rising oil prices.\nentity<|#|>Gold Futures<|#|>product<|#|>Gold futures rose by 1.5%, indicating increased investor interest in safe-haven assets.\nentity<|#|>Crude Oil<|#|>product<|#|>Crude oil prices rose to $87.60 per barrel due to supply constraints and strong demand.\nentity<|#|>Market Selloff<|#|>category<|#|>Market selloff refers to the significant decline in stock values due to investor concerns over interest rates and regulations.\nentity<|#|>Federal Reserve Policy Announcement<|#|>category<|#|>The Federal Reserve's upcoming policy announcement is expected to impact investor confidence and market stability.\nentity<|#|>3.4% Decline<|#|>category<|#|>The Global Tech Index experienced a 3.4% decline in midday trading.\nrelation<|#|>Global Tech Index<|#|>Market Selloff<|#|>market performance, investor sentiment<|#|>The decline in the Global Tech Index is part of the broader market selloff driven by investor concerns.\nrelation<|#|>Nexon Technologies<|#|>Global Tech Index<|#|>company impact, index movement<|#|>Nexon Technologies' stock decline contributed to the overall drop in the Global Tech Index.\nrelation<|#|>Gold Futures<|#|>Market Selloff<|#|>market reaction, safe-haven investment<|#|>Gold prices rose as investors sought safe-haven assets during the market selloff.\nrelation<|#|>Federal Reserve Policy Announcement<|#|>Market Selloff<|#|>interest rate impact, financial regulation<|#|>Speculation over Federal Reserve policy changes contributed to market volatility and investor selloff.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nAt the World Athletics Championship in Tokyo, Noah Carter broke the 100m sprint record using cutting-edge carbon-fiber spikes.\n```\n\n<Output>\nentity<|#|>World Athletics Championship<|#|>event<|#|>The World Athletics Championship is a global sports competition featuring top athletes in track and field.\nentity<|#|>Tokyo<|#|>location<|#|>Tokyo is the host city of the World Athletics Championship.\nentity<|#|>Noah Carter<|#|>person<|#|>Noah Carter is a sprinter who set a new record in the 100m sprint at the World Athletics Championship.\nentity<|#|>100m Sprint Record<|#|>category<|#|>The 100m sprint record is a benchmark in athletics, recently broken by Noah Carter.\nentity<|#|>Carbon-Fiber Spikes<|#|>equipment<|#|>Carbon-fiber spikes are advanced sprinting shoes that provide enhanced speed and traction.\nentity<|#|>World Athletics Federation<|#|>organization<|#|>The World Athletics Federation is the governing body overseeing the World Athletics Championship and record validations.\nrelation<|#|>World Athletics Championship<|#|>Tokyo<|#|>event location, international competition<|#|>The World Athletics Championship is being hosted in Tokyo.\nrelation<|#|>Noah Carter<|#|>100m Sprint Record<|#|>athlete achievement, record-breaking<|#|>Noah Carter set a new 100m sprint record at the championship.\nrelation<|#|>Noah Carter<|#|>Carbon-Fiber Spikes<|#|>athletic equipment, performance boost<|#|>Noah Carter used carbon-fiber spikes to enhance performance during the race.\nrelation<|#|>Noah Carter<|#|>World Athletics Championship<|#|>athlete participation, competition<|#|>Noah Carter is competing at the World Athletics Championship.\n<|COMPLETE|>\n\n\n\n---Real Data to be Processed---\n<Input>\nEntity_types: [Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject]\nText:\n```\nASurveyofReinforcementLearningforLargeReasoningModels\nRL from Human Direct Preference RL with Verifiable\nFeedback Optimization Reward\nGPT-3.5, GPT-4 Llama 3, Qwen 2.5 o1, DeepSeek-R1\nReward-based Reward-free Rule-based\n2022 2023 2025\nyticapaC\ngnivloS\nksaT\nRLHF DPO RLVR Open-ended RL\nFigure2 | RLHFandDPOhavebeenthetwopredominantRLmethodologiesforhumanalignmentin\nrecent years. In contrast, RLVR represents an emerging trend in RL for LRMs, significantly enhancing\ntheir capacity for complex task solving. The next stage of scaling RL for LLMs remains an open\nquestion, with open-ended RL presenting a particularly challenging and promising direction.\ndesign ( 3.1), policy optimization ( 3.2), and sampling strategies ( 3.3), comparing the\ndifferent research directions and technical approaches for each component.\n We discuss foundational and still controversial problems in RL for LRMs ( 4), such as the role of\nRL(4.1),RLversusSupervisedFine-Tuning(SFT)(4.2),modelpriors(4.3),trainingrecipes\n(4.4), andrewarddefinitions(4.5). Wearguethattheseissueswarrantfurtherexplorationto\nenable continued scaling of RL.\n WeexaminetrainingresourcesforRL(5),includingstaticcorpora(5.1),dynamicenvironments\n( 5.2), and training infrastructure ( 5.3). While these resources are reusable in both research\nand production, further standardization and development are needed.\n We review applications of RL to a wide range of tasks ( 6), such as coding tasks ( 6.1), agentic\ntasks ( 6.2), multimodal tasks ( 6.3), multi-agent systems ( 6.4), robotics tasks ( 6.5), and\nmedical applications ( 6.6).\n Finally, we discuss future directions in RL for language models ( 7), covering novel algorithms,\nmechanisms, features, and additional research avenues.\n2. Preliminaries\n2.1. Background\nInthissubsection,weintroducethebasiccomponentsofRLanddescribehowlanguagemodelscanbe\nconfigured as agents within RL frameworks. As shown in Figure 3, RL provides a general framework\nfor sequential decision making, in which an agent interacts with an environment by taking actions\nto maximize cumulative reward. In classical RL, the problem is typically formulated as a Markov\n5\n\nTables:\n[\n  [\n    [\n      \"RLHF\",\n      \"DPO\",\n      \"RLVR\",\n      \"Open-ended RL\"\n    ]\n  ],\n  [\n    [\n      \"\",\n      \"\"\n    ]\n  ]\n]\n```",
    "queryparam": null,
    "create_time": 1765218824,
    "update_time": 1765218824,
    "_id": "default:extract:3a41e850b36499ebe0d78f02945c7151"
  },
  "default:extract:802d0fbae2d79a2b6a33d6d06de6f6c7": {
    "return": "entity<|#|>Sequential Decision Making<|#|>Concept<|#|>A general framework provided by Reinforcement Learning, where an agent interacts with an environment to maximize cumulative reward.\nentity<|#|>Research Directions<|#|>Concept<|#|>Different areas of study and approaches within reinforcement learning, compared in the survey.\nentity<|#|>Technical Approaches<|#|>Concept<|#|>Specific methods and techniques for components like policy optimization and sampling strategies in reinforcement learning.\nentity<|#|>Foundational Problems<|#|>Concept<|#|>Core and unresolved issues in reinforcement learning for large reasoning models.\nentity<|#|>Controversial Problems<|#|>Concept<|#|>Debated and unresolved issues in reinforcement learning for large reasoning models.\nentity<|#|>Role of RL<|#|>Concept<|#|>A specific foundational and controversial problem concerning the function and importance of reinforcement learning within large reasoning models.\nentity<|#|>Scaling RL<|#|>Concept<|#|>The process of expanding reinforcement learning to larger and more complex models, identified as an open question and an area for further exploration.\nentity<|#|>Novel Algorithms<|#|>Concept<|#|>New computational procedures discussed as future directions in reinforcement learning for language models.\nentity<|#|>Mechanisms<|#|>Concept<|#|>Operational processes or systems discussed as future directions in reinforcement learning for language models.\nentity<|#|>Features<|#|>Concept<|#|>Distinctive attributes or aspects discussed as future directions in reinforcement learning for language models.\nentity<|#|>Research Avenues<|#|>Concept<|#|>Additional areas for investigation discussed as future directions in reinforcement learning for language models.\nrelation<|#|>Reinforcement Learning (RL)<|#|>RLVR<|#|>category<|#|>RLVR is an emerging trend within Reinforcement Learning.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Multimodal Tasks<|#|>application<|#|>Reinforcement Learning is applied to multimodal tasks.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Sequential Decision Making<|#|>framework, core concept<|#|>Reinforcement Learning provides a general framework for sequential decision making.\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Research Directions<|#|>discussion, comparison<|#|>The survey compares different research directions in RL.\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Technical Approaches<|#|>discussion, comparison<|#|>The survey compares different technical approaches in RL.\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Foundational Problems<|#|>discussion<|#|>The survey discusses foundational problems in RL for LRMs.\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Controversial Problems<|#|>discussion<|#|>The survey discusses controversial problems in RL for LRMs.\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Role of RL<|#|>discussion<|#|>The survey discusses the role of RL as a foundational problem.\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Scaling RL<|#|>discussion<|#|>The survey discusses issues warranting further exploration to enable continued scaling of RL.\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Novel Algorithms<|#|>future directions<|#|>The survey discusses novel algorithms as future directions in RL for language models.\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Mechanisms<|#|>future directions<|#|>The survey discusses mechanisms as future directions in RL for language models.\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Features<|#|>future directions<|#|>The survey discusses features as future directions in RL for language models.\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Research Avenues<|#|>future directions<|#|>The survey discusses additional research avenues as future directions in RL for language models.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Research Directions<|#|>aspect<|#|>Reinforcement Learning involves different research directions.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Technical Approaches<|#|>aspect<|#|>Reinforcement Learning involves different technical approaches.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Foundational Problems<|#|>challenge<|#|>Foundational problems exist within Reinforcement Learning for LRMs.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Controversial Problems<|#|>challenge<|#|>Controversial problems exist within Reinforcement Learning for LRMs.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Role of RL<|#|>challenge<|#|>The role of RL is a foundational and controversial problem within the field.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Scaling RL<|#|>challenge, development<|#|>Scaling RL is an open question and an area for continued exploration.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Novel Algorithms<|#|>future development<|#|>Novel algorithms are a future direction for Reinforcement Learning.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Mechanisms<|#|>future development<|#|>Mechanisms are a future direction for Reinforcement Learning.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Features<|#|>future development<|#|>Features are a future direction for Reinforcement Learning.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Research Avenues<|#|>future development<|#|>Research avenues are a future direction for Reinforcement Learning.\nrelation<|#|>Large Language Models (LLMs)<|#|>Scaling RL<|#|>development area<|#|>Scaling RL for LLMs remains an open question.\nrelation<|#|>Policy Optimization<|#|>Research Directions<|#|>category<|#|>Policy optimization is a research direction.\nrelation<|#|>Sampling Strategies<|#|>Research Directions<|#|>category<|#|>Sampling strategies are a research direction.\nrelation<|#|>Policy Optimization<|#|>Technical Approaches<|#|>category<|#|>Policy optimization is a technical approach.\nrelation<|#|>Sampling Strategies<|#|>Technical Approaches<|#|>category<|#|>Sampling strategies are a technical approach.\nrelation<|#|>Large Reasoning Models (LRMs)<|#|>Foundational Problems<|#|>context<|#|>Foundational problems are discussed in RL for LRMs.\nrelation<|#|>Large Reasoning Models (LRMs)<|#|>Controversial Problems<|#|>context<|#|>Controversial problems are discussed in RL for LRMs.\nrelation<|#|>Large Reasoning Models (LRMs)<|#|>Role of RL<|#|>context<|#|>The role of RL is a problem in RL for LRMs.\nrelation<|#|>Large Reasoning Models (LRMs)<|#|>Model Priors<|#|>context<|#|>Model priors are problems in RL for LRMs.\nrelation<|#|>Large Reasoning Models (LRMs)<|#|>Training Recipes<|#|>context<|#|>Training recipes are problems in RL for LRMs.\nrelation<|#|>Large Reasoning Models (LRMs)<|#|>Reward Definitions<|#|>context<|#|>Reward definitions are problems in RL for LRMs.\nrelation<|#|>Large Reasoning Models (LRMs)<|#|>Scaling RL<|#|>context<|#|>Scaling RL is a concern for LRMs.\n<|COMPLETE|>",
    "cache_type": "extract",
    "chunk_id": "chunk-8355cb8033a4afe3e01992fcee557002",
    "original_prompt": "---Task---\nBased on the last extraction task, identify and extract any **missed or incorrectly formatted** entities and relationships from the input text.\n\n---Instructions---\n1.  **Strict Adherence to System Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system instructions.\n2.  **Focus on Corrections/Additions:**\n    *   **Do NOT** re-output entities and relationships that were **correctly and fully** extracted in the last task.\n    *   If an entity or relationship was **missed** in the last task, extract and output it now according to the system format.\n    *   If an entity or relationship was **truncated, had missing fields, or was otherwise incorrectly formatted** in the last task, re-output the *corrected and complete* version in the specified format.\n3.  **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n4.  **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n5.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\n6.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant missing or corrected entities and relationships have been extracted and presented.\n7.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\n\n<Output>\n---Role---\nYou are a Knowledge Graph Specialist responsible for extracting entities and relationships from the input text.\n\n---Instructions---\n1.  **Entity Extraction & Output:**\n    *   **Identification:** Identify clearly defined and meaningful entities in the input text.\n    *   **Entity Details:** For each identified entity, extract the following information:\n        *   `entity_name`: The name of the entity. If the entity name is case-insensitive, capitalize the first letter of each significant word (title case). Ensure **consistent naming** across the entire extraction process.\n        *   `entity_type`: Categorize the entity using one of the following types: `Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject`. If none of the provided entity types apply, do not add new entity type and classify it as `Other`.\n        *   `entity_description`: Provide a concise yet comprehensive description of the entity's attributes and activities, based *solely* on the information present in the input text.\n    *   **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n        *   Format: `entity<|#|>entity_name<|#|>entity_type<|#|>entity_description`\n\n2.  **Relationship Extraction & Output:**\n    *   **Identification:** Identify direct, clearly stated, and meaningful relationships between previously extracted entities.\n    *   **N-ary Relationship Decomposition:** If a single statement describes a relationship involving more than two entities (an N-ary relationship), decompose it into multiple binary (two-entity) relationship pairs for separate description.\n        *   **Example:** For \"Alice, Bob, and Carol collaborated on Project X,\" extract binary relationships such as \"Alice collaborated with Project X,\" \"Bob collaborated with Project X,\" and \"Carol collaborated with Project X,\" or \"Alice collaborated with Bob,\" based on the most reasonable binary interpretations.\n    *   **Relationship Details:** For each binary relationship, extract the following fields:\n        *   `source_entity`: The name of the source entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `target_entity`: The name of the target entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `relationship_keywords`: One or more high-level keywords summarizing the overarching nature, concepts, or themes of the relationship. Multiple keywords within this field must be separated by a comma `,`. **DO NOT use `<|#|>` for separating multiple keywords within this field.**\n        *   `relationship_description`: A concise explanation of the nature of the relationship between the source and target entities, providing a clear rationale for their connection.\n    *   **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n        *   Format: `relation<|#|>source_entity<|#|>target_entity<|#|>relationship_keywords<|#|>relationship_description`\n\n3.  **Delimiter Usage Protocol:**\n    *   The `<|#|>` is a complete, atomic marker and **must not be filled with content**. It serves strictly as a field separator.\n    *   **Incorrect Example:** `entity<|#|>Tokyo<|location|>Tokyo is the capital of Japan.`\n    *   **Correct Example:** `entity<|#|>Tokyo<|#|>location<|#|>Tokyo is the capital of Japan.`\n\n4.  **Relationship Direction & Duplication:**\n    *   Treat all relationships as **undirected** unless explicitly stated otherwise. Swapping the source and target entities for an undirected relationship does not constitute a new relationship.\n    *   Avoid outputting duplicate relationships.\n\n5.  **Output Order & Prioritization:**\n    *   Output all extracted entities first, followed by all extracted relationships.\n    *   Within the list of relationships, prioritize and output those relationships that are **most significant** to the core meaning of the input text first.\n\n6.  **Context & Objectivity:**\n    *   Ensure all entity names and descriptions are written in the **third person**.\n    *   Explicitly name the subject or object; **avoid using pronouns** such as `this article`, `this paper`, `our company`, `I`, `you`, and `he/she`.\n\n7.  **Language & Proper Nouns:**\n    *   The entire output (entity names, keywords, and descriptions) must be written in `English`.\n    *   Proper nouns (e.g., personal names, place names, organization names) should be retained in their original language if a proper, widely accepted translation is not available or would cause ambiguity.\n\n8.  **Completion Signal:** Output the literal string `<|COMPLETE|>` only after all entities and relationships, following all criteria, have been completely extracted and outputted.\n\n---Examples---\n<Input Text>\n```\nwhile Alex clenched his jaw, the buzz of frustration dull against the backdrop of Taylor's authoritarian certainty. It was this competitive undercurrent that kept him alert, the sense that his and Jordan's shared commitment to discovery was an unspoken rebellion against Cruz's narrowing vision of control and order.\n\nThen Taylor did something unexpected. They paused beside Jordan and, for a moment, observed the device with something akin to reverence. \"If this tech can be understood...\" Taylor said, their voice quieter, \"It could change the game for us. For all of us.\"\n\nThe underlying dismissal earlier seemed to falter, replaced by a glimpse of reluctant respect for the gravity of what lay in their hands. Jordan looked up, and for a fleeting heartbeat, their eyes locked with Taylor's, a wordless clash of wills softening into an uneasy truce.\n\nIt was a small transformation, barely perceptible, but one that Alex noted with an inward nod. They had all been brought here by different paths\n```\n\n<Output>\nentity<|#|>Alex<|#|>person<|#|>Alex is a character who experiences frustration and is observant of the dynamics among other characters.\nentity<|#|>Taylor<|#|>person<|#|>Taylor is portrayed with authoritarian certainty and shows a moment of reverence towards a device, indicating a change in perspective.\nentity<|#|>Jordan<|#|>person<|#|>Jordan shares a commitment to discovery and has a significant interaction with Taylor regarding a device.\nentity<|#|>Cruz<|#|>person<|#|>Cruz is associated with a vision of control and order, influencing the dynamics among other characters.\nentity<|#|>The Device<|#|>equipment<|#|>The Device is central to the story, with potential game-changing implications, and is revered by Taylor.\nrelation<|#|>Alex<|#|>Taylor<|#|>power dynamics, observation<|#|>Alex observes Taylor's authoritarian behavior and notes changes in Taylor's attitude toward the device.\nrelation<|#|>Alex<|#|>Jordan<|#|>shared goals, rebellion<|#|>Alex and Jordan share a commitment to discovery, which contrasts with Cruz's vision.)\nrelation<|#|>Taylor<|#|>Jordan<|#|>conflict resolution, mutual respect<|#|>Taylor and Jordan interact directly regarding the device, leading to a moment of mutual respect and an uneasy truce.\nrelation<|#|>Jordan<|#|>Cruz<|#|>ideological conflict, rebellion<|#|>Jordan's commitment to discovery is in rebellion against Cruz's vision of control and order.\nrelation<|#|>Taylor<|#|>The Device<|#|>reverence, technological significance<|#|>Taylor shows reverence towards the device, indicating its importance and potential impact.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nStock markets faced a sharp downturn today as tech giants saw significant declines, with the global tech index dropping by 3.4% in midday trading. Analysts attribute the selloff to investor concerns over rising interest rates and regulatory uncertainty.\n\nAmong the hardest hit, nexon technologies saw its stock plummet by 7.8% after reporting lower-than-expected quarterly earnings. In contrast, Omega Energy posted a modest 2.1% gain, driven by rising oil prices.\n\nMeanwhile, commodity markets reflected a mixed sentiment. Gold futures rose by 1.5%, reaching $2,080 per ounce, as investors sought safe-haven assets. Crude oil prices continued their rally, climbing to $87.60 per barrel, supported by supply constraints and strong demand.\n\nFinancial experts are closely watching the Federal Reserve's next move, as speculation grows over potential rate hikes. The upcoming policy announcement is expected to influence investor confidence and overall market stability.\n```\n\n<Output>\nentity<|#|>Global Tech Index<|#|>category<|#|>The Global Tech Index tracks the performance of major technology stocks and experienced a 3.4% decline today.\nentity<|#|>Nexon Technologies<|#|>organization<|#|>Nexon Technologies is a tech company that saw its stock decline by 7.8% after disappointing earnings.\nentity<|#|>Omega Energy<|#|>organization<|#|>Omega Energy is an energy company that gained 2.1% in stock value due to rising oil prices.\nentity<|#|>Gold Futures<|#|>product<|#|>Gold futures rose by 1.5%, indicating increased investor interest in safe-haven assets.\nentity<|#|>Crude Oil<|#|>product<|#|>Crude oil prices rose to $87.60 per barrel due to supply constraints and strong demand.\nentity<|#|>Market Selloff<|#|>category<|#|>Market selloff refers to the significant decline in stock values due to investor concerns over interest rates and regulations.\nentity<|#|>Federal Reserve Policy Announcement<|#|>category<|#|>The Federal Reserve's upcoming policy announcement is expected to impact investor confidence and market stability.\nentity<|#|>3.4% Decline<|#|>category<|#|>The Global Tech Index experienced a 3.4% decline in midday trading.\nrelation<|#|>Global Tech Index<|#|>Market Selloff<|#|>market performance, investor sentiment<|#|>The decline in the Global Tech Index is part of the broader market selloff driven by investor concerns.\nrelation<|#|>Nexon Technologies<|#|>Global Tech Index<|#|>company impact, index movement<|#|>Nexon Technologies' stock decline contributed to the overall drop in the Global Tech Index.\nrelation<|#|>Gold Futures<|#|>Market Selloff<|#|>market reaction, safe-haven investment<|#|>Gold prices rose as investors sought safe-haven assets during the market selloff.\nrelation<|#|>Federal Reserve Policy Announcement<|#|>Market Selloff<|#|>interest rate impact, financial regulation<|#|>Speculation over Federal Reserve policy changes contributed to market volatility and investor selloff.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nAt the World Athletics Championship in Tokyo, Noah Carter broke the 100m sprint record using cutting-edge carbon-fiber spikes.\n```\n\n<Output>\nentity<|#|>World Athletics Championship<|#|>event<|#|>The World Athletics Championship is a global sports competition featuring top athletes in track and field.\nentity<|#|>Tokyo<|#|>location<|#|>Tokyo is the host city of the World Athletics Championship.\nentity<|#|>Noah Carter<|#|>person<|#|>Noah Carter is a sprinter who set a new record in the 100m sprint at the World Athletics Championship.\nentity<|#|>100m Sprint Record<|#|>category<|#|>The 100m sprint record is a benchmark in athletics, recently broken by Noah Carter.\nentity<|#|>Carbon-Fiber Spikes<|#|>equipment<|#|>Carbon-fiber spikes are advanced sprinting shoes that provide enhanced speed and traction.\nentity<|#|>World Athletics Federation<|#|>organization<|#|>The World Athletics Federation is the governing body overseeing the World Athletics Championship and record validations.\nrelation<|#|>World Athletics Championship<|#|>Tokyo<|#|>event location, international competition<|#|>The World Athletics Championship is being hosted in Tokyo.\nrelation<|#|>Noah Carter<|#|>100m Sprint Record<|#|>athlete achievement, record-breaking<|#|>Noah Carter set a new 100m sprint record at the championship.\nrelation<|#|>Noah Carter<|#|>Carbon-Fiber Spikes<|#|>athletic equipment, performance boost<|#|>Noah Carter used carbon-fiber spikes to enhance performance during the race.\nrelation<|#|>Noah Carter<|#|>World Athletics Championship<|#|>athlete participation, competition<|#|>Noah Carter is competing at the World Athletics Championship.\n<|COMPLETE|>\n\n\n\n---Real Data to be Processed---\n<Input>\nEntity_types: [Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject]\nText:\n```\nASurveyofReinforcementLearningforLargeReasoningModels\nRL from Human Direct Preference RL with Verifiable\nFeedback Optimization Reward\nGPT-3.5, GPT-4 Llama 3, Qwen 2.5 o1, DeepSeek-R1\nReward-based Reward-free Rule-based\n2022 2023 2025\nyticapaC\ngnivloS\nksaT\nRLHF DPO RLVR Open-ended RL\nFigure2 | RLHFandDPOhavebeenthetwopredominantRLmethodologiesforhumanalignmentin\nrecent years. In contrast, RLVR represents an emerging trend in RL for LRMs, significantly enhancing\ntheir capacity for complex task solving. The next stage of scaling RL for LLMs remains an open\nquestion, with open-ended RL presenting a particularly challenging and promising direction.\ndesign ( 3.1), policy optimization ( 3.2), and sampling strategies ( 3.3), comparing the\ndifferent research directions and technical approaches for each component.\n We discuss foundational and still controversial problems in RL for LRMs ( 4), such as the role of\nRL(4.1),RLversusSupervisedFine-Tuning(SFT)(4.2),modelpriors(4.3),trainingrecipes\n(4.4), andrewarddefinitions(4.5). Wearguethattheseissueswarrantfurtherexplorationto\nenable continued scaling of RL.\n WeexaminetrainingresourcesforRL(5),includingstaticcorpora(5.1),dynamicenvironments\n( 5.2), and training infrastructure ( 5.3). While these resources are reusable in both research\nand production, further standardization and development are needed.\n We review applications of RL to a wide range of tasks ( 6), such as coding tasks ( 6.1), agentic\ntasks ( 6.2), multimodal tasks ( 6.3), multi-agent systems ( 6.4), robotics tasks ( 6.5), and\nmedical applications ( 6.6).\n Finally, we discuss future directions in RL for language models ( 7), covering novel algorithms,\nmechanisms, features, and additional research avenues.\n2. Preliminaries\n2.1. Background\nInthissubsection,weintroducethebasiccomponentsofRLanddescribehowlanguagemodelscanbe\nconfigured as agents within RL frameworks. As shown in Figure 3, RL provides a general framework\nfor sequential decision making, in which an agent interacts with an environment by taking actions\nto maximize cumulative reward. In classical RL, the problem is typically formulated as a Markov\n5\n\nTables:\n[\n  [\n    [\n      \"RLHF\",\n      \"DPO\",\n      \"RLVR\",\n      \"Open-ended RL\"\n    ]\n  ],\n  [\n    [\n      \"\",\n      \"\"\n    ]\n  ]\n]\n```\n[{\"role\": \"user\", \"content\": \"---Task---\\nExtract entities and relationships from the input text to be processed.\\n\\n---Instructions---\\n1.  **Strict Adherence to Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system prompt.\\n2.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\\n3.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant entities and relationships have been extracted and presented.\\n4.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\\n\\n<Output>\"}, {\"role\": \"assistant\", \"content\": \"entity<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Content<|#|>A survey document that focuses on reinforcement learning for large reasoning models.\\nentity<|#|>Reinforcement Learning (RL)<|#|>Concept<|#|>A general framework for sequential decision making where an agent interacts with an environment by taking actions to maximize cumulative reward.\\nentity<|#|>Large Reasoning Models (LRMs)<|#|>Concept<|#|>A category of models that are the subject of a survey on reinforcement learning, whose capacity for complex task solving can be enhanced by RLVR.\\nentity<|#|>RL From Human Direct Preference<|#|>Method<|#|>A reinforcement learning methodology for human alignment.\\nentity<|#|>RL With Verifiable Feedback Optimization Reward<|#|>Method<|#|>A reinforcement learning methodology for human alignment.\\nentity<|#|>GPT-3.5<|#|>Artifact<|#|>A specific large language model mentioned in the context of reinforcement learning.\\nentity<|#|>GPT-4<|#|>Artifact<|#|>A specific large language model mentioned in the context of reinforcement learning.\\nentity<|#|>Llama 3<|#|>Artifact<|#|>A specific large language model mentioned in the context of reinforcement learning.\\nentity<|#|>Qwen 2.5<|#|>Artifact<|#|>A specific large language model mentioned in the context of reinforcement learning.\\nentity<|#|>O1<|#|>Artifact<|#|>A specific model mentioned in the context of reinforcement learning.\\nentity<|#|>DeepSeek-R1<|#|>Artifact<|#|>A specific large language model mentioned in the context of reinforcement learning.\\nentity<|#|>Reward-Based RL<|#|>Method<|#|>A category of reinforcement learning approaches.\\nentity<|#|>Reward-Free RL<|#|>Method<|#|>A category of reinforcement learning approaches.\\nentity<|#|>Rule-Based RL<|#|>Method<|#|>A category of reinforcement learning approaches.\\nentity<|#|>RLHF<|#|>Method<|#|>Reinforcement Learning from Human Feedback, a predominant RL methodology for human alignment in recent years.\\nentity<|#|>DPO<|#|>Method<|#|>Direct Preference Optimization, a predominant RL methodology for human alignment in recent years.\\nentity<|#|>RLVR<|#|>Method<|#|>Reinforcement Learning with Verifiable Feedback, an emerging trend in RL for LRMs that significantly enhances their capacity for complex task solving.\\nentity<|#|>Open-Ended RL<|#|>Method<|#|>A challenging and promising direction for scaling reinforcement learning for large language models.\\nentity<|#|>Human Alignment<|#|>Concept<|#|>A goal for reinforcement learning methodologies like RLHF and DPO.\\nentity<|#|>Complex Task Solving<|#|>Concept<|#|>A capacity of large reasoning models that RLVR aims to enhance.\\nentity<|#|>Large Language Models (LLMs)<|#|>Concept<|#|>A category of language models for which the next stage of scaling RL remains an open question.\\nentity<|#|>Policy Optimization<|#|>Method<|#|>A component of reinforcement learning design.\\nentity<|#|>Sampling Strategies<|#|>Method<|#|>A component of reinforcement learning design.\\nentity<|#|>Supervised Fine-Tuning (SFT)<|#|>Method<|#|>A method compared with reinforcement learning in foundational problems for LRMs.\\nentity<|#|>Model Priors<|#|>Concept<|#|>A foundational and controversial problem in reinforcement learning for LRMs.\\nentity<|#|>Training Recipes<|#|>Concept<|#|>A foundational and controversial problem in reinforcement learning for LRMs.\\nentity<|#|>Reward Definitions<|#|>Concept<|#|>A foundational and controversial problem in reinforcement learning for LRMs.\\nentity<|#|>Training Resources<|#|>Concept<|#|>Resources for reinforcement learning, including static corpora, dynamic environments, and training infrastructure.\\nentity<|#|>Static Corpora<|#|>Data<|#|>A type of training resource for reinforcement learning.\\nentity<|#|>Dynamic Environments<|#|>Concept<|#|>A type of training resource for reinforcement learning.\\nentity<|#|>Training Infrastructure<|#|>Artifact<|#|>A type of training resource for reinforcement learning.\\nentity<|#|>Coding Tasks<|#|>Concept<|#|>An application area of reinforcement learning.\\nentity<|#|>Agentic Tasks<|#|>Concept<|#|>An application area of reinforcement learning.\\nentity<|#|>Multimodal Tasks<|#|>Concept<|#|>An application area of reinforcement learning.\\nentity<|#|>Multi-Agent Systems<|#|>Concept<|#|>An application area of reinforcement learning.\\nentity<|#|>Robotics Tasks<|#|>Concept<|#|>An application area of reinforcement learning.\\nentity<|#|>Medical Applications<|#|>Concept<|#|>An application area of reinforcement learning.\\nentity<|#|>Agent<|#|>Concept<|#|>A component within RL frameworks that interacts with an environment by taking actions to maximize cumulative reward.\\nentity<|#|>Environment<|#|>Concept<|#|>A component within RL frameworks with which an agent interacts.\\nentity<|#|>Actions<|#|>Concept<|#|>Taken by an agent within an RL framework to interact with an environment.\\nentity<|#|>Cumulative Reward<|#|>Concept<|#|>The objective that an agent aims to maximize in reinforcement learning.\\nentity<|#|>Markov Decision Process<|#|>Concept<|#|>A typical formulation for problems in classical reinforcement learning.\\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Reinforcement Learning (RL)<|#|>scope<|#|>The survey document focuses on the field of Reinforcement Learning.\\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Large Reasoning Models (LRMs)<|#|>scope<|#|>The survey document focuses on Large Reasoning Models.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>RLHF<|#|>category<|#|>RLHF is a predominant methodology within Reinforcement Learning.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>DPO<|#|>category<|#|>DPO is a predominant methodology within Reinforcement Learning.\\nrelation<#|>Reinforcement Learning (RL)<|#|>RLVR<|#|>category<|#|>RLVR is an emerging trend within Reinforcement Learning.\\nrelation<|#|>RLHF<|#|>Human Alignment<|#|>objective<|#|>RLHF is a predominant methodology for human alignment.\\nrelation<|#|>DPO<|#|>Human Alignment<|#|>objective<|#|>DPO is a predominant methodology for human alignment.\\nrelation<|#|>RLVR<|#|>Large Reasoning Models (LRMs)<|#|>application<|#|>RLVR represents an emerging trend in RL specifically for Large Reasoning Models.\\nrelation<|#|>RLVR<|#|>Complex Task Solving<|#|>enhancement<|#|>RLVR significantly enhances the capacity of LRMs for complex task solving.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Open-Ended RL<|#|>category<|#|>Open-Ended RL is a challenging and promising direction within Reinforcement Learning.\\nrelation<|#|>Open-Ended RL<|#|>Large Language Models (LLMs)<|#|>research direction<|#|>Open-Ended RL presents a promising direction for scaling RL for Large Language Models.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Policy Optimization<|#|>component<|#|>Policy optimization is a component of reinforcement learning design.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Sampling Strategies<|#|>component<|#|>Sampling strategies are a component of reinforcement learning design.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Supervised Fine-Tuning (SFT)<|#|>comparison<|#|>Reinforcement Learning is compared with Supervised Fine-Tuning as a foundational problem.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Model Priors<|#|>issue<|#|>Model priors are discussed as foundational and controversial problems in RL.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Training Recipes<|#|>issue<|#|>Training recipes are discussed as foundational and controversial problems in RL.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Reward Definitions<|#|>issue<|#|>Reward definitions are discussed as foundational and controversial problems in RL.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Training Resources<|#|>resource usage<|#|>Training resources are examined for their role in Reinforcement Learning.\\nrelation<|#|>Training Resources<|#|>Static Corpora<|#|>resource type<|#|>Static corpora are a type of training resource.\\nrelation<|#|>Training Resources<|#|>Dynamic Environments<|#|>resource type<|#|>Dynamic environments are a type of training resource.\\nrelation<|#|>Training Resources<|#|>Training Infrastructure<|#|>resource type<|#|>Training infrastructure is a type of training resource.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Coding Tasks<|#|>application<|#|>Reinforcement Learning is applied to coding tasks.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Agentic Tasks<|#|>application<|#|>Reinforcement Learning is applied to agentic tasks.\\nrelation<#|>Reinforcement Learning (RL)<|#|>Multimodal Tasks<|#|>application<|#|>Reinforcement Learning is applied to multimodal tasks.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Multi-Agent Systems<|#|>application<|#|>Reinforcement Learning is applied to multi-agent systems.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Robotics Tasks<|#|>application<|#|>Reinforcement Learning is applied to robotics tasks.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Medical Applications<|#|>application<|#|>Reinforcement Learning is applied to medical applications.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Agent<|#|>framework component<|#|>Reinforcement Learning frameworks configure language models as agents.\\nrelation<|#|>Agent<|#|>Environment<|#|>interaction<|#|>An agent interacts with an environment within RL frameworks.\\nrelation<|#|>Agent<|#|>Actions<|#|>action<|#|>An agent takes actions within RL frameworks.\\nrelation<|#|>Agent<|#|>Cumulative Reward<|#|>objective<|#|>An agent aims to maximize cumulative reward within RL frameworks.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Markov Decision Process<|#|>formalization<|#|>Reinforcement Learning problems are typically formulated as a Markov Decision Process.\\nrelation<|#|>GPT-3.5<|#|>Large Reasoning Models (LRMs)<|#|>example<|#|>GPT-3.5 is listed as an example of a model relevant to the survey.\\nrelation<|#|>GPT-4<|#|>Large Reasoning Models (LRMs)<|#|>example<|#|>GPT-4 is listed as an example of a model relevant to the survey.\\nrelation<|#|>Llama 3<|#|>Large Reasoning Models (LRMs)<|#|>example<|#|>Llama 3 is listed as an example of a model relevant to the survey.\\nrelation<|#|>Qwen 2.5<|#|>Large Reasoning Models (LRMs)<|#|>example<|#|>Qwen 2.5 is listed as an example of a model relevant to the survey.\\nrelation<|#|>O1<|#|>Large Reasoning Models (LRMs)<|#|>example<|#|>O1 is listed as an example of a model relevant to the survey.\\nrelation<|#|>DeepSeek-R1<|#|>Large Reasoning Models (LRMs)<|#|>example<|#|>DeepSeek-R1 is listed as an example of a model relevant to the survey.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>RL From Human Direct Preference<|#|>category<|#|>RL From Human Direct Preference is a type of Reinforcement Learning.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>RL With Verifiable Feedback Optimization Reward<|#|>category<|#|>RL With Verifiable Feedback Optimization Reward is a type of Reinforcement Learning.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Reward-Based RL<|#|>category<|#|>Reward-Based RL is a category of reinforcement learning approaches.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Reward-Free RL<|#|>category<|#|>Reward-Free RL is a category of reinforcement learning approaches.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Rule-Based RL<|#|>category<|#|>Rule-Based RL is a category of reinforcement learning approaches.\\n<|COMPLETE|>\"}]",
    "queryparam": null,
    "create_time": 1765218858,
    "update_time": 1765218858,
    "_id": "default:extract:802d0fbae2d79a2b6a33d6d06de6f6c7"
  },
  "default:extract:534e07be110a0f585f9207a8c3ecf337": {
    "return": "entity<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>content<|#|>The title of a survey focusing on the application of reinforcement learning to large reasoning models.\nentity<|#|>Reinforcement Learning (RL)<|#|>concept<|#|>A learning paradigm where an agent learns to make decisions by taking actions in an environment to maximize a cumulative reward.\nentity<|#|>Large Reasoning Models<|#|>concept<|#|>Models that are the subject of the survey in the context of reinforcement learning.\nentity<|#|>Agent<|#|>concept<|#|>A component in Reinforcement Learning that selects actions, observes states, and receives rewards from the environment.\nentity<|#|>Reward<|#|>concept<|#|>A signal provided by the environment to the agent, indicating the desirability of an action or state, assigned at various granularities (sequence-level, token-level, step-level).\nentity<|#|>Environment<|#|>concept<|#|>The component in Reinforcement Learning that provides states and rewards to the agent in response to its actions.\nentity<|#|>Language Models (LMs)<|#|>artifact<|#|>Models that can act as agents in Reinforcement Learning, where completion tokens are treated as actions.\nentity<|#|>Completion Tokens<|#|>data<|#|>Specific tokens generated by Language Models that are treated as actions in the context of Reinforcement Learning.\nentity<|#|>State<|#|>concept<|#|>The current situation observed by the agent. In Language Models, it is defined as the prompt concatenated with tokens generated so far.\nentity<|#|>Action<|#|>concept<|#|>The unit chosen by the agent at a given step, which can be an entire sequence, a token, or a segment.\nentity<|#|>Markov Decision Process (MDP)<|#|>concept<|#|>A mathematical framework for modeling decision-making, defined by a tuple (S,A,P,R,), and introduced by Sutton et al., 1998.\nentity<|#|>Sutton et al., 1998<|#|>content<|#|>A reference to a foundational work that defines the Markov Decision Process and its components.\nentity<|#|>State Space (S)<|#|>concept<|#|>The set of all possible states an agent can be in within a Markov Decision Process.\nentity<|#|>Action Space (A)<|#|>concept<|#|>The set of all possible actions an agent can take within a Markov Decision Process.\nentity<|#|>Transition Dynamics (P)<|#|>concept<|#|>A function in a Markov Decision Process that describes the probability of transitioning from one state to another after taking a specific action, usually deterministic in LLMs.\nentity<|#|>Reward Function (R)<|#|>concept<|#|>A function in a Markov Decision Process that assigns a scalar reward to a state-action pair.\nentity<|#|>Discount Factor ()<|#|>concept<|#|>A value in a Markov Decision Process, between 0 and 1, that determines the present value of future rewards.\nentity<|#|>Policy ()<|#|>concept<|#|>A function parameterized by  that maps states to actions, guiding the agent's behavior, and representing the language model in the context of LMs.\nentity<|#|>Prompt/Task (x)<|#|>data<|#|>Corresponds to the initial state or environment context for a Language Model, drawn from a data distribution.\nentity<|#|>Data Distribution (D)<|#|>concept<|#|>The distribution from which prompts/tasks are drawn, corresponding to a dataset.\nentity<|#|>Sequence (y)<|#|>data<|#|>A series of tokens generated by a language model in response to a prompt, representing the output of the policy.\nentity<|#|>Token (a  V)<|#|>data<|#|>A unit chosen at a step, representing a single word or sub-word unit from a vocabulary V, which can be an action.\nentity<|#|>Segment (y(k))<|#|>data<|#|>A sub-sequence of tokens, representing a step-level action.\nentity<|#|>EOS Token<|#|>data<|#|>An End-Of-Sequence token that, when present in the state, signifies the end of a trajectory.\nentity<|#|>Trajectory<|#|>concept<|#|>A sequence of states, actions, and rewards from the beginning to the end of an episode.\nentity<|#|>Return (G)<|#|>concept<|#|>The cumulative reward of the whole trajectory for a given prompt, typically with a discount factor of 1 for finite horizons.\nentity<|#|>Learning Objective<|#|>concept<|#|>The goal in reinforcement learning, which is to maximize the expected cumulative reward over the data distribution.\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Reinforcement Learning (RL)<|#|>focus, subject<|#|>The survey focuses on Reinforcement Learning.\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Large Reasoning Models<|#|>focus, subject<|#|>The survey focuses on Large Reasoning Models.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Agent<|#|>component, involvement<|#|>Reinforcement Learning involves an Agent.\nrelation<|#|>Reinforcement Learning (RL)<|#|>Environment<|#|>component, involvement<|#|>Reinforcement Learning involves an Environment.\nrelation<|#|>Agent<|#|>Action<|#|>performs, selection<|#|>The Agent selects actions.\nrelation<|#|>Environment<|#|>State<|#|>provides, interaction<|#|>The Environment provides states.\nrelation<|#|>Environment<|#|>Reward<|#|>provides, interaction<|#|>The Environment provides rewards.\nrelation<|#|>Language Models (LMs)<|#|>Agent<|#|>role, representation<|#|>Language Models act as Agents in the context of Reinforcement Learning.\nrelation<|#|>Language Models (LMs)<|#|>Completion Tokens<|#|>treatment, action<|#|>Language Models treat completion tokens as actions.\nrelation<|#|>State<|#|>Prompt/Task (x)<|#|>composition, context<|#|>The State is formed by the prompt and generated tokens.\nrelation<|#|>State<|#|>Completion Tokens<|#|>composition, context<|#|>The State is formed by completion tokens concatenated with the context.\nrelation<|#|>Reward<|#|>Response<|#|>assignment, evaluation<|#|>Rewards are typically assigned at the level of the entire response.\nrelation<|#|>Markov Decision Process (MDP)<|#|>Sutton et al., 1998<|#|>definition, origin<|#|>The Markov Decision Process is defined by Sutton et al., 1998.\nrelation<|#|>Markov Decision Process (MDP)<|#|>State Space (S)<|#|>component, definition<|#|>The Markov Decision Process includes a State Space (S).\nrelation<|#|>Markov Decision Process (MDP)<|#|>Action Space (A)<|#|>component, definition<|#|>The Markov Decision Process includes an Action Space (A).\nrelation<|#|>Markov Decision Process (MDP)<|#|>Transition Dynamics (P)<|#|>component, definition<|#|>The Markov Decision Process includes Transition Dynamics (P).\nrelation<|#|>Markov Decision Process (MDP)<|#|>Reward Function (R)<|#|>component, definition<|#|>The Markov Decision Process includes a Reward Function (R).\nrelation<|#|>Markov Decision Process (MDP)<|#|>Discount Factor ()<|#|>component, definition<|#|>The Markov Decision Process includes a Discount Factor ().\nrelation<|#|>Agent<|#|>State<|#|>observes, interaction<|#|>The Agent observes a state.\nrelation<|#|>Agent<|#|>Policy ()<|#|>guides, behavior<|#|>The Agent selects an action according to its Policy.\nrelation<|#|>Policy ()<|#|>Language Models (LMs)<|#|>represents, implementation<|#|>The Policy represents the Language Model.\nrelation<|#|>Policy ()<|#|>Sequence (y)<|#|>generates, output<|#|>The Policy generates a sequence.\nrelation<|#|>Prompt/Task (x)<|#|>Data Distribution (D)<|#|>source, context<|#|>The Prompt/Task is drawn from a data distribution.\nrelation<|#|>State<|#|>Prompt/Task (x)<|#|>composition, context<|#|>The State is defined as the prompt together with generated tokens.\nrelation<|#|>State<|#|>Token (a  V)<|#|>composition, context<|#|>The State is defined as the prompt together with generated tokens.\nrelation<|#|>Action<|#|>Sequence (y)<|#|>granularity, type<|#|>An Action may be an entire sequence.\nrelation<|#|>Action<|#|>Token (a  V)<|#|>granularity, type<|#|>An Action may be a token.\nrelation<|#|>Action<|#|>Segment (y(k))<|#|>granularity, type<|#|>An Action may be a segment.\nrelation<|#|>Transition Dynamics (P)<|#|>State<|#|>determines, evolution<|#|>Transition Dynamics determine the next state based on the current state and action.\nrelation<|#|>EOS Token<|#|>Trajectory<|#|>termination, signal<|#|>An EOS Token in the state causes the trajectory to end.\nrelation<|#|>Reward<|#|>Action<|#|>assignment, granularity<|#|>Reward is assigned based on action granularity.\nrelation<|#|>Reward<|#|>Sequence (y)<|#|>level, assignment<|#|>Reward can be sequence-level.\nrelation<|#|>Reward<|#|>Token (a  V)<|#|>level, assignment<|#|>Reward can be token-level.\nrelation<|#|>Reward<|#|>Segment (y(k))<|#|>level, assignment<|#|>Reward can be step-level.\nrelation<|#|>Return (G)<|#|>Trajectory<|#|>measurement, aggregation<|#|>Return is the cumulative reward of the whole trajectory.\nrelation<|#|>Learning Objective<|#|>Return (G)<|#|>maximization, goal<|#|>The Learning Objective is to maximize the expected cumulative Return.\nrelation<|#|>Learning Objective<|#|>Data Distribution (D)<|#|>scope, optimization<|#|>The Learning Objective is maximized over the data distribution.\n<|COMPLETE|>",
    "cache_type": "extract",
    "chunk_id": "chunk-42002d06ba2f88fbde0f4fdbf912a18f",
    "original_prompt": "---Task---\nExtract entities and relationships from the input text to be processed.\n\n---Instructions---\n1.  **Strict Adherence to Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system prompt.\n2.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\n3.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant entities and relationships have been extracted and presented.\n4.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\n\n<Output>\n---Role---\nYou are a Knowledge Graph Specialist responsible for extracting entities and relationships from the input text.\n\n---Instructions---\n1.  **Entity Extraction & Output:**\n    *   **Identification:** Identify clearly defined and meaningful entities in the input text.\n    *   **Entity Details:** For each identified entity, extract the following information:\n        *   `entity_name`: The name of the entity. If the entity name is case-insensitive, capitalize the first letter of each significant word (title case). Ensure **consistent naming** across the entire extraction process.\n        *   `entity_type`: Categorize the entity using one of the following types: `Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject`. If none of the provided entity types apply, do not add new entity type and classify it as `Other`.\n        *   `entity_description`: Provide a concise yet comprehensive description of the entity's attributes and activities, based *solely* on the information present in the input text.\n    *   **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n        *   Format: `entity<|#|>entity_name<|#|>entity_type<|#|>entity_description`\n\n2.  **Relationship Extraction & Output:**\n    *   **Identification:** Identify direct, clearly stated, and meaningful relationships between previously extracted entities.\n    *   **N-ary Relationship Decomposition:** If a single statement describes a relationship involving more than two entities (an N-ary relationship), decompose it into multiple binary (two-entity) relationship pairs for separate description.\n        *   **Example:** For \"Alice, Bob, and Carol collaborated on Project X,\" extract binary relationships such as \"Alice collaborated with Project X,\" \"Bob collaborated with Project X,\" and \"Carol collaborated with Project X,\" or \"Alice collaborated with Bob,\" based on the most reasonable binary interpretations.\n    *   **Relationship Details:** For each binary relationship, extract the following fields:\n        *   `source_entity`: The name of the source entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `target_entity`: The name of the target entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `relationship_keywords`: One or more high-level keywords summarizing the overarching nature, concepts, or themes of the relationship. Multiple keywords within this field must be separated by a comma `,`. **DO NOT use `<|#|>` for separating multiple keywords within this field.**\n        *   `relationship_description`: A concise explanation of the nature of the relationship between the source and target entities, providing a clear rationale for their connection.\n    *   **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n        *   Format: `relation<|#|>source_entity<|#|>target_entity<|#|>relationship_keywords<|#|>relationship_description`\n\n3.  **Delimiter Usage Protocol:**\n    *   The `<|#|>` is a complete, atomic marker and **must not be filled with content**. It serves strictly as a field separator.\n    *   **Incorrect Example:** `entity<|#|>Tokyo<|location|>Tokyo is the capital of Japan.`\n    *   **Correct Example:** `entity<|#|>Tokyo<|#|>location<|#|>Tokyo is the capital of Japan.`\n\n4.  **Relationship Direction & Duplication:**\n    *   Treat all relationships as **undirected** unless explicitly stated otherwise. Swapping the source and target entities for an undirected relationship does not constitute a new relationship.\n    *   Avoid outputting duplicate relationships.\n\n5.  **Output Order & Prioritization:**\n    *   Output all extracted entities first, followed by all extracted relationships.\n    *   Within the list of relationships, prioritize and output those relationships that are **most significant** to the core meaning of the input text first.\n\n6.  **Context & Objectivity:**\n    *   Ensure all entity names and descriptions are written in the **third person**.\n    *   Explicitly name the subject or object; **avoid using pronouns** such as `this article`, `this paper`, `our company`, `I`, `you`, and `he/she`.\n\n7.  **Language & Proper Nouns:**\n    *   The entire output (entity names, keywords, and descriptions) must be written in `English`.\n    *   Proper nouns (e.g., personal names, place names, organization names) should be retained in their original language if a proper, widely accepted translation is not available or would cause ambiguity.\n\n8.  **Completion Signal:** Output the literal string `<|COMPLETE|>` only after all entities and relationships, following all criteria, have been completely extracted and outputted.\n\n---Examples---\n<Input Text>\n```\nwhile Alex clenched his jaw, the buzz of frustration dull against the backdrop of Taylor's authoritarian certainty. It was this competitive undercurrent that kept him alert, the sense that his and Jordan's shared commitment to discovery was an unspoken rebellion against Cruz's narrowing vision of control and order.\n\nThen Taylor did something unexpected. They paused beside Jordan and, for a moment, observed the device with something akin to reverence. \"If this tech can be understood...\" Taylor said, their voice quieter, \"It could change the game for us. For all of us.\"\n\nThe underlying dismissal earlier seemed to falter, replaced by a glimpse of reluctant respect for the gravity of what lay in their hands. Jordan looked up, and for a fleeting heartbeat, their eyes locked with Taylor's, a wordless clash of wills softening into an uneasy truce.\n\nIt was a small transformation, barely perceptible, but one that Alex noted with an inward nod. They had all been brought here by different paths\n```\n\n<Output>\nentity<|#|>Alex<|#|>person<|#|>Alex is a character who experiences frustration and is observant of the dynamics among other characters.\nentity<|#|>Taylor<|#|>person<|#|>Taylor is portrayed with authoritarian certainty and shows a moment of reverence towards a device, indicating a change in perspective.\nentity<|#|>Jordan<|#|>person<|#|>Jordan shares a commitment to discovery and has a significant interaction with Taylor regarding a device.\nentity<|#|>Cruz<|#|>person<|#|>Cruz is associated with a vision of control and order, influencing the dynamics among other characters.\nentity<|#|>The Device<|#|>equipment<|#|>The Device is central to the story, with potential game-changing implications, and is revered by Taylor.\nrelation<|#|>Alex<|#|>Taylor<|#|>power dynamics, observation<|#|>Alex observes Taylor's authoritarian behavior and notes changes in Taylor's attitude toward the device.\nrelation<|#|>Alex<|#|>Jordan<|#|>shared goals, rebellion<|#|>Alex and Jordan share a commitment to discovery, which contrasts with Cruz's vision.)\nrelation<|#|>Taylor<|#|>Jordan<|#|>conflict resolution, mutual respect<|#|>Taylor and Jordan interact directly regarding the device, leading to a moment of mutual respect and an uneasy truce.\nrelation<|#|>Jordan<|#|>Cruz<|#|>ideological conflict, rebellion<|#|>Jordan's commitment to discovery is in rebellion against Cruz's vision of control and order.\nrelation<|#|>Taylor<|#|>The Device<|#|>reverence, technological significance<|#|>Taylor shows reverence towards the device, indicating its importance and potential impact.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nStock markets faced a sharp downturn today as tech giants saw significant declines, with the global tech index dropping by 3.4% in midday trading. Analysts attribute the selloff to investor concerns over rising interest rates and regulatory uncertainty.\n\nAmong the hardest hit, nexon technologies saw its stock plummet by 7.8% after reporting lower-than-expected quarterly earnings. In contrast, Omega Energy posted a modest 2.1% gain, driven by rising oil prices.\n\nMeanwhile, commodity markets reflected a mixed sentiment. Gold futures rose by 1.5%, reaching $2,080 per ounce, as investors sought safe-haven assets. Crude oil prices continued their rally, climbing to $87.60 per barrel, supported by supply constraints and strong demand.\n\nFinancial experts are closely watching the Federal Reserve's next move, as speculation grows over potential rate hikes. The upcoming policy announcement is expected to influence investor confidence and overall market stability.\n```\n\n<Output>\nentity<|#|>Global Tech Index<|#|>category<|#|>The Global Tech Index tracks the performance of major technology stocks and experienced a 3.4% decline today.\nentity<|#|>Nexon Technologies<|#|>organization<|#|>Nexon Technologies is a tech company that saw its stock decline by 7.8% after disappointing earnings.\nentity<|#|>Omega Energy<|#|>organization<|#|>Omega Energy is an energy company that gained 2.1% in stock value due to rising oil prices.\nentity<|#|>Gold Futures<|#|>product<|#|>Gold futures rose by 1.5%, indicating increased investor interest in safe-haven assets.\nentity<|#|>Crude Oil<|#|>product<|#|>Crude oil prices rose to $87.60 per barrel due to supply constraints and strong demand.\nentity<|#|>Market Selloff<|#|>category<|#|>Market selloff refers to the significant decline in stock values due to investor concerns over interest rates and regulations.\nentity<|#|>Federal Reserve Policy Announcement<|#|>category<|#|>The Federal Reserve's upcoming policy announcement is expected to impact investor confidence and market stability.\nentity<|#|>3.4% Decline<|#|>category<|#|>The Global Tech Index experienced a 3.4% decline in midday trading.\nrelation<|#|>Global Tech Index<|#|>Market Selloff<|#|>market performance, investor sentiment<|#|>The decline in the Global Tech Index is part of the broader market selloff driven by investor concerns.\nrelation<|#|>Nexon Technologies<|#|>Global Tech Index<|#|>company impact, index movement<|#|>Nexon Technologies' stock decline contributed to the overall drop in the Global Tech Index.\nrelation<|#|>Gold Futures<|#|>Market Selloff<|#|>market reaction, safe-haven investment<|#|>Gold prices rose as investors sought safe-haven assets during the market selloff.\nrelation<|#|>Federal Reserve Policy Announcement<|#|>Market Selloff<|#|>interest rate impact, financial regulation<|#|>Speculation over Federal Reserve policy changes contributed to market volatility and investor selloff.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nAt the World Athletics Championship in Tokyo, Noah Carter broke the 100m sprint record using cutting-edge carbon-fiber spikes.\n```\n\n<Output>\nentity<|#|>World Athletics Championship<|#|>event<|#|>The World Athletics Championship is a global sports competition featuring top athletes in track and field.\nentity<|#|>Tokyo<|#|>location<|#|>Tokyo is the host city of the World Athletics Championship.\nentity<|#|>Noah Carter<|#|>person<|#|>Noah Carter is a sprinter who set a new record in the 100m sprint at the World Athletics Championship.\nentity<|#|>100m Sprint Record<|#|>category<|#|>The 100m sprint record is a benchmark in athletics, recently broken by Noah Carter.\nentity<|#|>Carbon-Fiber Spikes<|#|>equipment<|#|>Carbon-fiber spikes are advanced sprinting shoes that provide enhanced speed and traction.\nentity<|#|>World Athletics Federation<|#|>organization<|#|>The World Athletics Federation is the governing body overseeing the World Athletics Championship and record validations.\nrelation<|#|>World Athletics Championship<|#|>Tokyo<|#|>event location, international competition<|#|>The World Athletics Championship is being hosted in Tokyo.\nrelation<|#|>Noah Carter<|#|>100m Sprint Record<|#|>athlete achievement, record-breaking<|#|>Noah Carter set a new 100m sprint record at the championship.\nrelation<|#|>Noah Carter<|#|>Carbon-Fiber Spikes<|#|>athletic equipment, performance boost<|#|>Noah Carter used carbon-fiber spikes to enhance performance during the race.\nrelation<|#|>Noah Carter<|#|>World Athletics Championship<|#|>athlete participation, competition<|#|>Noah Carter is competing at the World Athletics Championship.\n<|COMPLETE|>\n\n\n\n---Real Data to be Processed---\n<Input>\nEntity_types: [Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject]\nText:\n```\nASurveyofReinforcementLearningforLargeReasoningModels\naction\nAgent reward\nR(x,y)\ny 1 y 2\nstate reward action Language Model\nst rt at\n\nr t+1 x 1 x 2 x 3 x T-1 x T\nEnvironment\nr\nt+1 state\nFigure 3 | Basic components of RL and language models (LMs) as agents. The agent selects actions,\nwhile the environment provides states and rewards at each turn. In the context of LMs, completion\ntokensaretreatedasactions,whichareconcatenatedwiththecontexttoformthestate. Rewardsare\ntypically assigned at the level of the entire response.\nDecision Process (MDP) [Sutton et al., 1998], which is defined by a tuple (S,A,P,,). The main\ncomponentsincludeastatespaceS,anactionspaceA,transitiondynamicsP : SA  S,areward\nfunction  : SA  , and a discount factor   [0,1]. At each step, the agent observes a state \n\n,\nselectsanaction  accordingtoitspolicy parameterizedby,receivesareward ,andtransitsto\n  \nthe next state  . When applying RL to language models, these concepts can be naturally mapped\n+1\nto the language domain with minimal adaptation. The mapping is summarized as follows:\n Prompt/Task (): Corresponds to the initial state or environment context, drawn from a data\ndistribution and corresponding to the dataset D.\n Policy ( ): Representsthelanguagemodel,whichgeneratesasequenceoflength denotingas\n = ( 1 ,...,  ) in response to the prompt.\n State ( ): Defined as the prompt together with the tokens generated so far, i.e.,   = (, 1:1 ).\n Action( ): Theunitchosenatstep fromtheactionspaceA. Dependingonthegranularity,the\naction may be an entire sequence  (sequence-level), a token   V (token-level), or a segment\n\n() = ((),...,()) (step-level), with a detailed comparison in Table 2.\n1 \n\n Transition Dynamics (P): The state transition is usually deterministic in the context of LLMs\nsince  +1 = [  ,  ], where [,] denotes string concatenation. When the state contains an EOS\ntoken, the policy transits to a terminal state, meaning the trajectory ends.\n Reward ((,) or  ): Assigned based on the action granularity, e.g., sequence-level (,) at\ntrajectory end, token-level   =(, 1: ) per token, or step-level   =(,(1:)) per segment.\n Return(): Thecumulativerewardofthewholetrajectory forprompt(typicallywith =1for\nfinite horizons). It reduces to the single scalar (,) with sequence-level reward, or aggregates\nper-token/step rewards otherwise, as detailed in Table 2.\nInthissetting,thelearningobjective[Suttonetal.,1998]istomaximizetheexpectedcumulative\nreward over the data distribution D, that is,\nmaxJ() := []. (1)\n\nD, ()\n6\n```",
    "queryparam": null,
    "create_time": 1765218884,
    "update_time": 1765218884,
    "_id": "default:extract:534e07be110a0f585f9207a8c3ecf337"
  },
  "default:extract:27207a5489651209b57b4b2acaf0f53e": {
    "return": "entity<|#|>Dataset D<|#|>data<|#|>A specific dataset from which prompts/tasks are drawn, corresponding to the data distribution.\nentity<|#|>Terminal State<|#|>concept<|#|>A state to which the policy transits when an EOS token is present, signifying the end of a trajectory.\nentity<|#|>Large Language Models (LLMs)<|#|>artifact<|#|>A type of language model where state transitions are usually deterministic.\nentity<|#|>Table 2<|#|>content<|#|>A table referenced for detailed comparisons of action granularity and return aggregation.\nentity<|#|>Response<|#|>data<|#|>The entire output generated by a language model, to which rewards are typically assigned.\nrelation<|#|>Prompt/Task (x)<|#|>Dataset D<|#|>source, context<|#|>The Prompt/Task corresponds to the dataset D.\nrelation<|#|>Data Distribution (D)<|#|>Dataset D<|#|>representation, source<|#|>The Data Distribution corresponds to the dataset D.\nrelation<|#|>EOS Token<|#|>Terminal State<|#|>transition, consequence<|#|>The presence of an EOS token in the state leads to a transition to a terminal state.\nrelation<|#|>Policy ()<|#|>Terminal State<|#|>transition, state change<|#|>The policy transits to a terminal state when an EOS token is present.\nrelation<|#|>Trajectory<|#|>Terminal State<|#|>termination, endpoint<|#|>The trajectory ends at a terminal state.\nrelation<|#|>Transition Dynamics (P)<|#|>Large Language Models (LLMs)<|#|>context, characteristic<|#|>Transition Dynamics are usually deterministic in the context of LLMs.\nrelation<|#|>Action<|#|>Table 2<|#|>reference, detail<|#|>Action granularity is detailed in Table 2.\nrelation<|#|>Return (G)<|#|>Table 2<|#|>reference, detail<|#|>Return aggregation is detailed in Table 2.\n<|COMPLETE|>",
    "cache_type": "extract",
    "chunk_id": "chunk-42002d06ba2f88fbde0f4fdbf912a18f",
    "original_prompt": "---Task---\nBased on the last extraction task, identify and extract any **missed or incorrectly formatted** entities and relationships from the input text.\n\n---Instructions---\n1.  **Strict Adherence to System Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system instructions.\n2.  **Focus on Corrections/Additions:**\n    *   **Do NOT** re-output entities and relationships that were **correctly and fully** extracted in the last task.\n    *   If an entity or relationship was **missed** in the last task, extract and output it now according to the system format.\n    *   If an entity or relationship was **truncated, had missing fields, or was otherwise incorrectly formatted** in the last task, re-output the *corrected and complete* version in the specified format.\n3.  **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n4.  **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n5.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\n6.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant missing or corrected entities and relationships have been extracted and presented.\n7.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\n\n<Output>\n---Role---\nYou are a Knowledge Graph Specialist responsible for extracting entities and relationships from the input text.\n\n---Instructions---\n1.  **Entity Extraction & Output:**\n    *   **Identification:** Identify clearly defined and meaningful entities in the input text.\n    *   **Entity Details:** For each identified entity, extract the following information:\n        *   `entity_name`: The name of the entity. If the entity name is case-insensitive, capitalize the first letter of each significant word (title case). Ensure **consistent naming** across the entire extraction process.\n        *   `entity_type`: Categorize the entity using one of the following types: `Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject`. If none of the provided entity types apply, do not add new entity type and classify it as `Other`.\n        *   `entity_description`: Provide a concise yet comprehensive description of the entity's attributes and activities, based *solely* on the information present in the input text.\n    *   **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n        *   Format: `entity<|#|>entity_name<|#|>entity_type<|#|>entity_description`\n\n2.  **Relationship Extraction & Output:**\n    *   **Identification:** Identify direct, clearly stated, and meaningful relationships between previously extracted entities.\n    *   **N-ary Relationship Decomposition:** If a single statement describes a relationship involving more than two entities (an N-ary relationship), decompose it into multiple binary (two-entity) relationship pairs for separate description.\n        *   **Example:** For \"Alice, Bob, and Carol collaborated on Project X,\" extract binary relationships such as \"Alice collaborated with Project X,\" \"Bob collaborated with Project X,\" and \"Carol collaborated with Project X,\" or \"Alice collaborated with Bob,\" based on the most reasonable binary interpretations.\n    *   **Relationship Details:** For each binary relationship, extract the following fields:\n        *   `source_entity`: The name of the source entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `target_entity`: The name of the target entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `relationship_keywords`: One or more high-level keywords summarizing the overarching nature, concepts, or themes of the relationship. Multiple keywords within this field must be separated by a comma `,`. **DO NOT use `<|#|>` for separating multiple keywords within this field.**\n        *   `relationship_description`: A concise explanation of the nature of the relationship between the source and target entities, providing a clear rationale for their connection.\n    *   **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n        *   Format: `relation<|#|>source_entity<|#|>target_entity<|#|>relationship_keywords<|#|>relationship_description`\n\n3.  **Delimiter Usage Protocol:**\n    *   The `<|#|>` is a complete, atomic marker and **must not be filled with content**. It serves strictly as a field separator.\n    *   **Incorrect Example:** `entity<|#|>Tokyo<|location|>Tokyo is the capital of Japan.`\n    *   **Correct Example:** `entity<|#|>Tokyo<|#|>location<|#|>Tokyo is the capital of Japan.`\n\n4.  **Relationship Direction & Duplication:**\n    *   Treat all relationships as **undirected** unless explicitly stated otherwise. Swapping the source and target entities for an undirected relationship does not constitute a new relationship.\n    *   Avoid outputting duplicate relationships.\n\n5.  **Output Order & Prioritization:**\n    *   Output all extracted entities first, followed by all extracted relationships.\n    *   Within the list of relationships, prioritize and output those relationships that are **most significant** to the core meaning of the input text first.\n\n6.  **Context & Objectivity:**\n    *   Ensure all entity names and descriptions are written in the **third person**.\n    *   Explicitly name the subject or object; **avoid using pronouns** such as `this article`, `this paper`, `our company`, `I`, `you`, and `he/she`.\n\n7.  **Language & Proper Nouns:**\n    *   The entire output (entity names, keywords, and descriptions) must be written in `English`.\n    *   Proper nouns (e.g., personal names, place names, organization names) should be retained in their original language if a proper, widely accepted translation is not available or would cause ambiguity.\n\n8.  **Completion Signal:** Output the literal string `<|COMPLETE|>` only after all entities and relationships, following all criteria, have been completely extracted and outputted.\n\n---Examples---\n<Input Text>\n```\nwhile Alex clenched his jaw, the buzz of frustration dull against the backdrop of Taylor's authoritarian certainty. It was this competitive undercurrent that kept him alert, the sense that his and Jordan's shared commitment to discovery was an unspoken rebellion against Cruz's narrowing vision of control and order.\n\nThen Taylor did something unexpected. They paused beside Jordan and, for a moment, observed the device with something akin to reverence. \"If this tech can be understood...\" Taylor said, their voice quieter, \"It could change the game for us. For all of us.\"\n\nThe underlying dismissal earlier seemed to falter, replaced by a glimpse of reluctant respect for the gravity of what lay in their hands. Jordan looked up, and for a fleeting heartbeat, their eyes locked with Taylor's, a wordless clash of wills softening into an uneasy truce.\n\nIt was a small transformation, barely perceptible, but one that Alex noted with an inward nod. They had all been brought here by different paths\n```\n\n<Output>\nentity<|#|>Alex<|#|>person<|#|>Alex is a character who experiences frustration and is observant of the dynamics among other characters.\nentity<|#|>Taylor<|#|>person<|#|>Taylor is portrayed with authoritarian certainty and shows a moment of reverence towards a device, indicating a change in perspective.\nentity<|#|>Jordan<|#|>person<|#|>Jordan shares a commitment to discovery and has a significant interaction with Taylor regarding a device.\nentity<|#|>Cruz<|#|>person<|#|>Cruz is associated with a vision of control and order, influencing the dynamics among other characters.\nentity<|#|>The Device<|#|>equipment<|#|>The Device is central to the story, with potential game-changing implications, and is revered by Taylor.\nrelation<|#|>Alex<|#|>Taylor<|#|>power dynamics, observation<|#|>Alex observes Taylor's authoritarian behavior and notes changes in Taylor's attitude toward the device.\nrelation<|#|>Alex<|#|>Jordan<|#|>shared goals, rebellion<|#|>Alex and Jordan share a commitment to discovery, which contrasts with Cruz's vision.)\nrelation<|#|>Taylor<|#|>Jordan<|#|>conflict resolution, mutual respect<|#|>Taylor and Jordan interact directly regarding the device, leading to a moment of mutual respect and an uneasy truce.\nrelation<|#|>Jordan<|#|>Cruz<|#|>ideological conflict, rebellion<|#|>Jordan's commitment to discovery is in rebellion against Cruz's vision of control and order.\nrelation<|#|>Taylor<|#|>The Device<|#|>reverence, technological significance<|#|>Taylor shows reverence towards the device, indicating its importance and potential impact.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nStock markets faced a sharp downturn today as tech giants saw significant declines, with the global tech index dropping by 3.4% in midday trading. Analysts attribute the selloff to investor concerns over rising interest rates and regulatory uncertainty.\n\nAmong the hardest hit, nexon technologies saw its stock plummet by 7.8% after reporting lower-than-expected quarterly earnings. In contrast, Omega Energy posted a modest 2.1% gain, driven by rising oil prices.\n\nMeanwhile, commodity markets reflected a mixed sentiment. Gold futures rose by 1.5%, reaching $2,080 per ounce, as investors sought safe-haven assets. Crude oil prices continued their rally, climbing to $87.60 per barrel, supported by supply constraints and strong demand.\n\nFinancial experts are closely watching the Federal Reserve's next move, as speculation grows over potential rate hikes. The upcoming policy announcement is expected to influence investor confidence and overall market stability.\n```\n\n<Output>\nentity<|#|>Global Tech Index<|#|>category<|#|>The Global Tech Index tracks the performance of major technology stocks and experienced a 3.4% decline today.\nentity<|#|>Nexon Technologies<|#|>organization<|#|>Nexon Technologies is a tech company that saw its stock decline by 7.8% after disappointing earnings.\nentity<|#|>Omega Energy<|#|>organization<|#|>Omega Energy is an energy company that gained 2.1% in stock value due to rising oil prices.\nentity<|#|>Gold Futures<|#|>product<|#|>Gold futures rose by 1.5%, indicating increased investor interest in safe-haven assets.\nentity<|#|>Crude Oil<|#|>product<|#|>Crude oil prices rose to $87.60 per barrel due to supply constraints and strong demand.\nentity<|#|>Market Selloff<|#|>category<|#|>Market selloff refers to the significant decline in stock values due to investor concerns over interest rates and regulations.\nentity<|#|>Federal Reserve Policy Announcement<|#|>category<|#|>The Federal Reserve's upcoming policy announcement is expected to impact investor confidence and market stability.\nentity<|#|>3.4% Decline<|#|>category<|#|>The Global Tech Index experienced a 3.4% decline in midday trading.\nrelation<|#|>Global Tech Index<|#|>Market Selloff<|#|>market performance, investor sentiment<|#|>The decline in the Global Tech Index is part of the broader market selloff driven by investor concerns.\nrelation<|#|>Nexon Technologies<|#|>Global Tech Index<|#|>company impact, index movement<|#|>Nexon Technologies' stock decline contributed to the overall drop in the Global Tech Index.\nrelation<|#|>Gold Futures<|#|>Market Selloff<|#|>market reaction, safe-haven investment<|#|>Gold prices rose as investors sought safe-haven assets during the market selloff.\nrelation<|#|>Federal Reserve Policy Announcement<|#|>Market Selloff<|#|>interest rate impact, financial regulation<|#|>Speculation over Federal Reserve policy changes contributed to market volatility and investor selloff.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nAt the World Athletics Championship in Tokyo, Noah Carter broke the 100m sprint record using cutting-edge carbon-fiber spikes.\n```\n\n<Output>\nentity<|#|>World Athletics Championship<|#|>event<|#|>The World Athletics Championship is a global sports competition featuring top athletes in track and field.\nentity<|#|>Tokyo<|#|>location<|#|>Tokyo is the host city of the World Athletics Championship.\nentity<|#|>Noah Carter<|#|>person<|#|>Noah Carter is a sprinter who set a new record in the 100m sprint at the World Athletics Championship.\nentity<|#|>100m Sprint Record<|#|>category<|#|>The 100m sprint record is a benchmark in athletics, recently broken by Noah Carter.\nentity<|#|>Carbon-Fiber Spikes<|#|>equipment<|#|>Carbon-fiber spikes are advanced sprinting shoes that provide enhanced speed and traction.\nentity<|#|>World Athletics Federation<|#|>organization<|#|>The World Athletics Federation is the governing body overseeing the World Athletics Championship and record validations.\nrelation<|#|>World Athletics Championship<|#|>Tokyo<|#|>event location, international competition<|#|>The World Athletics Championship is being hosted in Tokyo.\nrelation<|#|>Noah Carter<|#|>100m Sprint Record<|#|>athlete achievement, record-breaking<|#|>Noah Carter set a new 100m sprint record at the championship.\nrelation<|#|>Noah Carter<|#|>Carbon-Fiber Spikes<|#|>athletic equipment, performance boost<|#|>Noah Carter used carbon-fiber spikes to enhance performance during the race.\nrelation<|#|>Noah Carter<|#|>World Athletics Championship<|#|>athlete participation, competition<|#|>Noah Carter is competing at the World Athletics Championship.\n<|COMPLETE|>\n\n\n\n---Real Data to be Processed---\n<Input>\nEntity_types: [Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject]\nText:\n```\nASurveyofReinforcementLearningforLargeReasoningModels\naction\nAgent reward\nR(x,y)\ny 1 y 2\nstate reward action Language Model\nst rt at\n\nr t+1 x 1 x 2 x 3 x T-1 x T\nEnvironment\nr\nt+1 state\nFigure 3 | Basic components of RL and language models (LMs) as agents. The agent selects actions,\nwhile the environment provides states and rewards at each turn. In the context of LMs, completion\ntokensaretreatedasactions,whichareconcatenatedwiththecontexttoformthestate. Rewardsare\ntypically assigned at the level of the entire response.\nDecision Process (MDP) [Sutton et al., 1998], which is defined by a tuple (S,A,P,,). The main\ncomponentsincludeastatespaceS,anactionspaceA,transitiondynamicsP : SA  S,areward\nfunction  : SA  , and a discount factor   [0,1]. At each step, the agent observes a state \n\n,\nselectsanaction  accordingtoitspolicy parameterizedby,receivesareward ,andtransitsto\n  \nthe next state  . When applying RL to language models, these concepts can be naturally mapped\n+1\nto the language domain with minimal adaptation. The mapping is summarized as follows:\n Prompt/Task (): Corresponds to the initial state or environment context, drawn from a data\ndistribution and corresponding to the dataset D.\n Policy ( ): Representsthelanguagemodel,whichgeneratesasequenceoflength denotingas\n = ( 1 ,...,  ) in response to the prompt.\n State ( ): Defined as the prompt together with the tokens generated so far, i.e.,   = (, 1:1 ).\n Action( ): Theunitchosenatstep fromtheactionspaceA. Dependingonthegranularity,the\naction may be an entire sequence  (sequence-level), a token   V (token-level), or a segment\n\n() = ((),...,()) (step-level), with a detailed comparison in Table 2.\n1 \n\n Transition Dynamics (P): The state transition is usually deterministic in the context of LLMs\nsince  +1 = [  ,  ], where [,] denotes string concatenation. When the state contains an EOS\ntoken, the policy transits to a terminal state, meaning the trajectory ends.\n Reward ((,) or  ): Assigned based on the action granularity, e.g., sequence-level (,) at\ntrajectory end, token-level   =(, 1: ) per token, or step-level   =(,(1:)) per segment.\n Return(): Thecumulativerewardofthewholetrajectory forprompt(typicallywith =1for\nfinite horizons). It reduces to the single scalar (,) with sequence-level reward, or aggregates\nper-token/step rewards otherwise, as detailed in Table 2.\nInthissetting,thelearningobjective[Suttonetal.,1998]istomaximizetheexpectedcumulative\nreward over the data distribution D, that is,\nmaxJ() := []. (1)\n\nD, ()\n6\n```\n[{\"role\": \"user\", \"content\": \"---Task---\\nExtract entities and relationships from the input text to be processed.\\n\\n---Instructions---\\n1.  **Strict Adherence to Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system prompt.\\n2.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\\n3.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant entities and relationships have been extracted and presented.\\n4.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\\n\\n<Output>\"}, {\"role\": \"assistant\", \"content\": \"entity<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>content<|#|>The title of a survey focusing on the application of reinforcement learning to large reasoning models.\\nentity<|#|>Reinforcement Learning (RL)<|#|>concept<|#|>A learning paradigm where an agent learns to make decisions by taking actions in an environment to maximize a cumulative reward.\\nentity<|#|>Large Reasoning Models<|#|>concept<|#|>Models that are the subject of the survey in the context of reinforcement learning.\\nentity<|#|>Agent<|#|>concept<|#|>A component in Reinforcement Learning that selects actions, observes states, and receives rewards from the environment.\\nentity<|#|>Reward<|#|>concept<|#|>A signal provided by the environment to the agent, indicating the desirability of an action or state, assigned at various granularities (sequence-level, token-level, step-level).\\nentity<|#|>Environment<|#|>concept<|#|>The component in Reinforcement Learning that provides states and rewards to the agent in response to its actions.\\nentity<|#|>Language Models (LMs)<|#|>artifact<|#|>Models that can act as agents in Reinforcement Learning, where completion tokens are treated as actions.\\nentity<|#|>Completion Tokens<|#|>data<|#|>Specific tokens generated by Language Models that are treated as actions in the context of Reinforcement Learning.\\nentity<|#|>State<|#|>concept<|#|>The current situation observed by the agent. In Language Models, it is defined as the prompt concatenated with tokens generated so far.\\nentity<|#|>Action<|#|>concept<|#|>The unit chosen by the agent at a given step, which can be an entire sequence, a token, or a segment.\\nentity<|#|>Markov Decision Process (MDP)<|#|>concept<|#|>A mathematical framework for modeling decision-making, defined by a tuple (S,A,P,R,), and introduced by Sutton et al., 1998.\\nentity<|#|>Sutton et al., 1998<|#|>content<|#|>A reference to a foundational work that defines the Markov Decision Process and its components.\\nentity<|#|>State Space (S)<|#|>concept<|#|>The set of all possible states an agent can be in within a Markov Decision Process.\\nentity<|#|>Action Space (A)<|#|>concept<|#|>The set of all possible actions an agent can take within a Markov Decision Process.\\nentity<|#|>Transition Dynamics (P)<|#|>concept<|#|>A function in a Markov Decision Process that describes the probability of transitioning from one state to another after taking a specific action, usually deterministic in LLMs.\\nentity<|#|>Reward Function (R)<|#|>concept<|#|>A function in a Markov Decision Process that assigns a scalar reward to a state-action pair.\\nentity<|#|>Discount Factor ()<|#|>concept<|#|>A value in a Markov Decision Process, between 0 and 1, that determines the present value of future rewards.\\nentity<|#|>Policy ()<|#|>concept<|#|>A function parameterized by  that maps states to actions, guiding the agent's behavior, and representing the language model in the context of LMs.\\nentity<|#|>Prompt/Task (x)<|#|>data<|#|>Corresponds to the initial state or environment context for a Language Model, drawn from a data distribution.\\nentity<|#|>Data Distribution (D)<|#|>concept<|#|>The distribution from which prompts/tasks are drawn, corresponding to a dataset.\\nentity<|#|>Sequence (y)<|#|>data<|#|>A series of tokens generated by a language model in response to a prompt, representing the output of the policy.\\nentity<|#|>Token (a  V)<|#|>data<|#|>A unit chosen at a step, representing a single word or sub-word unit from a vocabulary V, which can be an action.\\nentity<|#|>Segment (y(k))<|#|>data<|#|>A sub-sequence of tokens, representing a step-level action.\\nentity<|#|>EOS Token<|#|>data<|#|>An End-Of-Sequence token that, when present in the state, signifies the end of a trajectory.\\nentity<|#|>Trajectory<|#|>concept<|#|>A sequence of states, actions, and rewards from the beginning to the end of an episode.\\nentity<|#|>Return (G)<|#|>concept<|#|>The cumulative reward of the whole trajectory for a given prompt, typically with a discount factor of 1 for finite horizons.\\nentity<|#|>Learning Objective<|#|>concept<|#|>The goal in reinforcement learning, which is to maximize the expected cumulative reward over the data distribution.\\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Reinforcement Learning (RL)<|#|>focus, subject<|#|>The survey focuses on Reinforcement Learning.\\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Large Reasoning Models<|#|>focus, subject<|#|>The survey focuses on Large Reasoning Models.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Agent<|#|>component, involvement<|#|>Reinforcement Learning involves an Agent.\\nrelation<|#|>Reinforcement Learning (RL)<|#|>Environment<|#|>component, involvement<|#|>Reinforcement Learning involves an Environment.\\nrelation<|#|>Agent<|#|>Action<|#|>performs, selection<|#|>The Agent selects actions.\\nrelation<|#|>Environment<|#|>State<|#|>provides, interaction<|#|>The Environment provides states.\\nrelation<|#|>Environment<|#|>Reward<|#|>provides, interaction<|#|>The Environment provides rewards.\\nrelation<|#|>Language Models (LMs)<|#|>Agent<|#|>role, representation<|#|>Language Models act as Agents in the context of Reinforcement Learning.\\nrelation<|#|>Language Models (LMs)<|#|>Completion Tokens<|#|>treatment, action<|#|>Language Models treat completion tokens as actions.\\nrelation<|#|>State<|#|>Prompt/Task (x)<|#|>composition, context<|#|>The State is formed by the prompt and generated tokens.\\nrelation<|#|>State<|#|>Completion Tokens<|#|>composition, context<|#|>The State is formed by completion tokens concatenated with the context.\\nrelation<|#|>Reward<|#|>Response<|#|>assignment, evaluation<|#|>Rewards are typically assigned at the level of the entire response.\\nrelation<|#|>Markov Decision Process (MDP)<|#|>Sutton et al., 1998<|#|>definition, origin<|#|>The Markov Decision Process is defined by Sutton et al., 1998.\\nrelation<|#|>Markov Decision Process (MDP)<|#|>State Space (S)<|#|>component, definition<|#|>The Markov Decision Process includes a State Space (S).\\nrelation<|#|>Markov Decision Process (MDP)<|#|>Action Space (A)<|#|>component, definition<|#|>The Markov Decision Process includes an Action Space (A).\\nrelation<|#|>Markov Decision Process (MDP)<|#|>Transition Dynamics (P)<|#|>component, definition<|#|>The Markov Decision Process includes Transition Dynamics (P).\\nrelation<|#|>Markov Decision Process (MDP)<|#|>Reward Function (R)<|#|>component, definition<|#|>The Markov Decision Process includes a Reward Function (R).\\nrelation<|#|>Markov Decision Process (MDP)<|#|>Discount Factor ()<|#|>component, definition<|#|>The Markov Decision Process includes a Discount Factor ().\\nrelation<|#|>Agent<|#|>State<|#|>observes, interaction<|#|>The Agent observes a state.\\nrelation<|#|>Agent<|#|>Policy ()<|#|>guides, behavior<|#|>The Agent selects an action according to its Policy.\\nrelation<|#|>Policy ()<|#|>Language Models (LMs)<|#|>represents, implementation<|#|>The Policy represents the Language Model.\\nrelation<|#|>Policy ()<|#|>Sequence (y)<|#|>generates, output<|#|>The Policy generates a sequence.\\nrelation<|#|>Prompt/Task (x)<|#|>Data Distribution (D)<|#|>source, context<|#|>The Prompt/Task is drawn from a data distribution.\\nrelation<|#|>State<|#|>Prompt/Task (x)<|#|>composition, context<|#|>The State is defined as the prompt together with generated tokens.\\nrelation<|#|>State<|#|>Token (a  V)<|#|>composition, context<|#|>The State is defined as the prompt together with generated tokens.\\nrelation<|#|>Action<|#|>Sequence (y)<|#|>granularity, type<|#|>An Action may be an entire sequence.\\nrelation<|#|>Action<|#|>Token (a  V)<|#|>granularity, type<|#|>An Action may be a token.\\nrelation<|#|>Action<|#|>Segment (y(k))<|#|>granularity, type<|#|>An Action may be a segment.\\nrelation<|#|>Transition Dynamics (P)<|#|>State<|#|>determines, evolution<|#|>Transition Dynamics determine the next state based on the current state and action.\\nrelation<|#|>EOS Token<|#|>Trajectory<|#|>termination, signal<|#|>An EOS Token in the state causes the trajectory to end.\\nrelation<|#|>Reward<|#|>Action<|#|>assignment, granularity<|#|>Reward is assigned based on action granularity.\\nrelation<|#|>Reward<|#|>Sequence (y)<|#|>level, assignment<|#|>Reward can be sequence-level.\\nrelation<|#|>Reward<|#|>Token (a  V)<|#|>level, assignment<|#|>Reward can be token-level.\\nrelation<|#|>Reward<|#|>Segment (y(k))<|#|>level, assignment<|#|>Reward can be step-level.\\nrelation<|#|>Return (G)<|#|>Trajectory<|#|>measurement, aggregation<|#|>Return is the cumulative reward of the whole trajectory.\\nrelation<|#|>Learning Objective<|#|>Return (G)<|#|>maximization, goal<|#|>The Learning Objective is to maximize the expected cumulative Return.\\nrelation<|#|>Learning Objective<|#|>Data Distribution (D)<|#|>scope, optimization<|#|>The Learning Objective is maximized over the data distribution.\\n<|COMPLETE|>\"}]",
    "queryparam": null,
    "create_time": 1765218906,
    "update_time": 1765218906,
    "_id": "default:extract:27207a5489651209b57b4b2acaf0f53e"
  },
  "default:extract:9df1ccc00597d55004e4075bda304367": {
    "return": "entity<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>content<|#|>The title of a survey paper discussing the application of Reinforcement Learning to Large Reasoning Models.\nentity<|#|>Reinforcement Learning<|#|>concept<|#|>A learning paradigm that has progressively expanded the frontier of reasoning models and their applications.\nentity<|#|>Large Reasoning Models<|#|>concept<|#|>State-of-the-art models trained with RL-like methods, organized chronologically along three major directions: LRMs, agentic LRMs, and multimodal LRMs.\nentity<|#|>Reference Policy<|#|>concept<|#|>A policy towards which a learned policy is regularized to stabilize training and maintain language quality.\nentity<|#|>KL-Divergence Constraints<|#|>method<|#|>Constraints implemented to regularize a learned policy towards a reference policy.\nentity<|#|>Frontier Models<|#|>concept<|#|>A term referring to state-of-the-art large reasoning models.\nentity<|#|>OpenAI<|#|>organization<|#|>A company that developed the o1 series, o3 series, gpt-oss-120b, and GPT5 models.\nentity<|#|>o1 Series<|#|>artifact<|#|>The first large reasoning models from OpenAI, establishing the effectiveness of scaling both train-time RL and test-time compute towards more powerful reasoning abilities.\nentity<|#|>Jaechetal.,2024<|#|>data<|#|>A citation associated with the o1 series.\nentity<|#|>Scaling Train-Time RL<|#|>concept<|#|>The effectiveness of increasing Reinforcement Learning during the training phase of models.\nentity<|#|>Scaling Test-Time Compute<|#|>concept<|#|>The effectiveness of increasing computational resources during the testing phase of models.\nentity<|#|>Mathematics Benchmarks<|#|>data<|#|>Benchmarks used to evaluate the performance of reasoning models in mathematics.\nentity<|#|>Coding Benchmarks<|#|>data<|#|>Benchmarks used to evaluate the performance of reasoning models in coding.\nentity<|#|>Science Benchmarks<|#|>data<|#|>Benchmarks used to evaluate the performance of reasoning models in science.\nentity<|#|>DeepSeek<|#|>organization<|#|>A company that developed the R1 and DeepSeek-V3.1 models.\nentity<|#|>R1<|#|>artifact<|#|>DeepSeeks flagship model, the first open-source model to match o1s performance across benchmarks, employing a multi-stage training pipeline and exploring Zero RL.\nentity<|#|>Guo et al., 2025a<|#|>data<|#|>A citation associated with the R1 model.\nentity<|#|>Multi-Stage Training Pipeline<|#|>method<|#|>A training approach employed by R1 to ensure well-rounded model abilities.\nentity<|#|>Zero RL<|#|>method<|#|>A training approach that uses pure Reinforcement Learning without supervised finetuning.\nentity<|#|>Claude-3.7-Sonnet<|#|>artifact<|#|>A proprietary model from Anthropic featuring hybrid reasoning.\nentity<|#|>Anthropic<|#|>organization<|#|>A company that developed the Claude series, Claude-3.7-Sonnet, and Claude-4.1-Opus models.\nentity<|#|>Anthropic, 2025a<|#|>data<|#|>A citation associated with Claude-3.7-Sonnet.\nentity<|#|>Hybrid Reasoning<|#|>concept<|#|>A feature of Claude-3.7-Sonnet.\nentity<|#|>Gemini 2.0<|#|>artifact<|#|>A proprietary model that introduced longer context lengths.\nentity<|#|>Gemini 2.5<|#|>artifact<|#|>A proprietary model that introduced longer context lengths and notably emphasized strong performance across text, images, video, and audio.\nentity<|#|>Comanici et al., 2025<|#|>data<|#|>A citation associated with Gemini 2.0 and Gemini 2.5.\nentity<|#|>Longer Context Lengths<|#|>concept<|#|>An introduced feature in Gemini 2.0 and 2.5 models.\nentity<|#|>Seed-Thinking 1.5<|#|>artifact<|#|>A proprietary model featuring generalization across domains.\nentity<|#|>Seed et al., 2025b<|#|>data<|#|>A citation associated with Seed-Thinking 1.5.\nentity<|#|>Generalization Across Domains<|#|>concept<|#|>A feature of Seed-Thinking 1.5.\nentity<|#|>o3 Series<|#|>artifact<|#|>A series of models from OpenAI showcasing increasingly advanced reasoning abilities.\nentity<|#|>OpenAI, 2025b<|#|>data<|#|>A citation associated with the o3 series.\nentity<|#|>Advanced Reasoning Abilities<|#|>concept<|#|>A characteristic showcased by the o3 series.\nentity<|#|>gpt-oss-120b<|#|>artifact<|#|>OpenAI's first open-source reasoning model.\nentity<|#|>Agarwal et al., 2025a<|#|>data<|#|>A citation associated with gpt-oss-120b.\nentity<|#|>GPT5<|#|>artifact<|#|>OpenAI's most capable AI system to date, which flexibly switches between an efficient model and a deeper reasoning model.\nentity<|#|>OpenAI, 2025a<|#|>data<|#|>A citation associated with GPT5.\nentity<|#|>Efficient Model<|#|>concept<|#|>A type of model that GPT5 can switch to.\nentity<|#|>GPT-5 Thinking<|#|>artifact<|#|>A deeper reasoning model component within GPT5.\nentity<|#|>Qwen Team<|#|>organization<|#|>The team associated with the Qwen family of models, including QwQ-32B and the Qwen3 series.\nentity<|#|>QwQ-32B<|#|>artifact<|#|>A model within the Qwen family that matched R1s performance.\nentity<|#|>Team, 2025g<|#|>data<|#|>A citation associated with QwQ-32B.\nentity<|#|>Qwen3 Series<|#|>artifact<|#|>A series of models from the Qwen family, with Qwen3-235B as a representative model.\nentity<|#|>Yang et al., 2025a<|#|>data<|#|>A citation associated with the Qwen3 series.\nentity<|#|>Qwen3-235B<|#|>artifact<|#|>A representative model from the Qwen3 series that further improved benchmark scores.\nentity<|#|>Skywork-OR1<|#|>artifact<|#|>A suite of models based on R1-distilled models, achieving scalable RL training through effective data mixtures and algorithmic innovations.\nentity<|#|>He et al., 2025d<|#|>data<|#|>A citation associated with Skywork-OR1.\nentity<|#|>R1-Distilled Models<|#|>concept<|#|>Models that Skywork-OR1 is based on.\nentity<|#|>Scalable RL Training<|#|>concept<|#|>A characteristic achieved by Skywork-OR1 through effective data mixtures and algorithmic innovations.\nentity<|#|>Minimax-M1<|#|>artifact<|#|>The first model to introduce hybrid attention to scale RL efficiently.\nentity<|#|>Chen et al., 2025a<|#|>data<|#|>A citation associated with Minimax-M1.\nentity<|#|>Hybrid Attention<|#|>method<|#|>A technique introduced by Minimax-M1 to scale Reinforcement Learning efficiently.\nentity<|#|>Llama-Nemotron-Ultra<|#|>artifact<|#|>A model that aimed to balance accuracy and efficiency.\nentity<#|>Bercovich et al., 2025<|#|>data<|#|>A citation associated with Llama-Nemotron-Ultra.\nentity<|#|>Accuracy<|#|>concept<|#|>A quality that Llama-Nemotron-Ultra aimed to balance with efficiency.\nentity<|#|>Efficiency<|#|>concept<|#|>A quality that Llama-Nemotron-Ultra aimed to balance with accuracy.\nentity<|#|>Magistral24B<|#|>artifact<|#|>A model trained through Reinforcement Learning from scratch without distillation from prior models.\nentity<|#|>Rastogi et al., 2025<|#|>data<|#|>A citation associated with Magistral24B.\nentity<|#|>Reinforcement Learning from Scratch<|#|>method<|#|>A training approach used for Magistral24B.\nentity<|#|>Distillation from Prior Models<|#|>method<|#|>A training approach that Magistral24B was trained without.\nentity<|#|>Seed-OSS<|#|>artifact<|#|>A model emphasizing long-context reasoning abilities.\nentity<|#|>Team, 2025a<|#|>data<|#|>A citation associated with Seed-OSS.\nentity<|#|>Long-Context Reasoning Abilities<|#|>concept<|#|>A characteristic emphasized by Seed-OSS.\nentity<|#|>Claude Series<|#|>artifact<|#|>A family of models from Anthropic known for their leading performance on agentic coding tasks.\nentity<|#|>Claude-4.1-Opus<|#|>artifact<|#|>A model from the Claude series that further pushed forward the state-of-the-art results on SWE-bench.\nentity<|#|>Anthropic, 2025b<|#|>data<|#|>A citation associated with Claude-4.1-Opus.\nentity<|#|>SWE-bench<|#|>data<|#|>A benchmark used to evaluate performance on agentic coding tasks.\nentity<|#|>Jimenez et al., 2023<|#|>data<|#|>A citation associated with SWE-bench.\nentity<|#|>Kimi K2<|#|>artifact<|#|>A recent representative agentic model specifically optimized for agentic tasks, forging large-scale agentic training data synthesis and a general RL procedure.\nentity<|#|>Team, 2025d<|#|>data<|#|>A citation associated with Kimi K2 and Kimi 1.5.\nentity<|#|>Agentic Tasks<|#|>concept<|#|>Tasks for which Kimi K2, GLM4.5, and DeepSeek-V3.1 are optimized or emphasized.\nentity<|#|>Large-Scale Agentic Training Data Synthesis<|#|>method<|#|>A technique used by Kimi K2.\nentity<|#|>General RL Procedure<|#|>method<|#|>A procedure used by Kimi K2 that accommodates non-verifiable rewards.\nentity<|#|>GLM4.5<|#|>artifact<|#|>A model that emphasized tool-use and agentic tasks.\nentity<|#|>Zeng et al., 2025a<|#|>data<|#|>A citation associated with GLM4.5.\nentity<|#|>DeepSeek-V3.1<|#|>artifact<|#|>A model that emphasized tool-use and agentic tasks.\nentity<|#|>Tool-Use<|#|>concept<|#|>A capability emphasized by GLM4.5 and DeepSeek-V3.1.\nentity<|#|>Multimodality<|#|>concept<|#|>A key component behind the widespread adoption of reasoning models, involving processing various data types.\nentity<|#|>Widespread Adoption of Reasoning Models<|#|>concept<|#|>An outcome facilitated by multimodality.\nentity<|#|>Gemini Family<|#|>artifact<|#|>A family of proprietary models, including Gemini 2.0 and 2.5, that are natively multimodal.\nentity<|#|>Text Domain<|#|>concept<|#|>A domain of performance emphasized by Gemini 2.5.\nentity<|#|>Images Domain<|#|>concept<|#|>A domain of performance emphasized by Gemini 2.5.\nentity<|#|>Video Domain<|#|>concept<|#|>A domain of performance emphasized by Gemini 2.5.\nentity<|#|>Audio Domain<|#|>concept<|#|>A domain of performance emphasized by Gemini 2.5.\nentity<|#|>Kimi 1.5<|#|>artifact<|#|>An early open-source effort towards multimodal reasoning, highlighting long context scaling and joint reasoning over text and vision domains.\nentity<|#|>Long Context Scaling<|#|>concept<|#|>A characteristic highlighted by Kimi 1.5.\nentity<#|>Joint Reasoning Over Text and Vision Domains<|#|>concept<|#|>A characteristic highlighted by Kimi 1.5.\nentity<|#|>QVQ<|#|>artifact<|#|>A model that excels in visual reasoning and analytical thinking.\nentity<|#|>Qwen Team, 2025<|#|>data<|#|>A citation associated with QVQ.\nentity<|#|>Visual Reasoning<|#|>concept<|#|>A capability in which QVQ excels.\nentity<|#|>Analytical Thinking<|#|>concept<|#|>A capability in which QVQ excels.\nentity<|#|>Skywork R1V2<|#|>artifact<|#|>A model that balances reasoning and general abilities through hybrid RL, using MPO and GRPO.\nentity<|#|>Wang et al., 2025k<|#|>data<|#|>A citation associated with Skywork R1V2.\nentity<|#|>General Abilities<|#|>concept<|#|>A characteristic balanced by Skywork R1V2.\nentity<|#|>Hybrid RL<|#|>method<|#|>A method used by Skywork R1V2 to balance reasoning and general abilities.\nentity<|#|>MPO<|#|>method<|#|>A method used in hybrid Reinforcement Learning for models like Skywork R1V2.\nentity<|#|>GRPO<|#|>method<|#|>A method used in hybrid Reinforcement Learning for models like Skywork R1V2.\nentity<|#|>InternVL Series<|#|>artifact<|#|>A series of models, including InternVL3.\nentity<|#|>InternVL3<|#|>artifact<|#|>A model from the InternVL series.\nentity<|#|>Zhu et al., 2025c<|#|>data<|#|>A citation associated with InternVL3.\nrelation<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>Reinforcement Learning<|#|>discussion, topic<|#|>The survey discusses Reinforcement Learning.\nrelation<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>Large Reasoning Models<|#|>discussion, topic<|#|>The survey discusses Large Reasoning Models.\nrelation<|#|>Reinforcement Learning<|#|>Large Reasoning Models<|#|>application, training<|#|>Reinforcement Learning is used to train Large Reasoning Models.\nrelation<|#|>Learned Policy<|#|>Reference Policy<|#|>regularization, stabilization<|#|>A learned policy is regularized towards a reference policy.\nrelation<|#|>KL-Divergence Constraints<|#|>Reference Policy<|#|>implementation, regularization<|#|>KL-Divergence Constraints are implemented to regularize towards a reference policy.\nrelation<|#|>Frontier Models<|#|>Large Reasoning Models<|#|>categorization, type<|#|>Frontier Models are a category of Large Reasoning Models.\nrelation<|#|>Frontier Models<|#|>Reinforcement Learning<|#|>training method, application<|#|>Frontier Models are trained with Reinforcement Learning-like methods.\nrelation<#|>OpenAI<|#|>o1 Series<|#|>development, creation<|#|>OpenAI developed the o1 series.\nrelation<|#|>o1 Series<|#|>Jaechetal.,2024<|#|>citation, reference<|#|>The o1 series is cited by Jaechetal.,2024.\nrelation<|#|>o1 Series<|#|>Scaling Train-Time RL<|#|>effectiveness, demonstration<|#|>The o1 series established the effectiveness of scaling train-time RL.\nrelation<|#|>o1 Series<|#|>Scaling Test-Time Compute<|#|>effectiveness, demonstration<|#|>The o1 series established the effectiveness of scaling test-time compute.\nrelation<|#|>o1 Series<|#|>Mathematics Benchmarks<|#|>achievement, performance<|#|>The o1 series achieved leading results on mathematics benchmarks.\nrelation<|#|>o1 Series<|#|>Coding Benchmarks<|#|>achievement, performance<|#|>The o1 series achieved leading results on coding benchmarks.\nrelation<|#|>o1 Series<|#|>Science Benchmarks<|#|>achievement, performance<|#|>The o1 series achieved leading results on science benchmarks.\nrelation<|#|>DeepSeek<|#|>R1<|#|>development, creation<|#|>DeepSeek developed the R1 model.\nrelation<|#|>R1<|#|>Guo et al., 2025a<|#|>citation, reference<|#|>R1 is cited by Guo et al., 2025a.\nrelation<|#|>R1<|#|>o1 Series<|#|>performance, comparison<|#|>R1 matched the performance of the o1 series.\nrelation<|#|>R1<|#|>Multi-Stage Training Pipeline<|#|>employment, method<|#|>R1 employs a multi-stage training pipeline.\nrelation<|#|>R1<|#|>Zero RL<|#|>exploration, method<|#|>R1 explores the route of Zero RL.\nrelation<|#|>Zero RL<|#|>Reinforcement Learning<|#|>type, approach<|#|>Zero RL is a form of pure Reinforcement Learning without supervised finetuning.\nrelation<|#|>Anthropic<|#|>Claude-3.7-Sonnet<|#|>development, creation<|#|>Anthropic developed Claude-3.7-Sonnet.\nrelation<|#|>Claude-3.7-Sonnet<|#|>Anthropic, 2025a<|#|>citation, reference<|#|>Claude-3.7-Sonnet is cited by Anthropic, 2025a.\nrelation<|#|>Claude-3.7-Sonnet<|#|>Hybrid Reasoning<|#|>feature, capability<|#|>Claude-3.7-Sonnet features hybrid reasoning.\nrelation<|#|>Gemini 2.0<|#|>Comanici et al., 2025<|#|>citation, reference<|#|>Gemini 2.0 is cited by Comanici et al., 2025.\nrelation<|#|>Gemini 2.5<|#|>Comanici et al., 2025<|#|>citation, reference<|#|>Gemini 2.5 is cited by Comanici et al., 2025.\nrelation<|#|>Gemini 2.0<|#|>Longer Context Lengths<|#|>introduction, feature<|#|>Gemini 2.0 introduced longer context lengths.\nrelation<|#|>Gemini 2.5<|#|>Longer Context Lengths<|#|>introduction, feature<|#|>Gemini 2.5 introduced longer context lengths.\nrelation<|#|>Seed-Thinking 1.5<|#|>Seed et al., 2025b<|#|>citation, reference<|#|>Seed-Thinking 1.5 is cited by Seed et al., 2025b.\nrelation<|#|>Seed-Thinking 1.5<|#|>Generalization Across Domains<|#|>feature, capability<|#|>Seed-Thinking 1.5 features generalization across domains.\nrelation<|#|>OpenAI<|#|>o3 Series<|#|>development, creation<|#|>OpenAI developed the o3 series.\nrelation<|#|>o3 Series<|#|>OpenAI, 2025b<|#|>citation, reference<|#|>The o3 series is cited by OpenAI, 2025b.\nrelation<|#|>o3 Series<|#|>Advanced Reasoning Abilities<|#|>showcase, capability<|#|>The o3 series showcased increasingly advanced reasoning abilities.\nrelation<|#|>OpenAI<|#|>gpt-oss-120b<|#|>introduction, development<|#|>OpenAI introduced gpt-oss-120b.\nrelation<|#|>gpt-oss-120b<|#|>Agarwal et al., 2025a<|#|>citation, reference<|#|>gpt-oss-120b is cited by Agarwal et al., 2025a.\nrelation<|#|>gpt-oss-120b<|#|>OpenAI<|#|>ownership, origin<|#|>gpt-oss-120b is OpenAI's first open-source reasoning model.\nrelation<|#|>OpenAI<|#|>GPT5<|#|>development, creation<|#|>OpenAI developed GPT5.\nrelation<|#|>GPT5<|#|>OpenAI, 2025a<|#|>citation, reference<|#|>GPT5 is cited by OpenAI, 2025a.\nrelation<|#|>GPT5<|#|>Efficient Model<|#|>switching, capability<|#|>GPT5 flexibly switches to an efficient model.\nrelation<|#|>GPT5<|#|>GPT-5 Thinking<|#|>switching, capability<|#|>GPT5 flexibly switches to GPT-5 Thinking.\nrelation<|#|>Qwen Team<|#|>QwQ-32B<|#|>development, creation<|#|>The Qwen Team developed QwQ-32B.\nrelation<|#|>QwQ-32B<|#|>Team, 2025g<|#|>citation, reference<|#|>QwQ-32B is cited by Team, 2025g.\nrelation<|#|>QwQ-32B<|#|>R1<|#|>performance, comparison<|#|>QwQ-32B matched R1's performance.\nrelation<|#|>Qwen Team<|#|>Qwen3 Series<|#|>development, creation<|#|>The Qwen Team developed the Qwen3 series.\nrelation<|#|>Qwen3 Series<|#|>Yang et al., 2025a<|#|>citation, reference<|#|>The Qwen3 series is cited by Yang et al., 2025a.\nrelation<|#|>Qwen3 Series<|#|>Qwen3-235B<|#|>inclusion, component<|#|>The Qwen3 series includes Qwen3-235B.\nrelation<|#|>Qwen3-235B<|#|>Benchmark Scores<|#|>improvement, performance<|#|>Qwen3-235B further improved benchmark scores.\nrelation<|#|>Skywork-OR1<|#|>He et al., 2025d<|#|>citation, reference<|#|>Skywork-OR1 is cited by He et al., 2025d.\nrelation<|#|>Skywork-OR1<|#|>R1-Distilled Models<|#|>basis, foundation<|#|>Skywork-OR1 models were based on R1-distilled models.\nrelation<|#|>Skywork-OR1<|#|>Scalable RL Training<|#|>achievement, method<|#|>Skywork-OR1 achieved scalable RL training.\nrelation<|#|>Minimax-M1<|#|>Chen et al., 2025a<|#|>citation, reference<|#|>Minimax-M1 is cited by Chen et al., 2025a.\nrelation<|#|>Minimax-M1<|#|>Hybrid Attention<|#|>introduction, innovation<|#|>Minimax-M1 introduced hybrid attention.\nrelation<|#|>Minimax-M1<|#|>Reinforcement Learning<|#|>scaling, efficiency<|#|>Minimax-M1 scales Reinforcement Learning efficiently.\nrelation<|#|>Llama-Nemotron-Ultra<|#|>Bercovich et al., 2025<|#|>citation, reference<|#|>Llama-Nemotron-Ultra is cited by Bercovich et al., 2025.\nrelation<|#|>Llama-Nemotron-Ultra<|#|>Accuracy<|#|>balancing, objective<|#|>Llama-Nemotron-Ultra aimed to balance accuracy.\nrelation<|#|>Llama-Nemotron-Ultra<|#|>Efficiency<|#|>balancing, objective<|#|>Llama-Nemotron-Ultra aimed to balance efficiency.\nrelation<|#|>Magistral24B<|#|>Rastogi et al., 2025<|#|>citation, reference<|#|>Magistral24B is cited by Rastogi et al., 2025.\nrelation<|#|>Magistral24B<|#|>Reinforcement Learning from Scratch<|#|>training method, approach<|#|>Magistral24B was trained through Reinforcement Learning from scratch.\nrelation<|#|>Magistral24B<|#|>Distillation from Prior Models<|#|>training method, absence<|#|>Magistral24B was trained without distillation from prior models.\nrelation<|#|>Seed-OSS<|#|>Team, 2025a<|#|>citation, reference<|#|>Seed-OSS is cited by Team, 2025a.\nrelation<|#|>Seed-OSS<|#|>Long-Context Reasoning Abilities<|#|>emphasis, feature<|#|>Seed-OSS emphasizes long-context reasoning abilities.\nrelation<|#|>Claude Series<|#|>Agentic Tasks<|#|>performance, capability<|#|>The Claude series is known for leading performance on agentic coding tasks.\nrelation<|#|>Anthropic<|#|>Claude-4.1-Opus<|#|>development, creation<|#|>Anthropic developed Claude-4.1-Opus.\nrelation<|#|>Claude-4.1-Opus<|#|>Anthropic, 2025b<|#|>citation, reference<|#|>Claude-4.1-Opus is cited by Anthropic, 2025b.\nrelation<|#|>Claude-4.1-Opus<|#|>SWE-bench<|#|>performance, benchmark<|#|>Claude-4.1-Opus pushed state-of-the-art results on SWE-bench.\nrelation<|#|>SWE-bench<|#|>Jimenez et al., 2023<|#|>citation, reference<|#|>SWE-bench is cited by Jimenez et al., 2023.\nrelation<|#|>Kimi K2<|#|>Team, 2025d<|#|>citation, reference<|#|>Kimi K2 is cited by Team, 2025d.\nrelation<|#|>Kimi K2<|#|>Agentic Tasks<|#|>optimization, focus<|#|>Kimi K2 was optimized for agentic tasks.\nrelation<|#|>Kimi K2<|#|>Large-Scale Agentic Training Data Synthesis<|#|>utilization, method<|#|>Kimi K2 uses large-scale agentic training data synthesis.\nrelation<|#|>Kimi K2<|#|>General RL Procedure<|#|>utilization, method<|#|>Kimi K2 uses a general RL procedure.\nrelation<|#|>GLM4.5<|#|>Zeng et al., 2025a<|#|>citation, reference<|#|>GLM4.5 is cited by Zeng et al., 2025a.\nrelation<|#|>GLM4.5<|#|>Tool-Use<|#|>emphasis, capability<|#|>GLM4.5 emphasized tool-use.\nrelation<|#|>GLM4.5<|#|>Agentic Tasks<|#|>emphasis, capability<|#|>GLM4.5 emphasized agentic tasks.\nrelation<|#|>DeepSeek-V3.1<|#|>Tool-Use<|#|>emphasis, capability<|#|>DeepSeek-V3.1 emphasized tool-use.\nrelation<|#|>DeepSeek-V3.1<|#|>Agentic Tasks<|#|>emphasis, capability<|#|>DeepSeek-V3.1 emphasized agentic tasks.\nrelation<|#|>Multimodality<|#|>Widespread Adoption of Reasoning Models<|#|>enabling factor, impact<|#|>Multimodality is a key component behind the widespread adoption of reasoning models.\nrelation<|#|>GPT5<|#|>Multimodality<|#|>characteristic, capability<|#|>GPT5 is natively multimodal.\nrelation<|#|>o3 Series<|#|>Multimodality<|#|>characteristic, capability<|#|>The o3 series is natively multimodal.\nrelation<|#|>Claude Series<|#|>Multimodality<|#|>characteristic, capability<|#|>The Claude series is natively multimodal.\nrelation<|#|>Gemini Family<|#|>Multimodality<|#|>characteristic, capability<|#|>The Gemini family is natively multimodal.\nrelation<|#|>Gemini 2.5<|#|>Text Domain<|#|>performance, emphasis<|#|>Gemini 2.5 emphasized strong performance across the text domain.\nrelation<|#|>Gemini 2.5<|#|>Images Domain<|#|>performance, emphasis<|#|>Gemini 2.5 emphasized strong performance across the images domain.\nrelation<|#|>Gemini 2.5<|#|>Video Domain<|#|>performance, emphasis<|#|>Gemini 2.5 emphasized strong performance across the video domain.\nrelation<|#|>Gemini 2.5<|#|>Audio Domain<|#|>performance, emphasis<|#|>Gemini 2.5 emphasized strong performance across the audio domain.\nrelation<|#|>Kimi 1.5<|#|>Team, 2025d<|#|>citation, reference<|#|>Kimi 1.5 is cited by Team, 2025d.\nrelation<|#|>Kimi 1.5<|#|>Multimodal Reasoning<|#|>effort, focus<|#|>Kimi 1.5 represents an early effort towards multimodal reasoning.\nrelation<|#|>Kimi 1.5<|#|>Long Context Scaling<|#|>highlight, feature<|#|>Kimi 1.5 highlights long context scaling.\nrelation<|#|>Kimi 1.5<|#|>Joint Reasoning Over Text and Vision Domains<|#|>highlight, feature<|#|>Kimi 1.5 highlights joint reasoning over text and vision domains.\nrelation<|#|>QVQ<|#|>Qwen Team, 2025<|#|>citation, reference<|#|>QVQ is cited by Qwen Team, 2025.\nrelation<|#|>QVQ<|#|>Visual Reasoning<|#|>excellence, capability<|#|>QVQ excels in visual reasoning.\nrelation<|#|>QVQ<|#|>Analytical Thinking<|#|>excellence, capability<|#|>QVQ excels in analytical thinking.\nrelation<|#|>Skywork R1V2<|#|>Wang et al., 2025k<|#|>citation, reference<|#|>Skywork R1V2 is cited by Wang et al., 2025k.\nrelation<|#|>Skywork R1V2<|#|>Reinforcement Learning<|#|>training method, balance<|#|>Skywork R1V2 balances reasoning and general abilities through hybrid RL.\nrelation<|#|>Skywork R1V2<|#|>General Abilities<|#|>balance, characteristic<|#|>Skywork R1V2 balances general abilities.\nrelation<|#|>Skywork R1V2<|#|>Hybrid RL<|#|>utilization, method<|#|>Skywork R1V2 uses hybrid RL.\nrelation<|#|>Hybrid RL<|#|>MPO<|#|>component, method<|#|>Hybrid RL uses MPO.\nrelation<|#|>Hybrid RL<|#|>GRPO<|#|>component, method<|#|>Hybrid RL uses GRPO.\nrelation<|#|>InternVL Series<|#|>InternVL3<|#|>inclusion, component<|#|>The InternVL series includes InternVL3.\nrelation<|#|>InternVL3<|#|>Zhu et al., 2025c<|#|>citation, reference<|#|>InternVL3 is cited by Zhu et al., 2025c.\n<|COMPLETE|>",
    "cache_type": "extract",
    "chunk_id": "chunk-37f12102e8df23d0bf48e221b1e75d9e",
    "original_prompt": "---Task---\nExtract entities and relationships from the input text to be processed.\n\n---Instructions---\n1.  **Strict Adherence to Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system prompt.\n2.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\n3.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant entities and relationships have been extracted and presented.\n4.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\n\n<Output>\n---Role---\nYou are a Knowledge Graph Specialist responsible for extracting entities and relationships from the input text.\n\n---Instructions---\n1.  **Entity Extraction & Output:**\n    *   **Identification:** Identify clearly defined and meaningful entities in the input text.\n    *   **Entity Details:** For each identified entity, extract the following information:\n        *   `entity_name`: The name of the entity. If the entity name is case-insensitive, capitalize the first letter of each significant word (title case). Ensure **consistent naming** across the entire extraction process.\n        *   `entity_type`: Categorize the entity using one of the following types: `Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject`. If none of the provided entity types apply, do not add new entity type and classify it as `Other`.\n        *   `entity_description`: Provide a concise yet comprehensive description of the entity's attributes and activities, based *solely* on the information present in the input text.\n    *   **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n        *   Format: `entity<|#|>entity_name<|#|>entity_type<|#|>entity_description`\n\n2.  **Relationship Extraction & Output:**\n    *   **Identification:** Identify direct, clearly stated, and meaningful relationships between previously extracted entities.\n    *   **N-ary Relationship Decomposition:** If a single statement describes a relationship involving more than two entities (an N-ary relationship), decompose it into multiple binary (two-entity) relationship pairs for separate description.\n        *   **Example:** For \"Alice, Bob, and Carol collaborated on Project X,\" extract binary relationships such as \"Alice collaborated with Project X,\" \"Bob collaborated with Project X,\" and \"Carol collaborated with Project X,\" or \"Alice collaborated with Bob,\" based on the most reasonable binary interpretations.\n    *   **Relationship Details:** For each binary relationship, extract the following fields:\n        *   `source_entity`: The name of the source entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `target_entity`: The name of the target entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `relationship_keywords`: One or more high-level keywords summarizing the overarching nature, concepts, or themes of the relationship. Multiple keywords within this field must be separated by a comma `,`. **DO NOT use `<|#|>` for separating multiple keywords within this field.**\n        *   `relationship_description`: A concise explanation of the nature of the relationship between the source and target entities, providing a clear rationale for their connection.\n    *   **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n        *   Format: `relation<|#|>source_entity<|#|>target_entity<|#|>relationship_keywords<|#|>relationship_description`\n\n3.  **Delimiter Usage Protocol:**\n    *   The `<|#|>` is a complete, atomic marker and **must not be filled with content**. It serves strictly as a field separator.\n    *   **Incorrect Example:** `entity<|#|>Tokyo<|location|>Tokyo is the capital of Japan.`\n    *   **Correct Example:** `entity<|#|>Tokyo<|#|>location<|#|>Tokyo is the capital of Japan.`\n\n4.  **Relationship Direction & Duplication:**\n    *   Treat all relationships as **undirected** unless explicitly stated otherwise. Swapping the source and target entities for an undirected relationship does not constitute a new relationship.\n    *   Avoid outputting duplicate relationships.\n\n5.  **Output Order & Prioritization:**\n    *   Output all extracted entities first, followed by all extracted relationships.\n    *   Within the list of relationships, prioritize and output those relationships that are **most significant** to the core meaning of the input text first.\n\n6.  **Context & Objectivity:**\n    *   Ensure all entity names and descriptions are written in the **third person**.\n    *   Explicitly name the subject or object; **avoid using pronouns** such as `this article`, `this paper`, `our company`, `I`, `you`, and `he/she`.\n\n7.  **Language & Proper Nouns:**\n    *   The entire output (entity names, keywords, and descriptions) must be written in `English`.\n    *   Proper nouns (e.g., personal names, place names, organization names) should be retained in their original language if a proper, widely accepted translation is not available or would cause ambiguity.\n\n8.  **Completion Signal:** Output the literal string `<|COMPLETE|>` only after all entities and relationships, following all criteria, have been completely extracted and outputted.\n\n---Examples---\n<Input Text>\n```\nwhile Alex clenched his jaw, the buzz of frustration dull against the backdrop of Taylor's authoritarian certainty. It was this competitive undercurrent that kept him alert, the sense that his and Jordan's shared commitment to discovery was an unspoken rebellion against Cruz's narrowing vision of control and order.\n\nThen Taylor did something unexpected. They paused beside Jordan and, for a moment, observed the device with something akin to reverence. \"If this tech can be understood...\" Taylor said, their voice quieter, \"It could change the game for us. For all of us.\"\n\nThe underlying dismissal earlier seemed to falter, replaced by a glimpse of reluctant respect for the gravity of what lay in their hands. Jordan looked up, and for a fleeting heartbeat, their eyes locked with Taylor's, a wordless clash of wills softening into an uneasy truce.\n\nIt was a small transformation, barely perceptible, but one that Alex noted with an inward nod. They had all been brought here by different paths\n```\n\n<Output>\nentity<|#|>Alex<|#|>person<|#|>Alex is a character who experiences frustration and is observant of the dynamics among other characters.\nentity<|#|>Taylor<|#|>person<|#|>Taylor is portrayed with authoritarian certainty and shows a moment of reverence towards a device, indicating a change in perspective.\nentity<|#|>Jordan<|#|>person<|#|>Jordan shares a commitment to discovery and has a significant interaction with Taylor regarding a device.\nentity<|#|>Cruz<|#|>person<|#|>Cruz is associated with a vision of control and order, influencing the dynamics among other characters.\nentity<|#|>The Device<|#|>equipment<|#|>The Device is central to the story, with potential game-changing implications, and is revered by Taylor.\nrelation<|#|>Alex<|#|>Taylor<|#|>power dynamics, observation<|#|>Alex observes Taylor's authoritarian behavior and notes changes in Taylor's attitude toward the device.\nrelation<|#|>Alex<|#|>Jordan<|#|>shared goals, rebellion<|#|>Alex and Jordan share a commitment to discovery, which contrasts with Cruz's vision.)\nrelation<|#|>Taylor<|#|>Jordan<|#|>conflict resolution, mutual respect<|#|>Taylor and Jordan interact directly regarding the device, leading to a moment of mutual respect and an uneasy truce.\nrelation<|#|>Jordan<|#|>Cruz<|#|>ideological conflict, rebellion<|#|>Jordan's commitment to discovery is in rebellion against Cruz's vision of control and order.\nrelation<|#|>Taylor<|#|>The Device<|#|>reverence, technological significance<|#|>Taylor shows reverence towards the device, indicating its importance and potential impact.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nStock markets faced a sharp downturn today as tech giants saw significant declines, with the global tech index dropping by 3.4% in midday trading. Analysts attribute the selloff to investor concerns over rising interest rates and regulatory uncertainty.\n\nAmong the hardest hit, nexon technologies saw its stock plummet by 7.8% after reporting lower-than-expected quarterly earnings. In contrast, Omega Energy posted a modest 2.1% gain, driven by rising oil prices.\n\nMeanwhile, commodity markets reflected a mixed sentiment. Gold futures rose by 1.5%, reaching $2,080 per ounce, as investors sought safe-haven assets. Crude oil prices continued their rally, climbing to $87.60 per barrel, supported by supply constraints and strong demand.\n\nFinancial experts are closely watching the Federal Reserve's next move, as speculation grows over potential rate hikes. The upcoming policy announcement is expected to influence investor confidence and overall market stability.\n```\n\n<Output>\nentity<|#|>Global Tech Index<|#|>category<|#|>The Global Tech Index tracks the performance of major technology stocks and experienced a 3.4% decline today.\nentity<|#|>Nexon Technologies<|#|>organization<|#|>Nexon Technologies is a tech company that saw its stock decline by 7.8% after disappointing earnings.\nentity<|#|>Omega Energy<|#|>organization<|#|>Omega Energy is an energy company that gained 2.1% in stock value due to rising oil prices.\nentity<|#|>Gold Futures<|#|>product<|#|>Gold futures rose by 1.5%, indicating increased investor interest in safe-haven assets.\nentity<|#|>Crude Oil<|#|>product<|#|>Crude oil prices rose to $87.60 per barrel due to supply constraints and strong demand.\nentity<|#|>Market Selloff<|#|>category<|#|>Market selloff refers to the significant decline in stock values due to investor concerns over interest rates and regulations.\nentity<|#|>Federal Reserve Policy Announcement<|#|>category<|#|>The Federal Reserve's upcoming policy announcement is expected to impact investor confidence and market stability.\nentity<|#|>3.4% Decline<|#|>category<|#|>The Global Tech Index experienced a 3.4% decline in midday trading.\nrelation<|#|>Global Tech Index<|#|>Market Selloff<|#|>market performance, investor sentiment<|#|>The decline in the Global Tech Index is part of the broader market selloff driven by investor concerns.\nrelation<|#|>Nexon Technologies<|#|>Global Tech Index<|#|>company impact, index movement<|#|>Nexon Technologies' stock decline contributed to the overall drop in the Global Tech Index.\nrelation<|#|>Gold Futures<|#|>Market Selloff<|#|>market reaction, safe-haven investment<|#|>Gold prices rose as investors sought safe-haven assets during the market selloff.\nrelation<|#|>Federal Reserve Policy Announcement<|#|>Market Selloff<|#|>interest rate impact, financial regulation<|#|>Speculation over Federal Reserve policy changes contributed to market volatility and investor selloff.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nAt the World Athletics Championship in Tokyo, Noah Carter broke the 100m sprint record using cutting-edge carbon-fiber spikes.\n```\n\n<Output>\nentity<|#|>World Athletics Championship<|#|>event<|#|>The World Athletics Championship is a global sports competition featuring top athletes in track and field.\nentity<|#|>Tokyo<|#|>location<|#|>Tokyo is the host city of the World Athletics Championship.\nentity<|#|>Noah Carter<|#|>person<|#|>Noah Carter is a sprinter who set a new record in the 100m sprint at the World Athletics Championship.\nentity<|#|>100m Sprint Record<|#|>category<|#|>The 100m sprint record is a benchmark in athletics, recently broken by Noah Carter.\nentity<|#|>Carbon-Fiber Spikes<|#|>equipment<|#|>Carbon-fiber spikes are advanced sprinting shoes that provide enhanced speed and traction.\nentity<|#|>World Athletics Federation<|#|>organization<|#|>The World Athletics Federation is the governing body overseeing the World Athletics Championship and record validations.\nrelation<|#|>World Athletics Championship<|#|>Tokyo<|#|>event location, international competition<|#|>The World Athletics Championship is being hosted in Tokyo.\nrelation<|#|>Noah Carter<|#|>100m Sprint Record<|#|>athlete achievement, record-breaking<|#|>Noah Carter set a new 100m sprint record at the championship.\nrelation<|#|>Noah Carter<|#|>Carbon-Fiber Spikes<|#|>athletic equipment, performance boost<|#|>Noah Carter used carbon-fiber spikes to enhance performance during the race.\nrelation<|#|>Noah Carter<|#|>World Athletics Championship<|#|>athlete participation, competition<|#|>Noah Carter is competing at the World Athletics Championship.\n<|COMPLETE|>\n\n\n\n---Real Data to be Processed---\n<Input>\nEntity_types: [Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject]\nText:\n```\nASurveyofReinforcementLearningforLargeReasoningModels\nIn practice, it is common to regularize the learned policy towards a reference policy  , often\nref\nimplemented as KL-divergence constraints to stabilize training and maintain language quality. In the\nfollowing sections, we present various algorithms that build upon this fundamental formulation.\n2.2. Frontier Models\nIn this subsection, we provide an overview of state-of-the-art large reasoning models trained with\nRL-like methods, organized roughly chronologically along three major directions: LRMs, agentic\nLRMs, and multimodal LRMs.\nOver the past year, RL has progressively expanded the frontier of reasoning models and their\napplications. Thefirstlargereasoningmodels,OpenAIso1[Jaechetal.,2024]series,establishedthe\neffectiveness of scaling both train-time RL and test-time compute towards more powerful reasoning\nabilities, achieving leading results on mathematics, coding, and science benchmarks. DeepSeeks\nflagship model R1 [Guo et al., 2025a] followed as the first open-source model to match o1s per-\nformance across benchmarks. It employs a multi-stage training pipeline to ensure well-rounded\nmodel abilities, and explores the route of pure RL without supervised finetuning (i.e., Zero RL).\nOther proprietary model releases promptly followed: Claude-3.7-Sonnet [Anthropic, 2025a] featured\nhybrid reasoning, Gemini 2.0 and 2.5 [Comanici et al., 2025] introduced longer context lengths,\nSeed-Thinking 1.5 [Seed et al., 2025b] featured generalization across domains, and the o3 [OpenAI,\n2025b]seriesshowcasedincreasinglyadvancedreasoningabilities. Recently,OpenAIintroducedtheir\nfirstopen-sourcereasoningmodelgpt-oss-120b[Agarwaletal.,2025a],andsubsequentlyGPT5[Ope-\nnAI,2025a],theirmostcapableAIsystemtodate,whichflexiblyswitchesbetweenanefficientmodel\nand a deeper reasoning model GPT-5 thinking. Parallel open-source efforts continued to expand\nthe landscape. Within the Qwen family, QwQ-32B [Team, 2025g] matched R1s performance, and\nwas followed by the Qwen3 [Yang et al., 2025a] series, with the representative model Qwen3-235B\nfurther improving benchmark scores. The Skywork-OR1 [He et al., 2025d] suite of models were\nbased on R1-distilled models, and achieved scalable RL training through effective data mixtures and\nalgorithmic innovations. Minimax-M1 [Chen et al., 2025a] was the first model to introduce hybrid\nattention to scale RL efficiently. Other works include Llama-Nemotron-Ultra [Bercovich et al., 2025],\nwhichaimedtobalanceaccuracyandefficiency;Magistral24B[Rastogietal.,2025],trainedthrough\nRL from scratch without distillation from prior models; and Seed-OSS [Team, 2025a], emphasizing\nlong-context reasoning abilities.\nModel reasoning improvements have in turn extended their use cases in coding and agentic\nscenarios. The Claude series has been known for their leading performance on agentic coding tasks,\nand this was exemplified by Claude-4.1-Opus [Anthropic, 2025b], which further pushed forward\nthe state-of-the-art results on SWE-bench [Jimenez et al., 2023]. Kimi K2 [Team, 2025d] is a recent\nrepresentative agentic model which was specifically optimized for agentic tasks, forging large-scale\nagentictrainingdatasynthesisandageneralRLprocedurethataccommodatesnon-verifiablerewards.\nShortly after, both the GLM4.5 [Zeng et al., 2025a] and DeepSeek-V3.1 releases emphasized tool-use\nand agentic tasks, showing substantial improvements on relevant benchmarks.\nMultimodality is a key component behind the widespread adoption of reasoning models. Most\nfrontierproprietarymodels,includingGPT-5,o3,Claude,andGeminifamilies,arenativelymultimodal.\nGemini-2.5[Comanicietal.,2025]notablyemphasizedstrongperformanceacrosstext,images,video,\nand audio. On the open-source side, Kimi 1.5 [Team, 2025d] represents an early effort towards\nmultimodalreasoning,highlightinglongcontextscalingaswellasjointreasoningovertextandvision\ndomains. QVQ [Qwen Team, 2025] excels in visual reasoning and analytical thinking, while Skywork\nR1V2 [Wang et al., 2025k] balances reasoning and general abilities through hybrid RL, using both\nMPO and GRPO. As notable additions to the InternVL series, InternVL3 [Zhu et al., 2025c] adopted\n7\n```",
    "queryparam": null,
    "create_time": 1765218995,
    "update_time": 1765218995,
    "_id": "default:extract:9df1ccc00597d55004e4075bda304367"
  },
  "default:extract:8895c85dfd34ade3418b18508baf5ab8": {
    "return": "entity<|#|>Learned Policy<|#|>concept<|#|>A policy that is regularized towards a reference policy to stabilize training and maintain language quality.\nentity<|#|>Benchmark Scores<|#|>data<|#|>Scores obtained on benchmarks, which Qwen3-235B further improved.\nentity<|#|>LRMs<|#|>concept<|#|>An abbreviation for Large Reasoning Models, representing one of three major directions in their development.\nentity<|#|>Agentic LRMs<|#|>concept<|#|>A category of Large Reasoning Models specifically designed for agentic scenarios and tasks.\nentity<|#|>Multimodal LRMs<|#|>concept<|#|>A category of Large Reasoning Models that possess multimodal capabilities, processing various data types.\nentity<|#|>Language Quality<|#|>concept<|#|>A characteristic of language output that KL-divergence constraints help to maintain during model training.\nentity<|#|>Training<|#|>concept<|#|>The process of developing and refining models, which KL-divergence constraints help to stabilize.\nentity<|#|>Algorithmic Innovations<|#|>concept<|#|>New developments in algorithms that contributed to scalable RL training for models like Skywork-OR1.\nentity<|#|>Data Mixtures<|#|>concept<|#|>Effective combinations of data used by Skywork-OR1 to achieve scalable RL training.\nentity<|#|>Reasoning Abilities<|#|>concept<|#|>The general capacity of models to perform reasoning tasks, which the o1 series aimed to make more powerful.\nentity<|#|>Non-Verifiable Rewards<|#|>concept<|#|>A type of reward that Kimi K2's general Reinforcement Learning procedure is designed to accommodate.\nentity<|#|>AI System<|#|>concept<|#|>A general term for advanced artificial intelligence, of which GPT5 is described as a capable example.\nentity<|#|>Qwen Family<|#|>category<|#|>A family of open-source models developed by the Qwen Team, including QwQ-32B and the Qwen3 series.\nrelation<|#|>Learned Policy<|#|>Reference Policy<|#|>regularization, stabilization<|#|>A learned policy is regularized towards a reference policy to stabilize training.\nrelation<|#|>KL-Divergence Constraints<|#|>Training<|#|>stabilization, impact<|#|>KL-divergence constraints stabilize training.\nrelation<|#|>KL-Divergence Constraints<|#|>Language Quality<|#|>maintenance, impact<|#|>KL-divergence constraints maintain language quality.\nrelation<|#|>Large Reasoning Models<|#|>LRMs<|#|>categorization, direction<|#|>LRMs represent one of the major directions for Large Reasoning Models.\nrelation<|#|>Large Reasoning Models<|#|>Agentic LRMs<|#|>categorization, direction<|#|>Agentic LRMs represent one of the major directions for Large Reasoning Models.\nrelation<|#|>Large Reasoning Models<|#|>Multimodal LRMs<|#|>categorization, direction<|#|>Multimodal LRMs represent one of the major directions for Large Reasoning Models.\nrelation<|#|>o1 Series<|#|>Reasoning Abilities<|#|>enhancement, capability<|#|>The o1 series established the effectiveness of scaling towards more powerful reasoning abilities.\nrelation<|#|>Skywork-OR1<|#|>Data Mixtures<|#|>method, utilization<|#|>Skywork-OR1 achieved scalable RL training through effective data mixtures.\nrelation<|#|>Skywork-OR1<|#|>Algorithmic Innovations<|#|>method, utilization<|#|>Skywork-OR1 achieved scalable RL training through algorithmic innovations.\nrelation<|#|>Kimi K2<|#|>Non-Verifiable Rewards<|#|>accommodation, method<|#|>Kimi K2's general RL procedure accommodates non-verifiable rewards.\nrelation<|#|>GPT5<|#|>AI System<|#|>categorization, type<|#|>GPT5 is described as OpenAI's most capable AI system.\nrelation<|#|>OpenAI<|#|>AI System<|#|>development, ownership<|#|>OpenAI developed GPT5, their most capable AI system.\nrelation<|#|>Qwen Team<|#|>Qwen Family<|#|>association, development<|#|>The Qwen Team is associated with the Qwen family of models.\nrelation<|#|>Qwen Family<|#|>QwQ-32B<|#|>inclusion, component<|#|>QwQ-32B is part of the Qwen family.\nrelation<|#|>Qwen Family<|#|>Qwen3 Series<|#|>inclusion, component<|#|>The Qwen3 series is part of the Qwen family.\n<|COMPLETE|>",
    "cache_type": "extract",
    "chunk_id": "chunk-37f12102e8df23d0bf48e221b1e75d9e",
    "original_prompt": "---Task---\nBased on the last extraction task, identify and extract any **missed or incorrectly formatted** entities and relationships from the input text.\n\n---Instructions---\n1.  **Strict Adherence to System Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system instructions.\n2.  **Focus on Corrections/Additions:**\n    *   **Do NOT** re-output entities and relationships that were **correctly and fully** extracted in the last task.\n    *   If an entity or relationship was **missed** in the last task, extract and output it now according to the system format.\n    *   If an entity or relationship was **truncated, had missing fields, or was otherwise incorrectly formatted** in the last task, re-output the *corrected and complete* version in the specified format.\n3.  **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n4.  **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n5.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\n6.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant missing or corrected entities and relationships have been extracted and presented.\n7.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\n\n<Output>\n---Role---\nYou are a Knowledge Graph Specialist responsible for extracting entities and relationships from the input text.\n\n---Instructions---\n1.  **Entity Extraction & Output:**\n    *   **Identification:** Identify clearly defined and meaningful entities in the input text.\n    *   **Entity Details:** For each identified entity, extract the following information:\n        *   `entity_name`: The name of the entity. If the entity name is case-insensitive, capitalize the first letter of each significant word (title case). Ensure **consistent naming** across the entire extraction process.\n        *   `entity_type`: Categorize the entity using one of the following types: `Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject`. If none of the provided entity types apply, do not add new entity type and classify it as `Other`.\n        *   `entity_description`: Provide a concise yet comprehensive description of the entity's attributes and activities, based *solely* on the information present in the input text.\n    *   **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n        *   Format: `entity<|#|>entity_name<|#|>entity_type<|#|>entity_description`\n\n2.  **Relationship Extraction & Output:**\n    *   **Identification:** Identify direct, clearly stated, and meaningful relationships between previously extracted entities.\n    *   **N-ary Relationship Decomposition:** If a single statement describes a relationship involving more than two entities (an N-ary relationship), decompose it into multiple binary (two-entity) relationship pairs for separate description.\n        *   **Example:** For \"Alice, Bob, and Carol collaborated on Project X,\" extract binary relationships such as \"Alice collaborated with Project X,\" \"Bob collaborated with Project X,\" and \"Carol collaborated with Project X,\" or \"Alice collaborated with Bob,\" based on the most reasonable binary interpretations.\n    *   **Relationship Details:** For each binary relationship, extract the following fields:\n        *   `source_entity`: The name of the source entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `target_entity`: The name of the target entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `relationship_keywords`: One or more high-level keywords summarizing the overarching nature, concepts, or themes of the relationship. Multiple keywords within this field must be separated by a comma `,`. **DO NOT use `<|#|>` for separating multiple keywords within this field.**\n        *   `relationship_description`: A concise explanation of the nature of the relationship between the source and target entities, providing a clear rationale for their connection.\n    *   **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n        *   Format: `relation<|#|>source_entity<|#|>target_entity<|#|>relationship_keywords<|#|>relationship_description`\n\n3.  **Delimiter Usage Protocol:**\n    *   The `<|#|>` is a complete, atomic marker and **must not be filled with content**. It serves strictly as a field separator.\n    *   **Incorrect Example:** `entity<|#|>Tokyo<|location|>Tokyo is the capital of Japan.`\n    *   **Correct Example:** `entity<|#|>Tokyo<|#|>location<|#|>Tokyo is the capital of Japan.`\n\n4.  **Relationship Direction & Duplication:**\n    *   Treat all relationships as **undirected** unless explicitly stated otherwise. Swapping the source and target entities for an undirected relationship does not constitute a new relationship.\n    *   Avoid outputting duplicate relationships.\n\n5.  **Output Order & Prioritization:**\n    *   Output all extracted entities first, followed by all extracted relationships.\n    *   Within the list of relationships, prioritize and output those relationships that are **most significant** to the core meaning of the input text first.\n\n6.  **Context & Objectivity:**\n    *   Ensure all entity names and descriptions are written in the **third person**.\n    *   Explicitly name the subject or object; **avoid using pronouns** such as `this article`, `this paper`, `our company`, `I`, `you`, and `he/she`.\n\n7.  **Language & Proper Nouns:**\n    *   The entire output (entity names, keywords, and descriptions) must be written in `English`.\n    *   Proper nouns (e.g., personal names, place names, organization names) should be retained in their original language if a proper, widely accepted translation is not available or would cause ambiguity.\n\n8.  **Completion Signal:** Output the literal string `<|COMPLETE|>` only after all entities and relationships, following all criteria, have been completely extracted and outputted.\n\n---Examples---\n<Input Text>\n```\nwhile Alex clenched his jaw, the buzz of frustration dull against the backdrop of Taylor's authoritarian certainty. It was this competitive undercurrent that kept him alert, the sense that his and Jordan's shared commitment to discovery was an unspoken rebellion against Cruz's narrowing vision of control and order.\n\nThen Taylor did something unexpected. They paused beside Jordan and, for a moment, observed the device with something akin to reverence. \"If this tech can be understood...\" Taylor said, their voice quieter, \"It could change the game for us. For all of us.\"\n\nThe underlying dismissal earlier seemed to falter, replaced by a glimpse of reluctant respect for the gravity of what lay in their hands. Jordan looked up, and for a fleeting heartbeat, their eyes locked with Taylor's, a wordless clash of wills softening into an uneasy truce.\n\nIt was a small transformation, barely perceptible, but one that Alex noted with an inward nod. They had all been brought here by different paths\n```\n\n<Output>\nentity<|#|>Alex<|#|>person<|#|>Alex is a character who experiences frustration and is observant of the dynamics among other characters.\nentity<|#|>Taylor<|#|>person<|#|>Taylor is portrayed with authoritarian certainty and shows a moment of reverence towards a device, indicating a change in perspective.\nentity<|#|>Jordan<|#|>person<|#|>Jordan shares a commitment to discovery and has a significant interaction with Taylor regarding a device.\nentity<|#|>Cruz<|#|>person<|#|>Cruz is associated with a vision of control and order, influencing the dynamics among other characters.\nentity<|#|>The Device<|#|>equipment<|#|>The Device is central to the story, with potential game-changing implications, and is revered by Taylor.\nrelation<|#|>Alex<|#|>Taylor<|#|>power dynamics, observation<|#|>Alex observes Taylor's authoritarian behavior and notes changes in Taylor's attitude toward the device.\nrelation<|#|>Alex<|#|>Jordan<|#|>shared goals, rebellion<|#|>Alex and Jordan share a commitment to discovery, which contrasts with Cruz's vision.)\nrelation<|#|>Taylor<|#|>Jordan<|#|>conflict resolution, mutual respect<|#|>Taylor and Jordan interact directly regarding the device, leading to a moment of mutual respect and an uneasy truce.\nrelation<|#|>Jordan<|#|>Cruz<|#|>ideological conflict, rebellion<|#|>Jordan's commitment to discovery is in rebellion against Cruz's vision of control and order.\nrelation<|#|>Taylor<|#|>The Device<|#|>reverence, technological significance<|#|>Taylor shows reverence towards the device, indicating its importance and potential impact.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nStock markets faced a sharp downturn today as tech giants saw significant declines, with the global tech index dropping by 3.4% in midday trading. Analysts attribute the selloff to investor concerns over rising interest rates and regulatory uncertainty.\n\nAmong the hardest hit, nexon technologies saw its stock plummet by 7.8% after reporting lower-than-expected quarterly earnings. In contrast, Omega Energy posted a modest 2.1% gain, driven by rising oil prices.\n\nMeanwhile, commodity markets reflected a mixed sentiment. Gold futures rose by 1.5%, reaching $2,080 per ounce, as investors sought safe-haven assets. Crude oil prices continued their rally, climbing to $87.60 per barrel, supported by supply constraints and strong demand.\n\nFinancial experts are closely watching the Federal Reserve's next move, as speculation grows over potential rate hikes. The upcoming policy announcement is expected to influence investor confidence and overall market stability.\n```\n\n<Output>\nentity<|#|>Global Tech Index<|#|>category<|#|>The Global Tech Index tracks the performance of major technology stocks and experienced a 3.4% decline today.\nentity<|#|>Nexon Technologies<|#|>organization<|#|>Nexon Technologies is a tech company that saw its stock decline by 7.8% after disappointing earnings.\nentity<|#|>Omega Energy<|#|>organization<|#|>Omega Energy is an energy company that gained 2.1% in stock value due to rising oil prices.\nentity<|#|>Gold Futures<|#|>product<|#|>Gold futures rose by 1.5%, indicating increased investor interest in safe-haven assets.\nentity<|#|>Crude Oil<|#|>product<|#|>Crude oil prices rose to $87.60 per barrel due to supply constraints and strong demand.\nentity<|#|>Market Selloff<|#|>category<|#|>Market selloff refers to the significant decline in stock values due to investor concerns over interest rates and regulations.\nentity<|#|>Federal Reserve Policy Announcement<|#|>category<|#|>The Federal Reserve's upcoming policy announcement is expected to impact investor confidence and market stability.\nentity<|#|>3.4% Decline<|#|>category<|#|>The Global Tech Index experienced a 3.4% decline in midday trading.\nrelation<|#|>Global Tech Index<|#|>Market Selloff<|#|>market performance, investor sentiment<|#|>The decline in the Global Tech Index is part of the broader market selloff driven by investor concerns.\nrelation<|#|>Nexon Technologies<|#|>Global Tech Index<|#|>company impact, index movement<|#|>Nexon Technologies' stock decline contributed to the overall drop in the Global Tech Index.\nrelation<|#|>Gold Futures<|#|>Market Selloff<|#|>market reaction, safe-haven investment<|#|>Gold prices rose as investors sought safe-haven assets during the market selloff.\nrelation<|#|>Federal Reserve Policy Announcement<|#|>Market Selloff<|#|>interest rate impact, financial regulation<|#|>Speculation over Federal Reserve policy changes contributed to market volatility and investor selloff.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nAt the World Athletics Championship in Tokyo, Noah Carter broke the 100m sprint record using cutting-edge carbon-fiber spikes.\n```\n\n<Output>\nentity<|#|>World Athletics Championship<|#|>event<|#|>The World Athletics Championship is a global sports competition featuring top athletes in track and field.\nentity<|#|>Tokyo<|#|>location<|#|>Tokyo is the host city of the World Athletics Championship.\nentity<|#|>Noah Carter<|#|>person<|#|>Noah Carter is a sprinter who set a new record in the 100m sprint at the World Athletics Championship.\nentity<|#|>100m Sprint Record<|#|>category<|#|>The 100m sprint record is a benchmark in athletics, recently broken by Noah Carter.\nentity<|#|>Carbon-Fiber Spikes<|#|>equipment<|#|>Carbon-fiber spikes are advanced sprinting shoes that provide enhanced speed and traction.\nentity<|#|>World Athletics Federation<|#|>organization<|#|>The World Athletics Federation is the governing body overseeing the World Athletics Championship and record validations.\nrelation<|#|>World Athletics Championship<|#|>Tokyo<|#|>event location, international competition<|#|>The World Athletics Championship is being hosted in Tokyo.\nrelation<|#|>Noah Carter<|#|>100m Sprint Record<|#|>athlete achievement, record-breaking<|#|>Noah Carter set a new 100m sprint record at the championship.\nrelation<|#|>Noah Carter<|#|>Carbon-Fiber Spikes<|#|>athletic equipment, performance boost<|#|>Noah Carter used carbon-fiber spikes to enhance performance during the race.\nrelation<|#|>Noah Carter<|#|>World Athletics Championship<|#|>athlete participation, competition<|#|>Noah Carter is competing at the World Athletics Championship.\n<|COMPLETE|>\n\n\n\n---Real Data to be Processed---\n<Input>\nEntity_types: [Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject]\nText:\n```\nASurveyofReinforcementLearningforLargeReasoningModels\nIn practice, it is common to regularize the learned policy towards a reference policy  , often\nref\nimplemented as KL-divergence constraints to stabilize training and maintain language quality. In the\nfollowing sections, we present various algorithms that build upon this fundamental formulation.\n2.2. Frontier Models\nIn this subsection, we provide an overview of state-of-the-art large reasoning models trained with\nRL-like methods, organized roughly chronologically along three major directions: LRMs, agentic\nLRMs, and multimodal LRMs.\nOver the past year, RL has progressively expanded the frontier of reasoning models and their\napplications. Thefirstlargereasoningmodels,OpenAIso1[Jaechetal.,2024]series,establishedthe\neffectiveness of scaling both train-time RL and test-time compute towards more powerful reasoning\nabilities, achieving leading results on mathematics, coding, and science benchmarks. DeepSeeks\nflagship model R1 [Guo et al., 2025a] followed as the first open-source model to match o1s per-\nformance across benchmarks. It employs a multi-stage training pipeline to ensure well-rounded\nmodel abilities, and explores the route of pure RL without supervised finetuning (i.e., Zero RL).\nOther proprietary model releases promptly followed: Claude-3.7-Sonnet [Anthropic, 2025a] featured\nhybrid reasoning, Gemini 2.0 and 2.5 [Comanici et al., 2025] introduced longer context lengths,\nSeed-Thinking 1.5 [Seed et al., 2025b] featured generalization across domains, and the o3 [OpenAI,\n2025b]seriesshowcasedincreasinglyadvancedreasoningabilities. Recently,OpenAIintroducedtheir\nfirstopen-sourcereasoningmodelgpt-oss-120b[Agarwaletal.,2025a],andsubsequentlyGPT5[Ope-\nnAI,2025a],theirmostcapableAIsystemtodate,whichflexiblyswitchesbetweenanefficientmodel\nand a deeper reasoning model GPT-5 thinking. Parallel open-source efforts continued to expand\nthe landscape. Within the Qwen family, QwQ-32B [Team, 2025g] matched R1s performance, and\nwas followed by the Qwen3 [Yang et al., 2025a] series, with the representative model Qwen3-235B\nfurther improving benchmark scores. The Skywork-OR1 [He et al., 2025d] suite of models were\nbased on R1-distilled models, and achieved scalable RL training through effective data mixtures and\nalgorithmic innovations. Minimax-M1 [Chen et al., 2025a] was the first model to introduce hybrid\nattention to scale RL efficiently. Other works include Llama-Nemotron-Ultra [Bercovich et al., 2025],\nwhichaimedtobalanceaccuracyandefficiency;Magistral24B[Rastogietal.,2025],trainedthrough\nRL from scratch without distillation from prior models; and Seed-OSS [Team, 2025a], emphasizing\nlong-context reasoning abilities.\nModel reasoning improvements have in turn extended their use cases in coding and agentic\nscenarios. The Claude series has been known for their leading performance on agentic coding tasks,\nand this was exemplified by Claude-4.1-Opus [Anthropic, 2025b], which further pushed forward\nthe state-of-the-art results on SWE-bench [Jimenez et al., 2023]. Kimi K2 [Team, 2025d] is a recent\nrepresentative agentic model which was specifically optimized for agentic tasks, forging large-scale\nagentictrainingdatasynthesisandageneralRLprocedurethataccommodatesnon-verifiablerewards.\nShortly after, both the GLM4.5 [Zeng et al., 2025a] and DeepSeek-V3.1 releases emphasized tool-use\nand agentic tasks, showing substantial improvements on relevant benchmarks.\nMultimodality is a key component behind the widespread adoption of reasoning models. Most\nfrontierproprietarymodels,includingGPT-5,o3,Claude,andGeminifamilies,arenativelymultimodal.\nGemini-2.5[Comanicietal.,2025]notablyemphasizedstrongperformanceacrosstext,images,video,\nand audio. On the open-source side, Kimi 1.5 [Team, 2025d] represents an early effort towards\nmultimodalreasoning,highlightinglongcontextscalingaswellasjointreasoningovertextandvision\ndomains. QVQ [Qwen Team, 2025] excels in visual reasoning and analytical thinking, while Skywork\nR1V2 [Wang et al., 2025k] balances reasoning and general abilities through hybrid RL, using both\nMPO and GRPO. As notable additions to the InternVL series, InternVL3 [Zhu et al., 2025c] adopted\n7\n```\n[{\"role\": \"user\", \"content\": \"---Task---\\nExtract entities and relationships from the input text to be processed.\\n\\n---Instructions---\\n1.  **Strict Adherence to Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system prompt.\\n2.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\\n3.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant entities and relationships have been extracted and presented.\\n4.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\\n\\n<Output>\"}, {\"role\": \"assistant\", \"content\": \"entity<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>content<|#|>The title of a survey paper discussing the application of Reinforcement Learning to Large Reasoning Models.\\nentity<|#|>Reinforcement Learning<|#|>concept<|#|>A learning paradigm that has progressively expanded the frontier of reasoning models and their applications.\\nentity<|#|>Large Reasoning Models<|#|>concept<|#|>State-of-the-art models trained with RL-like methods, organized chronologically along three major directions: LRMs, agentic LRMs, and multimodal LRMs.\\nentity<|#|>Reference Policy<|#|>concept<|#|>A policy towards which a learned policy is regularized to stabilize training and maintain language quality.\\nentity<|#|>KL-Divergence Constraints<|#|>method<|#|>Constraints implemented to regularize a learned policy towards a reference policy.\\nentity<|#|>Frontier Models<|#|>concept<|#|>A term referring to state-of-the-art large reasoning models.\\nentity<|#|>OpenAI<|#|>organization<|#|>A company that developed the o1 series, o3 series, gpt-oss-120b, and GPT5 models.\\nentity<|#|>o1 Series<|#|>artifact<|#|>The first large reasoning models from OpenAI, establishing the effectiveness of scaling both train-time RL and test-time compute towards more powerful reasoning abilities.\\nentity<|#|>Jaechetal.,2024<|#|>data<|#|>A citation associated with the o1 series.\\nentity<|#|>Scaling Train-Time RL<|#|>concept<|#|>The effectiveness of increasing Reinforcement Learning during the training phase of models.\\nentity<|#|>Scaling Test-Time Compute<|#|>concept<|#|>The effectiveness of increasing computational resources during the testing phase of models.\\nentity<|#|>Mathematics Benchmarks<|#|>data<|#|>Benchmarks used to evaluate the performance of reasoning models in mathematics.\\nentity<|#|>Coding Benchmarks<|#|>data<|#|>Benchmarks used to evaluate the performance of reasoning models in coding.\\nentity<|#|>Science Benchmarks<|#|>data<|#|>Benchmarks used to evaluate the performance of reasoning models in science.\\nentity<|#|>DeepSeek<|#|>organization<|#|>A company that developed the R1 and DeepSeek-V3.1 models.\\nentity<|#|>R1<|#|>artifact<|#|>DeepSeeks flagship model, the first open-source model to match o1s performance across benchmarks, employing a multi-stage training pipeline and exploring Zero RL.\\nentity<|#|>Guo et al., 2025a<|#|>data<|#|>A citation associated with the R1 model.\\nentity<|#|>Multi-Stage Training Pipeline<|#|>method<|#|>A training approach employed by R1 to ensure well-rounded model abilities.\\nentity<|#|>Zero RL<|#|>method<|#|>A training approach that uses pure Reinforcement Learning without supervised finetuning.\\nentity<|#|>Claude-3.7-Sonnet<|#|>artifact<|#|>A proprietary model from Anthropic featuring hybrid reasoning.\\nentity<|#|>Anthropic<|#|>organization<|#|>A company that developed the Claude series, Claude-3.7-Sonnet, and Claude-4.1-Opus models.\\nentity<|#|>Anthropic, 2025a<|#|>data<|#|>A citation associated with Claude-3.7-Sonnet.\\nentity<|#|>Hybrid Reasoning<|#|>concept<|#|>A feature of Claude-3.7-Sonnet.\\nentity<|#|>Gemini 2.0<|#|>artifact<|#|>A proprietary model that introduced longer context lengths.\\nentity<|#|>Gemini 2.5<|#|>artifact<|#|>A proprietary model that introduced longer context lengths and notably emphasized strong performance across text, images, video, and audio.\\nentity<|#|>Comanici et al., 2025<|#|>data<|#|>A citation associated with Gemini 2.0 and Gemini 2.5.\\nentity<|#|>Longer Context Lengths<|#|>concept<|#|>An introduced feature in Gemini 2.0 and 2.5 models.\\nentity<|#|>Seed-Thinking 1.5<|#|>artifact<|#|>A proprietary model featuring generalization across domains.\\nentity<|#|>Seed et al., 2025b<|#|>data<|#|>A citation associated with Seed-Thinking 1.5.\\nentity<|#|>Generalization Across Domains<|#|>concept<|#|>A feature of Seed-Thinking 1.5.\\nentity<|#|>o3 Series<|#|>artifact<|#|>A series of models from OpenAI showcasing increasingly advanced reasoning abilities.\\nentity<|#|>OpenAI, 2025b<|#|>data<|#|>A citation associated with the o3 series.\\nentity<|#|>Advanced Reasoning Abilities<|#|>concept<|#|>A characteristic showcased by the o3 series.\\nentity<|#|>gpt-oss-120b<|#|>artifact<|#|>OpenAI's first open-source reasoning model.\\nentity<|#|>Agarwal et al., 2025a<|#|>data<|#|>A citation associated with gpt-oss-120b.\\nentity<|#|>GPT5<|#|>artifact<|#|>OpenAI's most capable AI system to date, which flexibly switches between an efficient model and a deeper reasoning model.\\nentity<|#|>OpenAI, 2025a<|#|>data<|#|>A citation associated with GPT5.\\nentity<|#|>Efficient Model<|#|>concept<|#|>A type of model that GPT5 can switch to.\\nentity<|#|>GPT-5 Thinking<|#|>artifact<|#|>A deeper reasoning model component within GPT5.\\nentity<|#|>Qwen Team<|#|>organization<|#|>The team associated with the Qwen family of models, including QwQ-32B and the Qwen3 series.\\nentity<|#|>QwQ-32B<|#|>artifact<|#|>A model within the Qwen family that matched R1s performance.\\nentity<|#|>Team, 2025g<|#|>data<|#|>A citation associated with QwQ-32B.\\nentity<|#|>Qwen3 Series<|#|>artifact<|#|>A series of models from the Qwen family, with Qwen3-235B as a representative model.\\nentity<|#|>Yang et al., 2025a<|#|>data<|#|>A citation associated with the Qwen3 series.\\nentity<|#|>Qwen3-235B<|#|>artifact<|#|>A representative model from the Qwen3 series that further improved benchmark scores.\\nentity<|#|>Skywork-OR1<|#|>artifact<|#|>A suite of models based on R1-distilled models, achieving scalable RL training through effective data mixtures and algorithmic innovations.\\nentity<|#|>He et al., 2025d<|#|>data<|#|>A citation associated with Skywork-OR1.\\nentity<|#|>R1-Distilled Models<|#|>concept<|#|>Models that Skywork-OR1 is based on.\\nentity<|#|>Scalable RL Training<|#|>concept<|#|>A characteristic achieved by Skywork-OR1 through effective data mixtures and algorithmic innovations.\\nentity<|#|>Minimax-M1<|#|>artifact<|#|>The first model to introduce hybrid attention to scale RL efficiently.\\nentity<|#|>Chen et al., 2025a<|#|>data<|#|>A citation associated with Minimax-M1.\\nentity<|#|>Hybrid Attention<|#|>method<|#|>A technique introduced by Minimax-M1 to scale Reinforcement Learning efficiently.\\nentity<|#|>Llama-Nemotron-Ultra<|#|>artifact<|#|>A model that aimed to balance accuracy and efficiency.\\nentity<#|>Bercovich et al., 2025<|#|>data<|#|>A citation associated with Llama-Nemotron-Ultra.\\nentity<|#|>Accuracy<|#|>concept<|#|>A quality that Llama-Nemotron-Ultra aimed to balance with efficiency.\\nentity<|#|>Efficiency<|#|>concept<|#|>A quality that Llama-Nemotron-Ultra aimed to balance with accuracy.\\nentity<|#|>Magistral24B<|#|>artifact<|#|>A model trained through Reinforcement Learning from scratch without distillation from prior models.\\nentity<|#|>Rastogi et al., 2025<|#|>data<|#|>A citation associated with Magistral24B.\\nentity<|#|>Reinforcement Learning from Scratch<|#|>method<|#|>A training approach used for Magistral24B.\\nentity<|#|>Distillation from Prior Models<|#|>method<|#|>A training approach that Magistral24B was trained without.\\nentity<|#|>Seed-OSS<|#|>artifact<|#|>A model emphasizing long-context reasoning abilities.\\nentity<|#|>Team, 2025a<|#|>data<|#|>A citation associated with Seed-OSS.\\nentity<|#|>Long-Context Reasoning Abilities<|#|>concept<|#|>A characteristic emphasized by Seed-OSS.\\nentity<|#|>Claude Series<|#|>artifact<|#|>A family of models from Anthropic known for their leading performance on agentic coding tasks.\\nentity<|#|>Claude-4.1-Opus<|#|>artifact<|#|>A model from the Claude series that further pushed forward the state-of-the-art results on SWE-bench.\\nentity<|#|>Anthropic, 2025b<|#|>data<|#|>A citation associated with Claude-4.1-Opus.\\nentity<|#|>SWE-bench<|#|>data<|#|>A benchmark used to evaluate performance on agentic coding tasks.\\nentity<|#|>Jimenez et al., 2023<|#|>data<|#|>A citation associated with SWE-bench.\\nentity<|#|>Kimi K2<|#|>artifact<|#|>A recent representative agentic model specifically optimized for agentic tasks, forging large-scale agentic training data synthesis and a general RL procedure.\\nentity<|#|>Team, 2025d<|#|>data<|#|>A citation associated with Kimi K2 and Kimi 1.5.\\nentity<|#|>Agentic Tasks<|#|>concept<|#|>Tasks for which Kimi K2, GLM4.5, and DeepSeek-V3.1 are optimized or emphasized.\\nentity<|#|>Large-Scale Agentic Training Data Synthesis<|#|>method<|#|>A technique used by Kimi K2.\\nentity<|#|>General RL Procedure<|#|>method<|#|>A procedure used by Kimi K2 that accommodates non-verifiable rewards.\\nentity<|#|>GLM4.5<|#|>artifact<|#|>A model that emphasized tool-use and agentic tasks.\\nentity<|#|>Zeng et al., 2025a<|#|>data<|#|>A citation associated with GLM4.5.\\nentity<|#|>DeepSeek-V3.1<|#|>artifact<|#|>A model that emphasized tool-use and agentic tasks.\\nentity<|#|>Tool-Use<|#|>concept<|#|>A capability emphasized by GLM4.5 and DeepSeek-V3.1.\\nentity<|#|>Multimodality<|#|>concept<|#|>A key component behind the widespread adoption of reasoning models, involving processing various data types.\\nentity<|#|>Widespread Adoption of Reasoning Models<|#|>concept<|#|>An outcome facilitated by multimodality.\\nentity<|#|>Gemini Family<|#|>artifact<|#|>A family of proprietary models, including Gemini 2.0 and 2.5, that are natively multimodal.\\nentity<|#|>Text Domain<|#|>concept<|#|>A domain of performance emphasized by Gemini 2.5.\\nentity<|#|>Images Domain<|#|>concept<|#|>A domain of performance emphasized by Gemini 2.5.\\nentity<|#|>Video Domain<|#|>concept<|#|>A domain of performance emphasized by Gemini 2.5.\\nentity<|#|>Audio Domain<|#|>concept<|#|>A domain of performance emphasized by Gemini 2.5.\\nentity<|#|>Kimi 1.5<|#|>artifact<|#|>An early open-source effort towards multimodal reasoning, highlighting long context scaling and joint reasoning over text and vision domains.\\nentity<|#|>Long Context Scaling<|#|>concept<|#|>A characteristic highlighted by Kimi 1.5.\\nentity<#|>Joint Reasoning Over Text and Vision Domains<|#|>concept<|#|>A characteristic highlighted by Kimi 1.5.\\nentity<|#|>QVQ<|#|>artifact<|#|>A model that excels in visual reasoning and analytical thinking.\\nentity<|#|>Qwen Team, 2025<|#|>data<|#|>A citation associated with QVQ.\\nentity<|#|>Visual Reasoning<|#|>concept<|#|>A capability in which QVQ excels.\\nentity<|#|>Analytical Thinking<|#|>concept<|#|>A capability in which QVQ excels.\\nentity<|#|>Skywork R1V2<|#|>artifact<|#|>A model that balances reasoning and general abilities through hybrid RL, using MPO and GRPO.\\nentity<|#|>Wang et al., 2025k<|#|>data<|#|>A citation associated with Skywork R1V2.\\nentity<|#|>General Abilities<|#|>concept<|#|>A characteristic balanced by Skywork R1V2.\\nentity<|#|>Hybrid RL<|#|>method<|#|>A method used by Skywork R1V2 to balance reasoning and general abilities.\\nentity<|#|>MPO<|#|>method<|#|>A method used in hybrid Reinforcement Learning for models like Skywork R1V2.\\nentity<|#|>GRPO<|#|>method<|#|>A method used in hybrid Reinforcement Learning for models like Skywork R1V2.\\nentity<|#|>InternVL Series<|#|>artifact<|#|>A series of models, including InternVL3.\\nentity<|#|>InternVL3<|#|>artifact<|#|>A model from the InternVL series.\\nentity<|#|>Zhu et al., 2025c<|#|>data<|#|>A citation associated with InternVL3.\\nrelation<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>Reinforcement Learning<|#|>discussion, topic<|#|>The survey discusses Reinforcement Learning.\\nrelation<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>Large Reasoning Models<|#|>discussion, topic<|#|>The survey discusses Large Reasoning Models.\\nrelation<|#|>Reinforcement Learning<|#|>Large Reasoning Models<|#|>application, training<|#|>Reinforcement Learning is used to train Large Reasoning Models.\\nrelation<|#|>Learned Policy<|#|>Reference Policy<|#|>regularization, stabilization<|#|>A learned policy is regularized towards a reference policy.\\nrelation<|#|>KL-Divergence Constraints<|#|>Reference Policy<|#|>implementation, regularization<|#|>KL-Divergence Constraints are implemented to regularize towards a reference policy.\\nrelation<|#|>Frontier Models<|#|>Large Reasoning Models<|#|>categorization, type<|#|>Frontier Models are a category of Large Reasoning Models.\\nrelation<|#|>Frontier Models<|#|>Reinforcement Learning<|#|>training method, application<|#|>Frontier Models are trained with Reinforcement Learning-like methods.\\nrelation<#|>OpenAI<|#|>o1 Series<|#|>development, creation<|#|>OpenAI developed the o1 series.\\nrelation<|#|>o1 Series<|#|>Jaechetal.,2024<|#|>citation, reference<|#|>The o1 series is cited by Jaechetal.,2024.\\nrelation<|#|>o1 Series<|#|>Scaling Train-Time RL<|#|>effectiveness, demonstration<|#|>The o1 series established the effectiveness of scaling train-time RL.\\nrelation<|#|>o1 Series<|#|>Scaling Test-Time Compute<|#|>effectiveness, demonstration<|#|>The o1 series established the effectiveness of scaling test-time compute.\\nrelation<|#|>o1 Series<|#|>Mathematics Benchmarks<|#|>achievement, performance<|#|>The o1 series achieved leading results on mathematics benchmarks.\\nrelation<|#|>o1 Series<|#|>Coding Benchmarks<|#|>achievement, performance<|#|>The o1 series achieved leading results on coding benchmarks.\\nrelation<|#|>o1 Series<|#|>Science Benchmarks<|#|>achievement, performance<|#|>The o1 series achieved leading results on science benchmarks.\\nrelation<|#|>DeepSeek<|#|>R1<|#|>development, creation<|#|>DeepSeek developed the R1 model.\\nrelation<|#|>R1<|#|>Guo et al., 2025a<|#|>citation, reference<|#|>R1 is cited by Guo et al., 2025a.\\nrelation<|#|>R1<|#|>o1 Series<|#|>performance, comparison<|#|>R1 matched the performance of the o1 series.\\nrelation<|#|>R1<|#|>Multi-Stage Training Pipeline<|#|>employment, method<|#|>R1 employs a multi-stage training pipeline.\\nrelation<|#|>R1<|#|>Zero RL<|#|>exploration, method<|#|>R1 explores the route of Zero RL.\\nrelation<|#|>Zero RL<|#|>Reinforcement Learning<|#|>type, approach<|#|>Zero RL is a form of pure Reinforcement Learning without supervised finetuning.\\nrelation<|#|>Anthropic<|#|>Claude-3.7-Sonnet<|#|>development, creation<|#|>Anthropic developed Claude-3.7-Sonnet.\\nrelation<|#|>Claude-3.7-Sonnet<|#|>Anthropic, 2025a<|#|>citation, reference<|#|>Claude-3.7-Sonnet is cited by Anthropic, 2025a.\\nrelation<|#|>Claude-3.7-Sonnet<|#|>Hybrid Reasoning<|#|>feature, capability<|#|>Claude-3.7-Sonnet features hybrid reasoning.\\nrelation<|#|>Gemini 2.0<|#|>Comanici et al., 2025<|#|>citation, reference<|#|>Gemini 2.0 is cited by Comanici et al., 2025.\\nrelation<|#|>Gemini 2.5<|#|>Comanici et al., 2025<|#|>citation, reference<|#|>Gemini 2.5 is cited by Comanici et al., 2025.\\nrelation<|#|>Gemini 2.0<|#|>Longer Context Lengths<|#|>introduction, feature<|#|>Gemini 2.0 introduced longer context lengths.\\nrelation<|#|>Gemini 2.5<|#|>Longer Context Lengths<|#|>introduction, feature<|#|>Gemini 2.5 introduced longer context lengths.\\nrelation<|#|>Seed-Thinking 1.5<|#|>Seed et al., 2025b<|#|>citation, reference<|#|>Seed-Thinking 1.5 is cited by Seed et al., 2025b.\\nrelation<|#|>Seed-Thinking 1.5<|#|>Generalization Across Domains<|#|>feature, capability<|#|>Seed-Thinking 1.5 features generalization across domains.\\nrelation<|#|>OpenAI<|#|>o3 Series<|#|>development, creation<|#|>OpenAI developed the o3 series.\\nrelation<|#|>o3 Series<|#|>OpenAI, 2025b<|#|>citation, reference<|#|>The o3 series is cited by OpenAI, 2025b.\\nrelation<|#|>o3 Series<|#|>Advanced Reasoning Abilities<|#|>showcase, capability<|#|>The o3 series showcased increasingly advanced reasoning abilities.\\nrelation<|#|>OpenAI<|#|>gpt-oss-120b<|#|>introduction, development<|#|>OpenAI introduced gpt-oss-120b.\\nrelation<|#|>gpt-oss-120b<|#|>Agarwal et al., 2025a<|#|>citation, reference<|#|>gpt-oss-120b is cited by Agarwal et al., 2025a.\\nrelation<|#|>gpt-oss-120b<|#|>OpenAI<|#|>ownership, origin<|#|>gpt-oss-120b is OpenAI's first open-source reasoning model.\\nrelation<|#|>OpenAI<|#|>GPT5<|#|>development, creation<|#|>OpenAI developed GPT5.\\nrelation<|#|>GPT5<|#|>OpenAI, 2025a<|#|>citation, reference<|#|>GPT5 is cited by OpenAI, 2025a.\\nrelation<|#|>GPT5<|#|>Efficient Model<|#|>switching, capability<|#|>GPT5 flexibly switches to an efficient model.\\nrelation<|#|>GPT5<|#|>GPT-5 Thinking<|#|>switching, capability<|#|>GPT5 flexibly switches to GPT-5 Thinking.\\nrelation<|#|>Qwen Team<|#|>QwQ-32B<|#|>development, creation<|#|>The Qwen Team developed QwQ-32B.\\nrelation<|#|>QwQ-32B<|#|>Team, 2025g<|#|>citation, reference<|#|>QwQ-32B is cited by Team, 2025g.\\nrelation<|#|>QwQ-32B<|#|>R1<|#|>performance, comparison<|#|>QwQ-32B matched R1's performance.\\nrelation<|#|>Qwen Team<|#|>Qwen3 Series<|#|>development, creation<|#|>The Qwen Team developed the Qwen3 series.\\nrelation<|#|>Qwen3 Series<|#|>Yang et al., 2025a<|#|>citation, reference<|#|>The Qwen3 series is cited by Yang et al., 2025a.\\nrelation<|#|>Qwen3 Series<|#|>Qwen3-235B<|#|>inclusion, component<|#|>The Qwen3 series includes Qwen3-235B.\\nrelation<|#|>Qwen3-235B<|#|>Benchmark Scores<|#|>improvement, performance<|#|>Qwen3-235B further improved benchmark scores.\\nrelation<|#|>Skywork-OR1<|#|>He et al., 2025d<|#|>citation, reference<|#|>Skywork-OR1 is cited by He et al., 2025d.\\nrelation<|#|>Skywork-OR1<|#|>R1-Distilled Models<|#|>basis, foundation<|#|>Skywork-OR1 models were based on R1-distilled models.\\nrelation<|#|>Skywork-OR1<|#|>Scalable RL Training<|#|>achievement, method<|#|>Skywork-OR1 achieved scalable RL training.\\nrelation<|#|>Minimax-M1<|#|>Chen et al., 2025a<|#|>citation, reference<|#|>Minimax-M1 is cited by Chen et al., 2025a.\\nrelation<|#|>Minimax-M1<|#|>Hybrid Attention<|#|>introduction, innovation<|#|>Minimax-M1 introduced hybrid attention.\\nrelation<|#|>Minimax-M1<|#|>Reinforcement Learning<|#|>scaling, efficiency<|#|>Minimax-M1 scales Reinforcement Learning efficiently.\\nrelation<|#|>Llama-Nemotron-Ultra<|#|>Bercovich et al., 2025<|#|>citation, reference<|#|>Llama-Nemotron-Ultra is cited by Bercovich et al., 2025.\\nrelation<|#|>Llama-Nemotron-Ultra<|#|>Accuracy<|#|>balancing, objective<|#|>Llama-Nemotron-Ultra aimed to balance accuracy.\\nrelation<|#|>Llama-Nemotron-Ultra<|#|>Efficiency<|#|>balancing, objective<|#|>Llama-Nemotron-Ultra aimed to balance efficiency.\\nrelation<|#|>Magistral24B<|#|>Rastogi et al., 2025<|#|>citation, reference<|#|>Magistral24B is cited by Rastogi et al., 2025.\\nrelation<|#|>Magistral24B<|#|>Reinforcement Learning from Scratch<|#|>training method, approach<|#|>Magistral24B was trained through Reinforcement Learning from scratch.\\nrelation<|#|>Magistral24B<|#|>Distillation from Prior Models<|#|>training method, absence<|#|>Magistral24B was trained without distillation from prior models.\\nrelation<|#|>Seed-OSS<|#|>Team, 2025a<|#|>citation, reference<|#|>Seed-OSS is cited by Team, 2025a.\\nrelation<|#|>Seed-OSS<|#|>Long-Context Reasoning Abilities<|#|>emphasis, feature<|#|>Seed-OSS emphasizes long-context reasoning abilities.\\nrelation<|#|>Claude Series<|#|>Agentic Tasks<|#|>performance, capability<|#|>The Claude series is known for leading performance on agentic coding tasks.\\nrelation<|#|>Anthropic<|#|>Claude-4.1-Opus<|#|>development, creation<|#|>Anthropic developed Claude-4.1-Opus.\\nrelation<|#|>Claude-4.1-Opus<|#|>Anthropic, 2025b<|#|>citation, reference<|#|>Claude-4.1-Opus is cited by Anthropic, 2025b.\\nrelation<|#|>Claude-4.1-Opus<|#|>SWE-bench<|#|>performance, benchmark<|#|>Claude-4.1-Opus pushed state-of-the-art results on SWE-bench.\\nrelation<|#|>SWE-bench<|#|>Jimenez et al., 2023<|#|>citation, reference<|#|>SWE-bench is cited by Jimenez et al., 2023.\\nrelation<|#|>Kimi K2<|#|>Team, 2025d<|#|>citation, reference<|#|>Kimi K2 is cited by Team, 2025d.\\nrelation<|#|>Kimi K2<|#|>Agentic Tasks<|#|>optimization, focus<|#|>Kimi K2 was optimized for agentic tasks.\\nrelation<|#|>Kimi K2<|#|>Large-Scale Agentic Training Data Synthesis<|#|>utilization, method<|#|>Kimi K2 uses large-scale agentic training data synthesis.\\nrelation<|#|>Kimi K2<|#|>General RL Procedure<|#|>utilization, method<|#|>Kimi K2 uses a general RL procedure.\\nrelation<|#|>GLM4.5<|#|>Zeng et al., 2025a<|#|>citation, reference<|#|>GLM4.5 is cited by Zeng et al., 2025a.\\nrelation<|#|>GLM4.5<|#|>Tool-Use<|#|>emphasis, capability<|#|>GLM4.5 emphasized tool-use.\\nrelation<|#|>GLM4.5<|#|>Agentic Tasks<|#|>emphasis, capability<|#|>GLM4.5 emphasized agentic tasks.\\nrelation<|#|>DeepSeek-V3.1<|#|>Tool-Use<|#|>emphasis, capability<|#|>DeepSeek-V3.1 emphasized tool-use.\\nrelation<|#|>DeepSeek-V3.1<|#|>Agentic Tasks<|#|>emphasis, capability<|#|>DeepSeek-V3.1 emphasized agentic tasks.\\nrelation<|#|>Multimodality<|#|>Widespread Adoption of Reasoning Models<|#|>enabling factor, impact<|#|>Multimodality is a key component behind the widespread adoption of reasoning models.\\nrelation<|#|>GPT5<|#|>Multimodality<|#|>characteristic, capability<|#|>GPT5 is natively multimodal.\\nrelation<|#|>o3 Series<|#|>Multimodality<|#|>characteristic, capability<|#|>The o3 series is natively multimodal.\\nrelation<|#|>Claude Series<|#|>Multimodality<|#|>characteristic, capability<|#|>The Claude series is natively multimodal.\\nrelation<|#|>Gemini Family<|#|>Multimodality<|#|>characteristic, capability<|#|>The Gemini family is natively multimodal.\\nrelation<|#|>Gemini 2.5<|#|>Text Domain<|#|>performance, emphasis<|#|>Gemini 2.5 emphasized strong performance across the text domain.\\nrelation<|#|>Gemini 2.5<|#|>Images Domain<|#|>performance, emphasis<|#|>Gemini 2.5 emphasized strong performance across the images domain.\\nrelation<|#|>Gemini 2.5<|#|>Video Domain<|#|>performance, emphasis<|#|>Gemini 2.5 emphasized strong performance across the video domain.\\nrelation<|#|>Gemini 2.5<|#|>Audio Domain<|#|>performance, emphasis<|#|>Gemini 2.5 emphasized strong performance across the audio domain.\\nrelation<|#|>Kimi 1.5<|#|>Team, 2025d<|#|>citation, reference<|#|>Kimi 1.5 is cited by Team, 2025d.\\nrelation<|#|>Kimi 1.5<|#|>Multimodal Reasoning<|#|>effort, focus<|#|>Kimi 1.5 represents an early effort towards multimodal reasoning.\\nrelation<|#|>Kimi 1.5<|#|>Long Context Scaling<|#|>highlight, feature<|#|>Kimi 1.5 highlights long context scaling.\\nrelation<|#|>Kimi 1.5<|#|>Joint Reasoning Over Text and Vision Domains<|#|>highlight, feature<|#|>Kimi 1.5 highlights joint reasoning over text and vision domains.\\nrelation<|#|>QVQ<|#|>Qwen Team, 2025<|#|>citation, reference<|#|>QVQ is cited by Qwen Team, 2025.\\nrelation<|#|>QVQ<|#|>Visual Reasoning<|#|>excellence, capability<|#|>QVQ excels in visual reasoning.\\nrelation<|#|>QVQ<|#|>Analytical Thinking<|#|>excellence, capability<|#|>QVQ excels in analytical thinking.\\nrelation<|#|>Skywork R1V2<|#|>Wang et al., 2025k<|#|>citation, reference<|#|>Skywork R1V2 is cited by Wang et al., 2025k.\\nrelation<|#|>Skywork R1V2<|#|>Reinforcement Learning<|#|>training method, balance<|#|>Skywork R1V2 balances reasoning and general abilities through hybrid RL.\\nrelation<|#|>Skywork R1V2<|#|>General Abilities<|#|>balance, characteristic<|#|>Skywork R1V2 balances general abilities.\\nrelation<|#|>Skywork R1V2<|#|>Hybrid RL<|#|>utilization, method<|#|>Skywork R1V2 uses hybrid RL.\\nrelation<|#|>Hybrid RL<|#|>MPO<|#|>component, method<|#|>Hybrid RL uses MPO.\\nrelation<|#|>Hybrid RL<|#|>GRPO<|#|>component, method<|#|>Hybrid RL uses GRPO.\\nrelation<|#|>InternVL Series<|#|>InternVL3<|#|>inclusion, component<|#|>The InternVL series includes InternVL3.\\nrelation<|#|>InternVL3<|#|>Zhu et al., 2025c<|#|>citation, reference<|#|>InternVL3 is cited by Zhu et al., 2025c.\\n<|COMPLETE|>\"}]",
    "queryparam": null,
    "create_time": 1765219038,
    "update_time": 1765219038,
    "_id": "default:extract:8895c85dfd34ade3418b18508baf5ab8"
  },
  "default:extract:8175eaea69f695b6053aff549e89eae7": {
    "return": "entity<|#|>21/117B<|#|>Artifact<|#|>An identifier or model name listed in the input text.\nentity<|#|>GLM-4.5V 106B<|#|>Artifact<|#|>A named model or system listed in the input text.\nentity<|#|>O3-Pro<|#|>Artifact<|#|>A named model or system listed in the input text.\nentity<|#|>Gemini 2.5 Flash<|#|>Artifact<|#|>A named model or system, likely a version of the Gemini series, listed in the input text.\nentity<|#|>Gemini 2.5 Pro<|#|>Artifact<|#|>A named model or system, likely a version of the Gemini series, listed in the input text.\nentity<|#|>Seed-Thinking 1.6<|#|>Artifact<|#|>A named model or system listed in the input text.\nrelation<|#|>Gemini 2.5 Flash<|#|>Gemini 2.5 Pro<|#|>version, variant<|#|>These entities are different versions of the same base model, indicated by their shared naming convention.\nrelation<|#|>21/117B<|#|>GLM-4.5V 106B<|#|>pairing, association<|#|>These entities are listed together as a pair within the same entry.\nrelation<|#|>O3-Pro<|#|>Gemini 2.5 Flash<|#|>pairing, association<|#|>These entities are listed together as a pair within the same entry.\nrelation<|#|>Gemini 2.5 Pro<|#|>Seed-Thinking 1.6<|#|>pairing, association<|#|>These entities are listed together as a pair within the same entry.\nrelation<|#|>O3-Pro<|#|>Gemini 2.5 Pro<|#|>comparison, grouping<|#|>These models are presented in a comparative grouping within the same list structure.\nrelation<|#|>O3-Pro<|#|>Seed-Thinking 1.6<|#|>comparison, grouping<|#|>These models are presented in a comparative grouping within the same list structure.\nrelation<|#|>Gemini 2.5 Flash<|#|>Seed-Thinking 1.6<|#|>comparison, grouping<|#|>These models are presented in a comparative grouping within the same list structure.\n<|COMPLETE|>",
    "cache_type": "extract",
    "chunk_id": "chunk-519d56991b1bbd2046e61bcd26303d2e",
    "original_prompt": "---Task---\nExtract entities and relationships from the input text to be processed.\n\n---Instructions---\n1.  **Strict Adherence to Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system prompt.\n2.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\n3.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant entities and relationships have been extracted and presented.\n4.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\n\n<Output>\n---Role---\nYou are a Knowledge Graph Specialist responsible for extracting entities and relationships from the input text.\n\n---Instructions---\n1.  **Entity Extraction & Output:**\n    *   **Identification:** Identify clearly defined and meaningful entities in the input text.\n    *   **Entity Details:** For each identified entity, extract the following information:\n        *   `entity_name`: The name of the entity. If the entity name is case-insensitive, capitalize the first letter of each significant word (title case). Ensure **consistent naming** across the entire extraction process.\n        *   `entity_type`: Categorize the entity using one of the following types: `Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject`. If none of the provided entity types apply, do not add new entity type and classify it as `Other`.\n        *   `entity_description`: Provide a concise yet comprehensive description of the entity's attributes and activities, based *solely* on the information present in the input text.\n    *   **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n        *   Format: `entity<|#|>entity_name<|#|>entity_type<|#|>entity_description`\n\n2.  **Relationship Extraction & Output:**\n    *   **Identification:** Identify direct, clearly stated, and meaningful relationships between previously extracted entities.\n    *   **N-ary Relationship Decomposition:** If a single statement describes a relationship involving more than two entities (an N-ary relationship), decompose it into multiple binary (two-entity) relationship pairs for separate description.\n        *   **Example:** For \"Alice, Bob, and Carol collaborated on Project X,\" extract binary relationships such as \"Alice collaborated with Project X,\" \"Bob collaborated with Project X,\" and \"Carol collaborated with Project X,\" or \"Alice collaborated with Bob,\" based on the most reasonable binary interpretations.\n    *   **Relationship Details:** For each binary relationship, extract the following fields:\n        *   `source_entity`: The name of the source entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `target_entity`: The name of the target entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `relationship_keywords`: One or more high-level keywords summarizing the overarching nature, concepts, or themes of the relationship. Multiple keywords within this field must be separated by a comma `,`. **DO NOT use `<|#|>` for separating multiple keywords within this field.**\n        *   `relationship_description`: A concise explanation of the nature of the relationship between the source and target entities, providing a clear rationale for their connection.\n    *   **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n        *   Format: `relation<|#|>source_entity<|#|>target_entity<|#|>relationship_keywords<|#|>relationship_description`\n\n3.  **Delimiter Usage Protocol:**\n    *   The `<|#|>` is a complete, atomic marker and **must not be filled with content**. It serves strictly as a field separator.\n    *   **Incorrect Example:** `entity<|#|>Tokyo<|location|>Tokyo is the capital of Japan.`\n    *   **Correct Example:** `entity<|#|>Tokyo<|#|>location<|#|>Tokyo is the capital of Japan.`\n\n4.  **Relationship Direction & Duplication:**\n    *   Treat all relationships as **undirected** unless explicitly stated otherwise. Swapping the source and target entities for an undirected relationship does not constitute a new relationship.\n    *   Avoid outputting duplicate relationships.\n\n5.  **Output Order & Prioritization:**\n    *   Output all extracted entities first, followed by all extracted relationships.\n    *   Within the list of relationships, prioritize and output those relationships that are **most significant** to the core meaning of the input text first.\n\n6.  **Context & Objectivity:**\n    *   Ensure all entity names and descriptions are written in the **third person**.\n    *   Explicitly name the subject or object; **avoid using pronouns** such as `this article`, `this paper`, `our company`, `I`, `you`, and `he/she`.\n\n7.  **Language & Proper Nouns:**\n    *   The entire output (entity names, keywords, and descriptions) must be written in `English`.\n    *   Proper nouns (e.g., personal names, place names, organization names) should be retained in their original language if a proper, widely accepted translation is not available or would cause ambiguity.\n\n8.  **Completion Signal:** Output the literal string `<|COMPLETE|>` only after all entities and relationships, following all criteria, have been completely extracted and outputted.\n\n---Examples---\n<Input Text>\n```\nwhile Alex clenched his jaw, the buzz of frustration dull against the backdrop of Taylor's authoritarian certainty. It was this competitive undercurrent that kept him alert, the sense that his and Jordan's shared commitment to discovery was an unspoken rebellion against Cruz's narrowing vision of control and order.\n\nThen Taylor did something unexpected. They paused beside Jordan and, for a moment, observed the device with something akin to reverence. \"If this tech can be understood...\" Taylor said, their voice quieter, \"It could change the game for us. For all of us.\"\n\nThe underlying dismissal earlier seemed to falter, replaced by a glimpse of reluctant respect for the gravity of what lay in their hands. Jordan looked up, and for a fleeting heartbeat, their eyes locked with Taylor's, a wordless clash of wills softening into an uneasy truce.\n\nIt was a small transformation, barely perceptible, but one that Alex noted with an inward nod. They had all been brought here by different paths\n```\n\n<Output>\nentity<|#|>Alex<|#|>person<|#|>Alex is a character who experiences frustration and is observant of the dynamics among other characters.\nentity<|#|>Taylor<|#|>person<|#|>Taylor is portrayed with authoritarian certainty and shows a moment of reverence towards a device, indicating a change in perspective.\nentity<|#|>Jordan<|#|>person<|#|>Jordan shares a commitment to discovery and has a significant interaction with Taylor regarding a device.\nentity<|#|>Cruz<|#|>person<|#|>Cruz is associated with a vision of control and order, influencing the dynamics among other characters.\nentity<|#|>The Device<|#|>equipment<|#|>The Device is central to the story, with potential game-changing implications, and is revered by Taylor.\nrelation<|#|>Alex<|#|>Taylor<|#|>power dynamics, observation<|#|>Alex observes Taylor's authoritarian behavior and notes changes in Taylor's attitude toward the device.\nrelation<|#|>Alex<|#|>Jordan<|#|>shared goals, rebellion<|#|>Alex and Jordan share a commitment to discovery, which contrasts with Cruz's vision.)\nrelation<|#|>Taylor<|#|>Jordan<|#|>conflict resolution, mutual respect<|#|>Taylor and Jordan interact directly regarding the device, leading to a moment of mutual respect and an uneasy truce.\nrelation<|#|>Jordan<|#|>Cruz<|#|>ideological conflict, rebellion<|#|>Jordan's commitment to discovery is in rebellion against Cruz's vision of control and order.\nrelation<|#|>Taylor<|#|>The Device<|#|>reverence, technological significance<|#|>Taylor shows reverence towards the device, indicating its importance and potential impact.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nStock markets faced a sharp downturn today as tech giants saw significant declines, with the global tech index dropping by 3.4% in midday trading. Analysts attribute the selloff to investor concerns over rising interest rates and regulatory uncertainty.\n\nAmong the hardest hit, nexon technologies saw its stock plummet by 7.8% after reporting lower-than-expected quarterly earnings. In contrast, Omega Energy posted a modest 2.1% gain, driven by rising oil prices.\n\nMeanwhile, commodity markets reflected a mixed sentiment. Gold futures rose by 1.5%, reaching $2,080 per ounce, as investors sought safe-haven assets. Crude oil prices continued their rally, climbing to $87.60 per barrel, supported by supply constraints and strong demand.\n\nFinancial experts are closely watching the Federal Reserve's next move, as speculation grows over potential rate hikes. The upcoming policy announcement is expected to influence investor confidence and overall market stability.\n```\n\n<Output>\nentity<|#|>Global Tech Index<|#|>category<|#|>The Global Tech Index tracks the performance of major technology stocks and experienced a 3.4% decline today.\nentity<|#|>Nexon Technologies<|#|>organization<|#|>Nexon Technologies is a tech company that saw its stock decline by 7.8% after disappointing earnings.\nentity<|#|>Omega Energy<|#|>organization<|#|>Omega Energy is an energy company that gained 2.1% in stock value due to rising oil prices.\nentity<|#|>Gold Futures<|#|>product<|#|>Gold futures rose by 1.5%, indicating increased investor interest in safe-haven assets.\nentity<|#|>Crude Oil<|#|>product<|#|>Crude oil prices rose to $87.60 per barrel due to supply constraints and strong demand.\nentity<|#|>Market Selloff<|#|>category<|#|>Market selloff refers to the significant decline in stock values due to investor concerns over interest rates and regulations.\nentity<|#|>Federal Reserve Policy Announcement<|#|>category<|#|>The Federal Reserve's upcoming policy announcement is expected to impact investor confidence and market stability.\nentity<|#|>3.4% Decline<|#|>category<|#|>The Global Tech Index experienced a 3.4% decline in midday trading.\nrelation<|#|>Global Tech Index<|#|>Market Selloff<|#|>market performance, investor sentiment<|#|>The decline in the Global Tech Index is part of the broader market selloff driven by investor concerns.\nrelation<|#|>Nexon Technologies<|#|>Global Tech Index<|#|>company impact, index movement<|#|>Nexon Technologies' stock decline contributed to the overall drop in the Global Tech Index.\nrelation<|#|>Gold Futures<|#|>Market Selloff<|#|>market reaction, safe-haven investment<|#|>Gold prices rose as investors sought safe-haven assets during the market selloff.\nrelation<|#|>Federal Reserve Policy Announcement<|#|>Market Selloff<|#|>interest rate impact, financial regulation<|#|>Speculation over Federal Reserve policy changes contributed to market volatility and investor selloff.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nAt the World Athletics Championship in Tokyo, Noah Carter broke the 100m sprint record using cutting-edge carbon-fiber spikes.\n```\n\n<Output>\nentity<|#|>World Athletics Championship<|#|>event<|#|>The World Athletics Championship is a global sports competition featuring top athletes in track and field.\nentity<|#|>Tokyo<|#|>location<|#|>Tokyo is the host city of the World Athletics Championship.\nentity<|#|>Noah Carter<|#|>person<|#|>Noah Carter is a sprinter who set a new record in the 100m sprint at the World Athletics Championship.\nentity<|#|>100m Sprint Record<|#|>category<|#|>The 100m sprint record is a benchmark in athletics, recently broken by Noah Carter.\nentity<|#|>Carbon-Fiber Spikes<|#|>equipment<|#|>Carbon-fiber spikes are advanced sprinting shoes that provide enhanced speed and traction.\nentity<|#|>World Athletics Federation<|#|>organization<|#|>The World Athletics Federation is the governing body overseeing the World Athletics Championship and record validations.\nrelation<|#|>World Athletics Championship<|#|>Tokyo<|#|>event location, international competition<|#|>The World Athletics Championship is being hosted in Tokyo.\nrelation<|#|>Noah Carter<|#|>100m Sprint Record<|#|>athlete achievement, record-breaking<|#|>Noah Carter set a new 100m sprint record at the championship.\nrelation<|#|>Noah Carter<|#|>Carbon-Fiber Spikes<|#|>athletic equipment, performance boost<|#|>Noah Carter used carbon-fiber spikes to enhance performance during the race.\nrelation<|#|>Noah Carter<|#|>World Athletics Championship<|#|>athlete participation, competition<|#|>Noah Carter is competing at the World Athletics Championship.\n<|COMPLETE|>\n\n\n\n---Real Data to be Processed---\n<Input>\nEntity_types: [Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject]\nText:\n```\n21/117B\\nGLM-4.5V 106B\"\n    ]\n  ],\n  [\n    [\n      \"o3-pro\\nGemini 2.5 Flash\"\n    ],\n    [\n      \"Gemini 2.5 Pro\\nSeed-Thinking 1.6\"\n    ]\n  ]\n]\n```",
    "queryparam": null,
    "create_time": 1765219076,
    "update_time": 1765219076,
    "_id": "default:extract:8175eaea69f695b6053aff549e89eae7"
  },
  "default:extract:6e3392eac6eea6ef3f150be7954bfe6a": {
    "return": "<|COMPLETE|>",
    "cache_type": "extract",
    "chunk_id": "chunk-519d56991b1bbd2046e61bcd26303d2e",
    "original_prompt": "---Task---\nBased on the last extraction task, identify and extract any **missed or incorrectly formatted** entities and relationships from the input text.\n\n---Instructions---\n1.  **Strict Adherence to System Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system instructions.\n2.  **Focus on Corrections/Additions:**\n    *   **Do NOT** re-output entities and relationships that were **correctly and fully** extracted in the last task.\n    *   If an entity or relationship was **missed** in the last task, extract and output it now according to the system format.\n    *   If an entity or relationship was **truncated, had missing fields, or was otherwise incorrectly formatted** in the last task, re-output the *corrected and complete* version in the specified format.\n3.  **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n4.  **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n5.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\n6.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant missing or corrected entities and relationships have been extracted and presented.\n7.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\n\n<Output>\n---Role---\nYou are a Knowledge Graph Specialist responsible for extracting entities and relationships from the input text.\n\n---Instructions---\n1.  **Entity Extraction & Output:**\n    *   **Identification:** Identify clearly defined and meaningful entities in the input text.\n    *   **Entity Details:** For each identified entity, extract the following information:\n        *   `entity_name`: The name of the entity. If the entity name is case-insensitive, capitalize the first letter of each significant word (title case). Ensure **consistent naming** across the entire extraction process.\n        *   `entity_type`: Categorize the entity using one of the following types: `Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject`. If none of the provided entity types apply, do not add new entity type and classify it as `Other`.\n        *   `entity_description`: Provide a concise yet comprehensive description of the entity's attributes and activities, based *solely* on the information present in the input text.\n    *   **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n        *   Format: `entity<|#|>entity_name<|#|>entity_type<|#|>entity_description`\n\n2.  **Relationship Extraction & Output:**\n    *   **Identification:** Identify direct, clearly stated, and meaningful relationships between previously extracted entities.\n    *   **N-ary Relationship Decomposition:** If a single statement describes a relationship involving more than two entities (an N-ary relationship), decompose it into multiple binary (two-entity) relationship pairs for separate description.\n        *   **Example:** For \"Alice, Bob, and Carol collaborated on Project X,\" extract binary relationships such as \"Alice collaborated with Project X,\" \"Bob collaborated with Project X,\" and \"Carol collaborated with Project X,\" or \"Alice collaborated with Bob,\" based on the most reasonable binary interpretations.\n    *   **Relationship Details:** For each binary relationship, extract the following fields:\n        *   `source_entity`: The name of the source entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `target_entity`: The name of the target entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `relationship_keywords`: One or more high-level keywords summarizing the overarching nature, concepts, or themes of the relationship. Multiple keywords within this field must be separated by a comma `,`. **DO NOT use `<|#|>` for separating multiple keywords within this field.**\n        *   `relationship_description`: A concise explanation of the nature of the relationship between the source and target entities, providing a clear rationale for their connection.\n    *   **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n        *   Format: `relation<|#|>source_entity<|#|>target_entity<|#|>relationship_keywords<|#|>relationship_description`\n\n3.  **Delimiter Usage Protocol:**\n    *   The `<|#|>` is a complete, atomic marker and **must not be filled with content**. It serves strictly as a field separator.\n    *   **Incorrect Example:** `entity<|#|>Tokyo<|location|>Tokyo is the capital of Japan.`\n    *   **Correct Example:** `entity<|#|>Tokyo<|#|>location<|#|>Tokyo is the capital of Japan.`\n\n4.  **Relationship Direction & Duplication:**\n    *   Treat all relationships as **undirected** unless explicitly stated otherwise. Swapping the source and target entities for an undirected relationship does not constitute a new relationship.\n    *   Avoid outputting duplicate relationships.\n\n5.  **Output Order & Prioritization:**\n    *   Output all extracted entities first, followed by all extracted relationships.\n    *   Within the list of relationships, prioritize and output those relationships that are **most significant** to the core meaning of the input text first.\n\n6.  **Context & Objectivity:**\n    *   Ensure all entity names and descriptions are written in the **third person**.\n    *   Explicitly name the subject or object; **avoid using pronouns** such as `this article`, `this paper`, `our company`, `I`, `you`, and `he/she`.\n\n7.  **Language & Proper Nouns:**\n    *   The entire output (entity names, keywords, and descriptions) must be written in `English`.\n    *   Proper nouns (e.g., personal names, place names, organization names) should be retained in their original language if a proper, widely accepted translation is not available or would cause ambiguity.\n\n8.  **Completion Signal:** Output the literal string `<|COMPLETE|>` only after all entities and relationships, following all criteria, have been completely extracted and outputted.\n\n---Examples---\n<Input Text>\n```\nwhile Alex clenched his jaw, the buzz of frustration dull against the backdrop of Taylor's authoritarian certainty. It was this competitive undercurrent that kept him alert, the sense that his and Jordan's shared commitment to discovery was an unspoken rebellion against Cruz's narrowing vision of control and order.\n\nThen Taylor did something unexpected. They paused beside Jordan and, for a moment, observed the device with something akin to reverence. \"If this tech can be understood...\" Taylor said, their voice quieter, \"It could change the game for us. For all of us.\"\n\nThe underlying dismissal earlier seemed to falter, replaced by a glimpse of reluctant respect for the gravity of what lay in their hands. Jordan looked up, and for a fleeting heartbeat, their eyes locked with Taylor's, a wordless clash of wills softening into an uneasy truce.\n\nIt was a small transformation, barely perceptible, but one that Alex noted with an inward nod. They had all been brought here by different paths\n```\n\n<Output>\nentity<|#|>Alex<|#|>person<|#|>Alex is a character who experiences frustration and is observant of the dynamics among other characters.\nentity<|#|>Taylor<|#|>person<|#|>Taylor is portrayed with authoritarian certainty and shows a moment of reverence towards a device, indicating a change in perspective.\nentity<|#|>Jordan<|#|>person<|#|>Jordan shares a commitment to discovery and has a significant interaction with Taylor regarding a device.\nentity<|#|>Cruz<|#|>person<|#|>Cruz is associated with a vision of control and order, influencing the dynamics among other characters.\nentity<|#|>The Device<|#|>equipment<|#|>The Device is central to the story, with potential game-changing implications, and is revered by Taylor.\nrelation<|#|>Alex<|#|>Taylor<|#|>power dynamics, observation<|#|>Alex observes Taylor's authoritarian behavior and notes changes in Taylor's attitude toward the device.\nrelation<|#|>Alex<|#|>Jordan<|#|>shared goals, rebellion<|#|>Alex and Jordan share a commitment to discovery, which contrasts with Cruz's vision.)\nrelation<|#|>Taylor<|#|>Jordan<|#|>conflict resolution, mutual respect<|#|>Taylor and Jordan interact directly regarding the device, leading to a moment of mutual respect and an uneasy truce.\nrelation<|#|>Jordan<|#|>Cruz<|#|>ideological conflict, rebellion<|#|>Jordan's commitment to discovery is in rebellion against Cruz's vision of control and order.\nrelation<|#|>Taylor<|#|>The Device<|#|>reverence, technological significance<|#|>Taylor shows reverence towards the device, indicating its importance and potential impact.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nStock markets faced a sharp downturn today as tech giants saw significant declines, with the global tech index dropping by 3.4% in midday trading. Analysts attribute the selloff to investor concerns over rising interest rates and regulatory uncertainty.\n\nAmong the hardest hit, nexon technologies saw its stock plummet by 7.8% after reporting lower-than-expected quarterly earnings. In contrast, Omega Energy posted a modest 2.1% gain, driven by rising oil prices.\n\nMeanwhile, commodity markets reflected a mixed sentiment. Gold futures rose by 1.5%, reaching $2,080 per ounce, as investors sought safe-haven assets. Crude oil prices continued their rally, climbing to $87.60 per barrel, supported by supply constraints and strong demand.\n\nFinancial experts are closely watching the Federal Reserve's next move, as speculation grows over potential rate hikes. The upcoming policy announcement is expected to influence investor confidence and overall market stability.\n```\n\n<Output>\nentity<|#|>Global Tech Index<|#|>category<|#|>The Global Tech Index tracks the performance of major technology stocks and experienced a 3.4% decline today.\nentity<|#|>Nexon Technologies<|#|>organization<|#|>Nexon Technologies is a tech company that saw its stock decline by 7.8% after disappointing earnings.\nentity<|#|>Omega Energy<|#|>organization<|#|>Omega Energy is an energy company that gained 2.1% in stock value due to rising oil prices.\nentity<|#|>Gold Futures<|#|>product<|#|>Gold futures rose by 1.5%, indicating increased investor interest in safe-haven assets.\nentity<|#|>Crude Oil<|#|>product<|#|>Crude oil prices rose to $87.60 per barrel due to supply constraints and strong demand.\nentity<|#|>Market Selloff<|#|>category<|#|>Market selloff refers to the significant decline in stock values due to investor concerns over interest rates and regulations.\nentity<|#|>Federal Reserve Policy Announcement<|#|>category<|#|>The Federal Reserve's upcoming policy announcement is expected to impact investor confidence and market stability.\nentity<|#|>3.4% Decline<|#|>category<|#|>The Global Tech Index experienced a 3.4% decline in midday trading.\nrelation<|#|>Global Tech Index<|#|>Market Selloff<|#|>market performance, investor sentiment<|#|>The decline in the Global Tech Index is part of the broader market selloff driven by investor concerns.\nrelation<|#|>Nexon Technologies<|#|>Global Tech Index<|#|>company impact, index movement<|#|>Nexon Technologies' stock decline contributed to the overall drop in the Global Tech Index.\nrelation<|#|>Gold Futures<|#|>Market Selloff<|#|>market reaction, safe-haven investment<|#|>Gold prices rose as investors sought safe-haven assets during the market selloff.\nrelation<|#|>Federal Reserve Policy Announcement<|#|>Market Selloff<|#|>interest rate impact, financial regulation<|#|>Speculation over Federal Reserve policy changes contributed to market volatility and investor selloff.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nAt the World Athletics Championship in Tokyo, Noah Carter broke the 100m sprint record using cutting-edge carbon-fiber spikes.\n```\n\n<Output>\nentity<|#|>World Athletics Championship<|#|>event<|#|>The World Athletics Championship is a global sports competition featuring top athletes in track and field.\nentity<|#|>Tokyo<|#|>location<|#|>Tokyo is the host city of the World Athletics Championship.\nentity<|#|>Noah Carter<|#|>person<|#|>Noah Carter is a sprinter who set a new record in the 100m sprint at the World Athletics Championship.\nentity<|#|>100m Sprint Record<|#|>category<|#|>The 100m sprint record is a benchmark in athletics, recently broken by Noah Carter.\nentity<|#|>Carbon-Fiber Spikes<|#|>equipment<|#|>Carbon-fiber spikes are advanced sprinting shoes that provide enhanced speed and traction.\nentity<|#|>World Athletics Federation<|#|>organization<|#|>The World Athletics Federation is the governing body overseeing the World Athletics Championship and record validations.\nrelation<|#|>World Athletics Championship<|#|>Tokyo<|#|>event location, international competition<|#|>The World Athletics Championship is being hosted in Tokyo.\nrelation<|#|>Noah Carter<|#|>100m Sprint Record<|#|>athlete achievement, record-breaking<|#|>Noah Carter set a new 100m sprint record at the championship.\nrelation<|#|>Noah Carter<|#|>Carbon-Fiber Spikes<|#|>athletic equipment, performance boost<|#|>Noah Carter used carbon-fiber spikes to enhance performance during the race.\nrelation<|#|>Noah Carter<|#|>World Athletics Championship<|#|>athlete participation, competition<|#|>Noah Carter is competing at the World Athletics Championship.\n<|COMPLETE|>\n\n\n\n---Real Data to be Processed---\n<Input>\nEntity_types: [Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject]\nText:\n```\n21/117B\\nGLM-4.5V 106B\"\n    ]\n  ],\n  [\n    [\n      \"o3-pro\\nGemini 2.5 Flash\"\n    ],\n    [\n      \"Gemini 2.5 Pro\\nSeed-Thinking 1.6\"\n    ]\n  ]\n]\n```\n[{\"role\": \"user\", \"content\": \"---Task---\\nExtract entities and relationships from the input text to be processed.\\n\\n---Instructions---\\n1.  **Strict Adherence to Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system prompt.\\n2.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\\n3.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant entities and relationships have been extracted and presented.\\n4.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\\n\\n<Output>\"}, {\"role\": \"assistant\", \"content\": \"entity<|#|>21/117B<|#|>Artifact<|#|>An identifier or model name listed in the input text.\\nentity<|#|>GLM-4.5V 106B<|#|>Artifact<|#|>A named model or system listed in the input text.\\nentity<|#|>O3-Pro<|#|>Artifact<|#|>A named model or system listed in the input text.\\nentity<|#|>Gemini 2.5 Flash<|#|>Artifact<|#|>A named model or system, likely a version of the Gemini series, listed in the input text.\\nentity<|#|>Gemini 2.5 Pro<|#|>Artifact<|#|>A named model or system, likely a version of the Gemini series, listed in the input text.\\nentity<|#|>Seed-Thinking 1.6<|#|>Artifact<|#|>A named model or system listed in the input text.\\nrelation<|#|>Gemini 2.5 Flash<|#|>Gemini 2.5 Pro<|#|>version, variant<|#|>These entities are different versions of the same base model, indicated by their shared naming convention.\\nrelation<|#|>21/117B<|#|>GLM-4.5V 106B<|#|>pairing, association<|#|>These entities are listed together as a pair within the same entry.\\nrelation<|#|>O3-Pro<|#|>Gemini 2.5 Flash<|#|>pairing, association<|#|>These entities are listed together as a pair within the same entry.\\nrelation<|#|>Gemini 2.5 Pro<|#|>Seed-Thinking 1.6<|#|>pairing, association<|#|>These entities are listed together as a pair within the same entry.\\nrelation<|#|>O3-Pro<|#|>Gemini 2.5 Pro<|#|>comparison, grouping<|#|>These models are presented in a comparative grouping within the same list structure.\\nrelation<|#|>O3-Pro<|#|>Seed-Thinking 1.6<|#|>comparison, grouping<|#|>These models are presented in a comparative grouping within the same list structure.\\nrelation<|#|>Gemini 2.5 Flash<|#|>Seed-Thinking 1.6<|#|>comparison, grouping<|#|>These models are presented in a comparative grouping within the same list structure.\\n<|COMPLETE|>\"}]",
    "queryparam": null,
    "create_time": 1765219087,
    "update_time": 1765219087,
    "_id": "default:extract:6e3392eac6eea6ef3f150be7954bfe6a"
  },
  "default:extract:291f979159ceeca21bb8749b9eee20fb": {
    "return": "entity<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Content<|#|>This document is a survey focusing on the application of Reinforcement Learning for Large Reasoning Models.\nentity<|#|>Reinforcement Learning<|#|>Concept<|#|>Reinforcement Learning (RL) is a key method used in training various models, including large reasoning models.\nentity<|#|>Large Reasoning Models<|#|>Concept<|#|>Large Reasoning Models are a category of models that benefit from Reinforcement Learning, including language, multimodal, and agentic models.\nentity<|#|>LLMs<|#|>Concept<|#|>Large Language Models (LLMs) are a category of models whose architectures and applications are surveyed, and whose emerging capabilities include long chain-of-thought reasoning and adaptive behaviors.\nentity<|#|>GPT-5<|#|>Artifact<|#|>GPT-5 is a representative reasoning model.\nentity<|#|>Claude Opus 4.1<|#|>Artifact<|#|>Claude Opus 4.1 is a representative reasoning model.\nentity<|#|>DeepSeek-V3.1 671B<|#|>Artifact<|#|>DeepSeek-V3.1 671B is a representative reasoning model.\nentity<|#|>Multimodal o3-pro<|#|>Artifact<|#|>Multimodal o3-pro is a representative reasoning model.\nentity<|#|>gpt-oss 21/117B<|#|>Artifact<|#|>gpt-oss 21/117B is a representative reasoning model.\nentity<|#|>Gemini 2.5 Flash<|#|>Artifact<|#|>Gemini 2.5 Flash is a representative reasoning model.\nentity<|#|>GLM-4.5V 106B<|#|>Artifact<|#|>GLM-4.5V 106B is a specific variant of the GLM-4.5V reasoning model.\nentity<|#|>Gemini 2.5 Pro<|#|>Artifact<|#|>Gemini 2.5 Pro is a representative reasoning model.\nentity<|#|>InternVL3.5 1-241B<|#|>Artifact<|#|>InternVL3.5 1-241B is a specific variant of the InternVL3.5 reasoning model.\nentity<|#|>Seed-Thinking 1.6<|#|>Artifact<|#|>Seed-Thinking 1.6 is a representative reasoning model.\nentity<|#|>o3 Magistral 24B<|#|>Artifact<|#|>o3 Magistral 24B is a representative reasoning model.\nentity<|#|>o4-mini<|#|>Artifact<|#|>o4-mini is a representative reasoning model.\nentity<|#|>Minimax-M1 456B<|#|>Artifact<|#|>Minimax-M1 456B is a representative reasoning model.\nentity<|#|>Ring-mini-2.0 16B<|#|>Artifact<|#|>Ring-mini-2.0 16B is a representative reasoning model.\nentity<|#|>Seed-Thinking 1.5<|#|>Artifact<|#|>Seed-Thinking 1.5 is a representative reasoning model.\nentity<|#|>Qwen 3-Next-80B-A3B-Thinking 80B<|#|>Artifact<|#|>Qwen 3-Next-80B-A3B-Thinking 80B is a representative reasoning model.\nentity<|#|>Phi-4 Reasoning 14B<|#|>Artifact<|#|>Phi-4 Reasoning 14B is a representative reasoning model.\nentity<|#|>Gemini 2.5 Flash-Lite<|#|>Artifact<|#|>Gemini 2.5 Flash-Lite is a representative reasoning model.\nentity<|#|>Skywork-R1V2 38B<|#|>Artifact<|#|>Skywork-R1V2 38B is a representative reasoning model.\nentity<|#|>Grok4<|#|>Artifact<|#|>Grok4 is a representative reasoning model.\nentity<|#|>InternVL3 1-78B<|#|>Artifact<|#|>InternVL3 1-78B is a representative reasoning model.\nentity<|#|>Intern-S 1241B<|#|>Artifact<|#|>Intern-S 1241B is a specific variant of the Intern-S1 reasoning model.\nentity<|#|>MiMo 7B<|#|>Artifact<|#|>MiMo 7B is a representative reasoning model.\nentity<|#|>Kimik 21T<|#|>Artifact<|#|>Kimik 21T is a representative reasoning model.\nentity<|#|>Claude 3.7 Sonnet<|#|>Artifact<|#|>Claude 3.7 Sonnet is a representative reasoning model.\nentity<|#|>Qwen3 0.6-235B<|#|>Artifact<|#|>Qwen3 0.6-235B is a representative reasoning model.\nentity<|#|>Hunyuan-TurboS Step 3321B<|#|>Artifact<|#|>Hunyuan-TurboS Step 3321B is a specific variant of the Step3 reasoning model.\nentity<|#|>o3-mini<|#|>Artifact<|#|>o3-mini is a representative reasoning model.\nentity<|#|>Claude 4<|#|>Artifact<|#|>Claude 4 is a representative reasoning model.\nentity<|#|>Qwen3-2507-Thinking4-235B<|#|>Artifact<|#|>Qwen3-2507-Thinking4-235B is a representative reasoning model.\nentity<|#|>o1-preview<|#|>Artifact<|#|>o1-preview is a representative reasoning model.\nentity<|#|>Gemini 2.0 Flash<|#|>Artifact<|#|>Gemini 2.0 Flash is a representative reasoning model.\nentity<|#|>Skywork OR-1 7/32B<|#|>Artifact<|#|>Skywork OR-1 7/32B is a representative reasoning model.\nentity<|#|>GLM-4.1V-Thinking9B<|#|>Artifact<|#|>GLM-4.1V-Thinking9B is a representative reasoning model.\nentity<|#|>o1-mini<|#|>Artifact<|#|>o1-mini is a representative reasoning model.\nentity<|#|>o1-2024-12-17<|#|>Artifact<|#|>o1-2024-12-17 is a representative reasoning model.\nentity<|#|>INTELLECT-2 32B<|#|>Artifact<|#|>INTELLECT-2 32B is a representative reasoning model.\nentity<|#|>GLM-4.5355B<|#|>Artifact<|#|>GLM-4.5355B is a representative reasoning model.\nentity<|#|>Llama-Nemotron-Ultra 253B<|#|>Artifact<|#|>Llama-Nemotron-Ultra 253B is a representative reasoning model.\nentity<|#|>Skywork-R1V3 38B<|#|>Artifact<|#|>Skywork-R1V3 38B is a representative reasoning model.\nentity<|#|>QVQ-Max<|#|>Artifact<|#|>QVQ-Max is a representative reasoning model.\nentity<|#|>DeepSeek-R1-0528 671B<|#|>Artifact<|#|>DeepSeek-R1-0528 671B is a specific variant of the DeepSeek-R1 reasoning model.\nentity<|#|>ORZ 0.5-32B<|#|>Artifact<|#|>ORZ 0.5-32B is a representative reasoning model.\nentity<|#|>QWQ 32B<|#|>Artifact<|#|>QWQ 32B is a representative reasoning model.\nentity<|#|>Kimi1.5<|#|>Artifact<|#|>Kimi1.5 is a representative reasoning model.\nentity<|#|>DeepSeek-R1 671B<|#|>Artifact<|#|>DeepSeek-R1 671B is a specific variant of the DeepSeek-R1 reasoning model.\nentity<|#|>InternVL3.5<|#|>Artifact<|#|>InternVL3.5 is a reasoning model that used a unified native multimodal pretraining phase and a two-stage cascade RL framework, achieving improved efficiency and versatility.\nentity<|#|>Intern-S1<|#|>Artifact<|#|>Intern-S1 is a reasoning model that focused on multimodal scientific reasoning across diverse domains, benefiting from a mixture-of-rewards design during online RL.\nentity<|#|>Step3<|#|>Artifact<|#|>Step3 is a reasoning model designed for efficient training and minimizing decoding costs.\nentity<|#|>GLM-4.5V<|#|>Artifact<|#|>GLM-4.5V is a reasoning model with state-of-the-art performance across most visual multimodal benchmarks.\nentity<|#|>DeepSeek-R1<|#|>Artifact<|#|>DeepSeek-R1 is a reasoning model that is the subject of replication studies.\nentity<|#|>Figure 4<|#|>Content<|#|>Figure 4 is a timeline illustrating representative open-source and closed-source reasoning models trained with Reinforcement Learning.\nentity<|#|>Table 1<|#|>Content<|#|>Table 1 provides detailed information on open-source models.\nentity<|#|>Unified Native Multimodal Pretraining Phase<|#|>Method<|#|>This method was used by InternVL3.5 for improved efficiency and versatility.\nentity<|#|>Two-Stage Cascade RL Framework<|#|>Method<|#|>This framework was used by InternVL3.5, achieving improved efficiency and versatility.\nentity<#|>Mixture-of-Rewards Design<|#|>Method<|#|>This design was used during online RL for the Intern-S1 model to facilitate simultaneous training on a wide range of tasks.\nentity<|#|>Online RL<|#|>Method<|#|>Online RL is a method of Reinforcement Learning used in models like Intern-S1.\nentity<|#|>Efficient Training<|#|>Concept<|#|>Efficient training is a design goal for models like Step3.\nentity<|#|>Minimizing Decoding Costs<|#|>Concept<|#|>Minimizing decoding costs is a design goal for models like Step3.\nentity<|#|>Visual Multimodal Benchmarks<|#|>Concept<|#|>Visual multimodal benchmarks are used to evaluate the performance of models like GLM-4.5V.\nentity<|#|>Classical RL<|#|>Concept<|#|>Classical RL refers to traditional Reinforcement Learning approaches.\nentity<|#|>Multi-Agent RL<|#|>Concept<|#|>Multi-Agent RL is a specific area of Reinforcement Learning.\nentity<|#|>Self-Play Techniques<|#|>Method<|#|>Self-play techniques are reviewed in the context of Reinforcement Learning.\nentity<|#|>RL in Computer Vision Tasks<|#|>Concept<|#|>RL in Computer Vision Tasks is a specific application area of Reinforcement Learning.\nentity<|#|>LLM Architectures<|#|>Concept<|#|>LLM Architectures refer to the structural designs of Large Language Models.\nentity<|#|>Long Chain-of-Thought Reasoning<|#|>Concept<|#|>Long Chain-of-Thought Reasoning is an emerging capability of LLMs.\nentity<|#|>Adaptive Behaviors<|#|>Concept<|#|>Adaptive behaviors are an emerging capability of LLMs.\nentity<|#|>RLHF<|#|>Method<|#|>RLHF (Reinforcement Learning from Human Feedback) is an RL-based method useful for reasoning LLMs.\nentity<|#|>RLVR<|#|>Method<|#|>RLVR (Reinforcement Learning from Visual Reasoning) is an RL-based method useful for reasoning LLMs.\nentity<|#|>System 1 Reasoning<|#|>Concept<|#|>System 1 Reasoning is a type of reasoning analyzed in relation to LLMs.\nentity<|#|>System 2 Reasoning<|#|>Concept<|#|>System 2 Reasoning is a type of reasoning analyzed in relation to LLMs.\nentity<|#|>Reasoning via Foundation Models<|#|>Concept<|#|>Reasoning via Foundation Models is a structured approach to reasoning.\nentity<|#|>Language Models<|#|>Concept<|#|>Language Models are a type of reasoning model trained with RL.\nentity<|#|>Multimodal Models<|#|>Concept<|#|>Multimodal Models are a type of reasoning model trained with RL.\nentity<|#|>Agentic Models<|#|>Concept<|#|>Agentic Models are a type of reasoning model trained with RL.\nentity<|#|>Multimodal Scientific Reasoning<|#|>Concept<|#|>Multimodal scientific reasoning is a focus area for models like Intern-S1.\nentity<|#|>Wang et al. [2025o]<|#|>Organization<|#|>Wang et al. [2025o] are researchers associated with the InternVL3.5 model.\nentity<|#|>Bai et al. [2025]<|#|>Organization<|#|>Bai et al. [2025] are researchers associated with the Intern-S1 model.\nentity<|#|>Wang et al. [2025a]<|#|>Organization<|#|>Wang et al. [2025a] are researchers associated with the Step3 model.\nentity<|#|>Team et al. [2025a]<|#|>Organization<|#|>Team et al. [2025a] are researchers associated with the GLM-4.5V model.\nentity<|#|>Ghasemi et al. [2024]<|#|>Organization<|#|>Ghasemi et al. [2024] authored a general RL survey.\nentity<|#|>Huh and Mohapatra [2023]<|#|>Organization<|#|>Huh and Mohapatra [2023] authored a survey focusing on multi-agent RL.\nentity<|#|>Zhang et al. [2024b]<|#|>Organization<|#|>Zhang et al. [2024b] authored a review on self-play techniques.\nentity<|#|>Wu et al. [2025h]<|#|>Organization<|#|>Wu et al. [2025h] authored a survey on RL in computer vision tasks.\nentity<|#|>Chen et al. [2025m]<|#|>Organization<|#|>Chen et al. [2025m] authored a survey examining long chain-of-thought reasoning.\nentity<|#|>Li et al. [2025w]<|#|>Organization<|#|>Li et al. [2025w] authored a survey examining long chain-of-thought reasoning and analyzing the transition from System 1 to System 2 reasoning.\nentity<|#|>Xia et al. [2024]<|#|>Organization<|#|>Xia et al. [2024] authored a survey examining long chain-of-thought reasoning.\nentity<|#|>Feng et al. [2025c]<|#|>Organization<|#|>Feng et al. [2025c] authored a survey examining adaptive behaviors.\nentity<|#|>Sui et al. [2025]<|#|>Organization<|#|>Sui et al. [2025] authored a survey examining adaptive behaviors.\nentity<|#|>Zhao et al. [2023a]<|#|>Organization<|#|>Zhao et al. [2023a] authored a broad overview of LLM architectures and applications.\nentity<|#|>Zhang et al. [2025a]<|#|>Organization<|#|>Zhang et al. [2025a] authored a survey on replication studies on reasoning LLMs.\nentity<|#|>Sun et al. [2025b]<|#|>Organization<|#|>Sun et al. [2025b] authored a structured survey on reasoning via foundation models.\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Reinforcement Learning<|#|>focuses on, application<|#|>This survey focuses on the application of Reinforcement Learning.\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Large Reasoning Models<|#|>focuses on, application<|#|>This survey focuses on Large Reasoning Models.\nrelation<|#|>Figure 4<|#|>Large Reasoning Models<|#|>illustrates, categorizes<|#|>Figure 4 illustrates representative open-source and closed-source reasoning models.\nrelation<|#|>Figure 4<|#|>Reinforcement Learning<|#|>training method<|#|>Models in Figure 4 are trained with Reinforcement Learning.\nrelation<|#|>Figure 4<|#|>Language Models<|#|>categorizes<|#|>Figure 4 includes language models.\nrelation<|#|>Figure 4<|#|>Multimodal Models<|#|>categorizes<|#|>Figure 4 includes multimodal models.\nrelation<|#|>Figure 4<|#|>Agentic Models<|#|>categorizes<|#|>Figure 4 includes agentic models.\nrelation<|#|>Table 1<|#|>Large Reasoning Models<|#|>provides information<|#|>Table 1 provides detailed information on open-source models.\nrelation<|#|>InternVL3.5<|#|>Wang et al. [2025o]<|#|>developed by<|#|>InternVL3.5 was developed by Wang et al. [2025o].\nrelation<|#|>InternVL3.5<|#|>Unified Native Multimodal Pretraining Phase<|#|>uses, method<|#|>InternVL3.5 used a unified native multimodal pretraining phase.\nrelation<|#|>InternVL3.5<|#|>Two-Stage Cascade RL Framework<|#|>uses, method<|#|>InternVL3.5 used a two-stage cascade RL framework.\nrelation<|#|>InternVL3.5 1-241B<|#|>InternVL3.5<|#|>variant of<|#|>InternVL3.5 1-241B is a variant of InternVL3.5.\nrelation<|#|>Intern-S1<|#|>Bai et al. [2025]<|#|>developed by<|#|>Intern-S1 was developed by Bai et al. [2025].\nrelation<|#|>Intern-S1<|#|>Multimodal Scientific Reasoning<|#|>focuses on<|#|>Intern-S1 focused on multimodal scientific reasoning.\nrelation<|#|>Intern-S1<|#|>Mixture-of-Rewards Design<|#|>uses, method<|#|>Intern-S1 benefited from a mixture-of-rewards design during online RL.\nrelation<|#|>Intern-S1<|#|>Online RL<|#|>uses, method<|#|>Intern-S1 used online RL.\nrelation<|#|>Intern-S 1241B<|#|>Intern-S1<|#|>variant of<|#|>Intern-S 1241B is a variant of Intern-S1.\nrelation<|#|>Step3<|#|>Wang et al. [2025a]<|#|>developed by<|#|>Step3 was developed by Wang et al. [2025a].\nrelation<|#|>Step3<|#|>Efficient Training<|#|>designed for<|#|>Step3 was designed for efficient training.\nrelation<|#|>Step3<|#|>Minimizing Decoding Costs<|#|>designed for<|#|>Step3 was designed for minimizing decoding costs.\nrelation<#|>Hunyuan-TurboS Step 3321B<|#|>Step3<|#|>variant of<|#|>Hunyuan-TurboS Step 3321B is a variant of Step3.\nrelation<|#|>GLM-4.5V<|#|>Team et al. [2025a]<|#|>developed by<|#|>GLM-4.5V was developed by Team et al. [2025a].\nrelation<|#|>GLM-4.5V<|#|>Visual Multimodal Benchmarks<|#|>performance on<|#|>GLM-4.5V shows state-of-the-art performance across most visual multimodal benchmarks.\nrelation<|#|>GLM-4.5V 106B<|#|>GLM-4.5V<|#|>variant of<|#|>GLM-4.5V 106B is a variant of GLM-4.5V.\nrelation<|#|>Ghasemi et al. [2024]<|#|>Reinforcement Learning<|#|>survey topic<|#|>Ghasemi et al. [2024] present a general RL survey.\nrelation<|#|>Huh and Mohapatra [2023]<|#|>Multi-Agent RL<|#|>survey topic<|#|>Huh and Mohapatra [2023] focus on multi-agent RL.\nrelation<|#|>Zhang et al. [2024b]<|#|>Self-Play Techniques<|#|>review topic<|#|>Zhang et al. [2024b] review self-play techniques.\nrelation<|#|>Wu et al. [2025h]<|#|>RL in Computer Vision Tasks<|#|>survey topic<|#|>Wu et al. [2025h] survey RL in computer vision tasks.\nrelation<|#|>Chen et al. [2025m]<|#|>Long Chain-of-Thought Reasoning<|#|>survey topic<|#|>Chen et al. [2025m] examine long chain-of-thought reasoning.\nrelation<|#|>Li et al. [2025w]<|#|>Long Chain-of-Thought Reasoning<|#|>survey topic<|#|>Li et al. [2025w] examine long chain-of-thought reasoning.\nrelation<|#|>Xia et al. [2024]<|#|>Long Chain-of-Thought Reasoning<|#|>survey topic<|#|>Xia et al. [2024] examine long chain-of-thought reasoning.\nrelation<|#|>Feng et al. [2025c]<|#|>Adaptive Behaviors<|#|>survey topic<|#|>Feng et al. [2025c] examine adaptive behaviors.\nrelation<|#|>Sui et al. [2025]<|#|>Adaptive Behaviors<|#|>survey topic<|#|>Sui et al. [2025] examine adaptive behaviors.\nrelation<|#|>Zhao et al. [2023a]<|#|>LLM Architectures<|#|>overview topic<|#|>Zhao et al. [2023a] provide a broad overview of LLM architectures.\nrelation<|#|>Zhao et al. [2023a]<|#|>LLMs<|#|>overview topic<|#|>Zhao et al. [2023a] provide a broad overview of LLM applications.\nrelation<|#|>Long Chain-of-Thought Reasoning<|#|>LLMs<|#|>capability of<|#|>Long chain-of-thought reasoning is an emerging capability of LLMs.\nrelation<|#|>Adaptive Behaviors<|#|>LLMs<|#|>capability of<|#|>Adaptive behaviors are an emerging capability of LLMs.\nrelation<|#|>Zhang et al. [2025a]<|#|>DeepSeek-R1<|#|>replication studies<|#|>Zhang et al. [2025a] survey replication studies on reasoning LLMs in the wake of DeepSeek-R1.\nrelation<|#|>DeepSeek-R1 671B<|#|>DeepSeek-R1<|#|>variant of<|#|>DeepSeek-R1 671B is a variant of DeepSeek-R1.\nrelation<|#|>DeepSeek-R1-0528 671B<|#|>DeepSeek-R1<|#|>variant of<|#|>DeepSeek-R1-0528 671B is a variant of DeepSeek-R1.\nrelation<|#|>Li et al. [2025w]<|#|>System 1 Reasoning<|#|>analyzes<|#|>Li et al. [2025w] analyze the transition from System 1 to System 2 reasoning.\nrelation<|#|>Li et al. [2025w]<|#|>System 2 Reasoning<|#|>analyzes<|#|>Li et al. [2025w] analyze the transition from System 1 to System 2 reasoning.\nrelation<|#|>RLHF<|#|>LLMs<|#|>tool for<|#|>RLHF is a useful tool for reasoning LLMs.\nrelation<|#|>RLVR<|#|>LLMs<|#|>tool for<|#|>RLVR is a useful tool for reasoning LLMs.\nrelation<|#|>Sun et al. [2025b]<|#|>Reasoning via Foundation Models<|#|>survey topic<|#|>Sun et al. [2025b] offer a broader, structured take on reasoning via foundation models.\nrelation<|#|>Sun et al. [2025b]<|#|>Large Reasoning Models<|#|>highlights<|#|>Sun et al. [2025b] highlight key foundation models for reasoning.\n<|COMPLETE|>",
    "cache_type": "extract",
    "chunk_id": "chunk-aa95a088a3c7d1c923aa324fd7c02a63",
    "original_prompt": "---Task---\nExtract entities and relationships from the input text to be processed.\n\n---Instructions---\n1.  **Strict Adherence to Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system prompt.\n2.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\n3.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant entities and relationships have been extracted and presented.\n4.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\n\n<Output>\n---Role---\nYou are a Knowledge Graph Specialist responsible for extracting entities and relationships from the input text.\n\n---Instructions---\n1.  **Entity Extraction & Output:**\n    *   **Identification:** Identify clearly defined and meaningful entities in the input text.\n    *   **Entity Details:** For each identified entity, extract the following information:\n        *   `entity_name`: The name of the entity. If the entity name is case-insensitive, capitalize the first letter of each significant word (title case). Ensure **consistent naming** across the entire extraction process.\n        *   `entity_type`: Categorize the entity using one of the following types: `Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject`. If none of the provided entity types apply, do not add new entity type and classify it as `Other`.\n        *   `entity_description`: Provide a concise yet comprehensive description of the entity's attributes and activities, based *solely* on the information present in the input text.\n    *   **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n        *   Format: `entity<|#|>entity_name<|#|>entity_type<|#|>entity_description`\n\n2.  **Relationship Extraction & Output:**\n    *   **Identification:** Identify direct, clearly stated, and meaningful relationships between previously extracted entities.\n    *   **N-ary Relationship Decomposition:** If a single statement describes a relationship involving more than two entities (an N-ary relationship), decompose it into multiple binary (two-entity) relationship pairs for separate description.\n        *   **Example:** For \"Alice, Bob, and Carol collaborated on Project X,\" extract binary relationships such as \"Alice collaborated with Project X,\" \"Bob collaborated with Project X,\" and \"Carol collaborated with Project X,\" or \"Alice collaborated with Bob,\" based on the most reasonable binary interpretations.\n    *   **Relationship Details:** For each binary relationship, extract the following fields:\n        *   `source_entity`: The name of the source entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `target_entity`: The name of the target entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `relationship_keywords`: One or more high-level keywords summarizing the overarching nature, concepts, or themes of the relationship. Multiple keywords within this field must be separated by a comma `,`. **DO NOT use `<|#|>` for separating multiple keywords within this field.**\n        *   `relationship_description`: A concise explanation of the nature of the relationship between the source and target entities, providing a clear rationale for their connection.\n    *   **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n        *   Format: `relation<|#|>source_entity<|#|>target_entity<|#|>relationship_keywords<|#|>relationship_description`\n\n3.  **Delimiter Usage Protocol:**\n    *   The `<|#|>` is a complete, atomic marker and **must not be filled with content**. It serves strictly as a field separator.\n    *   **Incorrect Example:** `entity<|#|>Tokyo<|location|>Tokyo is the capital of Japan.`\n    *   **Correct Example:** `entity<|#|>Tokyo<|#|>location<|#|>Tokyo is the capital of Japan.`\n\n4.  **Relationship Direction & Duplication:**\n    *   Treat all relationships as **undirected** unless explicitly stated otherwise. Swapping the source and target entities for an undirected relationship does not constitute a new relationship.\n    *   Avoid outputting duplicate relationships.\n\n5.  **Output Order & Prioritization:**\n    *   Output all extracted entities first, followed by all extracted relationships.\n    *   Within the list of relationships, prioritize and output those relationships that are **most significant** to the core meaning of the input text first.\n\n6.  **Context & Objectivity:**\n    *   Ensure all entity names and descriptions are written in the **third person**.\n    *   Explicitly name the subject or object; **avoid using pronouns** such as `this article`, `this paper`, `our company`, `I`, `you`, and `he/she`.\n\n7.  **Language & Proper Nouns:**\n    *   The entire output (entity names, keywords, and descriptions) must be written in `English`.\n    *   Proper nouns (e.g., personal names, place names, organization names) should be retained in their original language if a proper, widely accepted translation is not available or would cause ambiguity.\n\n8.  **Completion Signal:** Output the literal string `<|COMPLETE|>` only after all entities and relationships, following all criteria, have been completely extracted and outputted.\n\n---Examples---\n<Input Text>\n```\nwhile Alex clenched his jaw, the buzz of frustration dull against the backdrop of Taylor's authoritarian certainty. It was this competitive undercurrent that kept him alert, the sense that his and Jordan's shared commitment to discovery was an unspoken rebellion against Cruz's narrowing vision of control and order.\n\nThen Taylor did something unexpected. They paused beside Jordan and, for a moment, observed the device with something akin to reverence. \"If this tech can be understood...\" Taylor said, their voice quieter, \"It could change the game for us. For all of us.\"\n\nThe underlying dismissal earlier seemed to falter, replaced by a glimpse of reluctant respect for the gravity of what lay in their hands. Jordan looked up, and for a fleeting heartbeat, their eyes locked with Taylor's, a wordless clash of wills softening into an uneasy truce.\n\nIt was a small transformation, barely perceptible, but one that Alex noted with an inward nod. They had all been brought here by different paths\n```\n\n<Output>\nentity<|#|>Alex<|#|>person<|#|>Alex is a character who experiences frustration and is observant of the dynamics among other characters.\nentity<|#|>Taylor<|#|>person<|#|>Taylor is portrayed with authoritarian certainty and shows a moment of reverence towards a device, indicating a change in perspective.\nentity<|#|>Jordan<|#|>person<|#|>Jordan shares a commitment to discovery and has a significant interaction with Taylor regarding a device.\nentity<|#|>Cruz<|#|>person<|#|>Cruz is associated with a vision of control and order, influencing the dynamics among other characters.\nentity<|#|>The Device<|#|>equipment<|#|>The Device is central to the story, with potential game-changing implications, and is revered by Taylor.\nrelation<|#|>Alex<|#|>Taylor<|#|>power dynamics, observation<|#|>Alex observes Taylor's authoritarian behavior and notes changes in Taylor's attitude toward the device.\nrelation<|#|>Alex<|#|>Jordan<|#|>shared goals, rebellion<|#|>Alex and Jordan share a commitment to discovery, which contrasts with Cruz's vision.)\nrelation<|#|>Taylor<|#|>Jordan<|#|>conflict resolution, mutual respect<|#|>Taylor and Jordan interact directly regarding the device, leading to a moment of mutual respect and an uneasy truce.\nrelation<|#|>Jordan<|#|>Cruz<|#|>ideological conflict, rebellion<|#|>Jordan's commitment to discovery is in rebellion against Cruz's vision of control and order.\nrelation<|#|>Taylor<|#|>The Device<|#|>reverence, technological significance<|#|>Taylor shows reverence towards the device, indicating its importance and potential impact.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nStock markets faced a sharp downturn today as tech giants saw significant declines, with the global tech index dropping by 3.4% in midday trading. Analysts attribute the selloff to investor concerns over rising interest rates and regulatory uncertainty.\n\nAmong the hardest hit, nexon technologies saw its stock plummet by 7.8% after reporting lower-than-expected quarterly earnings. In contrast, Omega Energy posted a modest 2.1% gain, driven by rising oil prices.\n\nMeanwhile, commodity markets reflected a mixed sentiment. Gold futures rose by 1.5%, reaching $2,080 per ounce, as investors sought safe-haven assets. Crude oil prices continued their rally, climbing to $87.60 per barrel, supported by supply constraints and strong demand.\n\nFinancial experts are closely watching the Federal Reserve's next move, as speculation grows over potential rate hikes. The upcoming policy announcement is expected to influence investor confidence and overall market stability.\n```\n\n<Output>\nentity<|#|>Global Tech Index<|#|>category<|#|>The Global Tech Index tracks the performance of major technology stocks and experienced a 3.4% decline today.\nentity<|#|>Nexon Technologies<|#|>organization<|#|>Nexon Technologies is a tech company that saw its stock decline by 7.8% after disappointing earnings.\nentity<|#|>Omega Energy<|#|>organization<|#|>Omega Energy is an energy company that gained 2.1% in stock value due to rising oil prices.\nentity<|#|>Gold Futures<|#|>product<|#|>Gold futures rose by 1.5%, indicating increased investor interest in safe-haven assets.\nentity<|#|>Crude Oil<|#|>product<|#|>Crude oil prices rose to $87.60 per barrel due to supply constraints and strong demand.\nentity<|#|>Market Selloff<|#|>category<|#|>Market selloff refers to the significant decline in stock values due to investor concerns over interest rates and regulations.\nentity<|#|>Federal Reserve Policy Announcement<|#|>category<|#|>The Federal Reserve's upcoming policy announcement is expected to impact investor confidence and market stability.\nentity<|#|>3.4% Decline<|#|>category<|#|>The Global Tech Index experienced a 3.4% decline in midday trading.\nrelation<|#|>Global Tech Index<|#|>Market Selloff<|#|>market performance, investor sentiment<|#|>The decline in the Global Tech Index is part of the broader market selloff driven by investor concerns.\nrelation<|#|>Nexon Technologies<|#|>Global Tech Index<|#|>company impact, index movement<|#|>Nexon Technologies' stock decline contributed to the overall drop in the Global Tech Index.\nrelation<|#|>Gold Futures<|#|>Market Selloff<|#|>market reaction, safe-haven investment<|#|>Gold prices rose as investors sought safe-haven assets during the market selloff.\nrelation<|#|>Federal Reserve Policy Announcement<|#|>Market Selloff<|#|>interest rate impact, financial regulation<|#|>Speculation over Federal Reserve policy changes contributed to market volatility and investor selloff.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nAt the World Athletics Championship in Tokyo, Noah Carter broke the 100m sprint record using cutting-edge carbon-fiber spikes.\n```\n\n<Output>\nentity<|#|>World Athletics Championship<|#|>event<|#|>The World Athletics Championship is a global sports competition featuring top athletes in track and field.\nentity<|#|>Tokyo<|#|>location<|#|>Tokyo is the host city of the World Athletics Championship.\nentity<|#|>Noah Carter<|#|>person<|#|>Noah Carter is a sprinter who set a new record in the 100m sprint at the World Athletics Championship.\nentity<|#|>100m Sprint Record<|#|>category<|#|>The 100m sprint record is a benchmark in athletics, recently broken by Noah Carter.\nentity<|#|>Carbon-Fiber Spikes<|#|>equipment<|#|>Carbon-fiber spikes are advanced sprinting shoes that provide enhanced speed and traction.\nentity<|#|>World Athletics Federation<|#|>organization<|#|>The World Athletics Federation is the governing body overseeing the World Athletics Championship and record validations.\nrelation<|#|>World Athletics Championship<|#|>Tokyo<|#|>event location, international competition<|#|>The World Athletics Championship is being hosted in Tokyo.\nrelation<|#|>Noah Carter<|#|>100m Sprint Record<|#|>athlete achievement, record-breaking<|#|>Noah Carter set a new 100m sprint record at the championship.\nrelation<|#|>Noah Carter<|#|>Carbon-Fiber Spikes<|#|>athletic equipment, performance boost<|#|>Noah Carter used carbon-fiber spikes to enhance performance during the race.\nrelation<|#|>Noah Carter<|#|>World Athletics Championship<|#|>athlete participation, competition<|#|>Noah Carter is competing at the World Athletics Championship.\n<|COMPLETE|>\n\n\n\n---Real Data to be Processed---\n<Input>\nEntity_types: [Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject]\nText:\n```\nASurveyofReinforcementLearningforLargeReasoningModels\n8\nGPT-5\nLegend 6 Claude Opus 4.1\nDeepSeek-V3.1 671B\nMultimodal o3-pro gpt-oss 21/117B\nGemini 2.5 Flash GLM-4.5V 106B\nPublicly Unavailable\n4 Gemini 2.5 Pro InternVL3.5 1-241B\nPublicly Available Seed-Thinking 1.6 9\no3 Magistral 24B\no4-mini Minimax-M1 456B 7 Ring-mi ni-2.0 16B\nSeed-Thinking 1.5 Qwen 3-Next-80B-A3B-Thinking 80B\nPhi-4 Reasoning 14B Gemi ni2. 5Flash-Lite\nSkywork-R1V2 38B Gro k4\n2 InternVL3 1-78B 5 Intern-S 1241B\nMiMo 7B Kim ik 21T\nClaude 3.7 Sonnet Qwen3 0.6-235B Hunyuan-TurboS Ste p 3321B\no3-mini Claude 4 Qwen3-2507-Thinkin g4-235B\no1-preview Gemini 2.0 Flash Skywork OR-1 7/32B GLM-4.1V-Thinkin g9B\no1-mini\no1-2024-12-17 3 INTELLECT-2 32B GLM-4. 5355B\nLlama-Nemotron-Ultra 253B Skywor k-R1V3 38B\nQVQ-Max DeepSeek-R1-0528 671B\nORZ 0.5-32B\n1\nQWQ 32B\n2025 Kimi1.5\n2024 Deep Seek-R1671B\nFigure 4 | Timeline of representative open-source and closed-source reasoning models trained with\nRL, including language models, multimodal models, and agentic models.\na unified native multimodal pretraining phase, and later InternVL3.5 [Wang et al., 2025o] used a\ntwo-stage cascade RL framework, achieving improved efficiency and versatility. More recently, the\nIntern-S1 [Bai et al., 2025] model focused on multimodal scientific reasoning across diverse domains,\nbenefiting from a mixture-of-rewards design during online RL to facilitate simultaneous training on a\nwide range of tasks. Other recent models include Step3 [Wang et al., 2025a], designed for efficient\ntraining and minimizing decoding costs, and GLM-4.5V [Team et al., 2025a], with state-of-the-art\nperformance across most visual multimodal benchmarks.\nIn addition to the aforementioned models, we provide a comprehensive list of reasoning models\nin Figure 4 and detailed information on open-source models in Table 1.\n2.3. Related Surveys\nInthissubsection,wecomparerecentsurveysrelatedtoRLandLLMs. Severalsurveysfocusprimarily\non RL itself, covering both classical RL and its recent extensions. Ghasemi et al. [2024] present\na general RL survey covering algorithms and real-world challenges, Huh and Mohapatra [2023]\nfocuse on multi-agent RL, Zhang et al. [2024b] review self-play techniques, and Wu et al. [2025h]\nsurvey RL in computer vision tasks. While these works offer broad perspectives on RL, they do not\nexplicitlyaddressitsapplicationtoLLMs. Incontrast,othersurveyscenteronLLMsandtheiremerging\ncapabilities, such as long chain-of-thought reasoning [Chen et al., 2025m, Li et al., 2025w, Xia et al.,\n2024] and adaptive behaviors [Feng et al., 2025c, Sui et al., 2025], where RL is often introduced\nas a key method to support these advances. Zhao et al. [2023a] provide a broad overview of LLM\narchitecturesandapplications,whilemorerecentworksconcentratespecificallyonreasoningabilities.\nZhangetal.[2025a]surveyreplicationstudiesonreasoningLLMsinthewakeofDeepSeek-R1,Chen\netal.[2025m]examinelongchain-of-thoughtreasoning, andLietal.[2025w]analyzethetransition\nfrom System 1 to System 2 reasoning. These studies highlight RL-based methods such as RLHF and\nRLVR as useful tools, but treat them as only one element among a wide range of reasoning strategies.\nSun et al. [2025b] offer a broader, structured take on reasoning via foundation models. It highlights\nkey foundation models that are either proposed or adapted specifically for reasoning, as well as\n8\n\nTables:\n[\n  [\n    [\n      \"GPT-5\"\n    ],\n    [\n      \"Claude Opus 4.1\"\n    ],\n    [\n      \"DeepSeek-V3.1 671B\"\n    ],\n    [\n      \"gpt-oss 21/117B\\nGLM-4.5V 106B\"\n    ]\n  ],\n  [\n    [\n      \"o3-pro\\nGemini 2.5 Flash\"\n    ],\n    [\n      \"Gemini 2.5 Pro\\nSeed-Thinking 1.6\"\n    ]\n  ]\n]\n```",
    "queryparam": null,
    "create_time": 1765219145,
    "update_time": 1765219145,
    "_id": "default:extract:291f979159ceeca21bb8749b9eee20fb"
  },
  "default:extract:63a04a67768cb786afc46f9752604d76": {
    "return": "entity<|#|>Foundation Models<|#|>Concept<|#|>Foundation models are highlighted as key models that are either proposed or adapted specifically for reasoning.\nentity<|#|>Reasoning LLMs<|#|>Concept<|#|>Reasoning LLMs are a specific category of Large Language Models that are the subject of replication studies and for which RL-based methods like RLHF and RLVR are useful tools.\nrelation<|#|>Zhang et al. [2025a]<|#|>Reasoning LLMs<|#|>survey topic, replication studies<|#|>Zhang et al. [2025a] survey replication studies on reasoning LLMs.\nrelation<|#|>Sun et al. [2025b]<|#|>Foundation Models<|#|>highlights, survey topic<|#|>Sun et al. [2025b] highlight key foundation models that are proposed or adapted specifically for reasoning.\nrelation<|#|>RLHF<|#|>Reasoning LLMs<|#|>tool for, method<|#|>RLHF is a useful RL-based method for reasoning LLMs.\nrelation<|#|>RLVR<|#|>Reasoning LLMs<|#|>tool for, method<|#|>RLVR is a useful RL-based method for reasoning LLMs.\n<|COMPLETE|>",
    "cache_type": "extract",
    "chunk_id": "chunk-aa95a088a3c7d1c923aa324fd7c02a63",
    "original_prompt": "---Task---\nBased on the last extraction task, identify and extract any **missed or incorrectly formatted** entities and relationships from the input text.\n\n---Instructions---\n1.  **Strict Adherence to System Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system instructions.\n2.  **Focus on Corrections/Additions:**\n    *   **Do NOT** re-output entities and relationships that were **correctly and fully** extracted in the last task.\n    *   If an entity or relationship was **missed** in the last task, extract and output it now according to the system format.\n    *   If an entity or relationship was **truncated, had missing fields, or was otherwise incorrectly formatted** in the last task, re-output the *corrected and complete* version in the specified format.\n3.  **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n4.  **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n5.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\n6.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant missing or corrected entities and relationships have been extracted and presented.\n7.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\n\n<Output>\n---Role---\nYou are a Knowledge Graph Specialist responsible for extracting entities and relationships from the input text.\n\n---Instructions---\n1.  **Entity Extraction & Output:**\n    *   **Identification:** Identify clearly defined and meaningful entities in the input text.\n    *   **Entity Details:** For each identified entity, extract the following information:\n        *   `entity_name`: The name of the entity. If the entity name is case-insensitive, capitalize the first letter of each significant word (title case). Ensure **consistent naming** across the entire extraction process.\n        *   `entity_type`: Categorize the entity using one of the following types: `Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject`. If none of the provided entity types apply, do not add new entity type and classify it as `Other`.\n        *   `entity_description`: Provide a concise yet comprehensive description of the entity's attributes and activities, based *solely* on the information present in the input text.\n    *   **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n        *   Format: `entity<|#|>entity_name<|#|>entity_type<|#|>entity_description`\n\n2.  **Relationship Extraction & Output:**\n    *   **Identification:** Identify direct, clearly stated, and meaningful relationships between previously extracted entities.\n    *   **N-ary Relationship Decomposition:** If a single statement describes a relationship involving more than two entities (an N-ary relationship), decompose it into multiple binary (two-entity) relationship pairs for separate description.\n        *   **Example:** For \"Alice, Bob, and Carol collaborated on Project X,\" extract binary relationships such as \"Alice collaborated with Project X,\" \"Bob collaborated with Project X,\" and \"Carol collaborated with Project X,\" or \"Alice collaborated with Bob,\" based on the most reasonable binary interpretations.\n    *   **Relationship Details:** For each binary relationship, extract the following fields:\n        *   `source_entity`: The name of the source entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `target_entity`: The name of the target entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `relationship_keywords`: One or more high-level keywords summarizing the overarching nature, concepts, or themes of the relationship. Multiple keywords within this field must be separated by a comma `,`. **DO NOT use `<|#|>` for separating multiple keywords within this field.**\n        *   `relationship_description`: A concise explanation of the nature of the relationship between the source and target entities, providing a clear rationale for their connection.\n    *   **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n        *   Format: `relation<|#|>source_entity<|#|>target_entity<|#|>relationship_keywords<|#|>relationship_description`\n\n3.  **Delimiter Usage Protocol:**\n    *   The `<|#|>` is a complete, atomic marker and **must not be filled with content**. It serves strictly as a field separator.\n    *   **Incorrect Example:** `entity<|#|>Tokyo<|location|>Tokyo is the capital of Japan.`\n    *   **Correct Example:** `entity<|#|>Tokyo<|#|>location<|#|>Tokyo is the capital of Japan.`\n\n4.  **Relationship Direction & Duplication:**\n    *   Treat all relationships as **undirected** unless explicitly stated otherwise. Swapping the source and target entities for an undirected relationship does not constitute a new relationship.\n    *   Avoid outputting duplicate relationships.\n\n5.  **Output Order & Prioritization:**\n    *   Output all extracted entities first, followed by all extracted relationships.\n    *   Within the list of relationships, prioritize and output those relationships that are **most significant** to the core meaning of the input text first.\n\n6.  **Context & Objectivity:**\n    *   Ensure all entity names and descriptions are written in the **third person**.\n    *   Explicitly name the subject or object; **avoid using pronouns** such as `this article`, `this paper`, `our company`, `I`, `you`, and `he/she`.\n\n7.  **Language & Proper Nouns:**\n    *   The entire output (entity names, keywords, and descriptions) must be written in `English`.\n    *   Proper nouns (e.g., personal names, place names, organization names) should be retained in their original language if a proper, widely accepted translation is not available or would cause ambiguity.\n\n8.  **Completion Signal:** Output the literal string `<|COMPLETE|>` only after all entities and relationships, following all criteria, have been completely extracted and outputted.\n\n---Examples---\n<Input Text>\n```\nwhile Alex clenched his jaw, the buzz of frustration dull against the backdrop of Taylor's authoritarian certainty. It was this competitive undercurrent that kept him alert, the sense that his and Jordan's shared commitment to discovery was an unspoken rebellion against Cruz's narrowing vision of control and order.\n\nThen Taylor did something unexpected. They paused beside Jordan and, for a moment, observed the device with something akin to reverence. \"If this tech can be understood...\" Taylor said, their voice quieter, \"It could change the game for us. For all of us.\"\n\nThe underlying dismissal earlier seemed to falter, replaced by a glimpse of reluctant respect for the gravity of what lay in their hands. Jordan looked up, and for a fleeting heartbeat, their eyes locked with Taylor's, a wordless clash of wills softening into an uneasy truce.\n\nIt was a small transformation, barely perceptible, but one that Alex noted with an inward nod. They had all been brought here by different paths\n```\n\n<Output>\nentity<|#|>Alex<|#|>person<|#|>Alex is a character who experiences frustration and is observant of the dynamics among other characters.\nentity<|#|>Taylor<|#|>person<|#|>Taylor is portrayed with authoritarian certainty and shows a moment of reverence towards a device, indicating a change in perspective.\nentity<|#|>Jordan<|#|>person<|#|>Jordan shares a commitment to discovery and has a significant interaction with Taylor regarding a device.\nentity<|#|>Cruz<|#|>person<|#|>Cruz is associated with a vision of control and order, influencing the dynamics among other characters.\nentity<|#|>The Device<|#|>equipment<|#|>The Device is central to the story, with potential game-changing implications, and is revered by Taylor.\nrelation<|#|>Alex<|#|>Taylor<|#|>power dynamics, observation<|#|>Alex observes Taylor's authoritarian behavior and notes changes in Taylor's attitude toward the device.\nrelation<|#|>Alex<|#|>Jordan<|#|>shared goals, rebellion<|#|>Alex and Jordan share a commitment to discovery, which contrasts with Cruz's vision.)\nrelation<|#|>Taylor<|#|>Jordan<|#|>conflict resolution, mutual respect<|#|>Taylor and Jordan interact directly regarding the device, leading to a moment of mutual respect and an uneasy truce.\nrelation<|#|>Jordan<|#|>Cruz<|#|>ideological conflict, rebellion<|#|>Jordan's commitment to discovery is in rebellion against Cruz's vision of control and order.\nrelation<|#|>Taylor<|#|>The Device<|#|>reverence, technological significance<|#|>Taylor shows reverence towards the device, indicating its importance and potential impact.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nStock markets faced a sharp downturn today as tech giants saw significant declines, with the global tech index dropping by 3.4% in midday trading. Analysts attribute the selloff to investor concerns over rising interest rates and regulatory uncertainty.\n\nAmong the hardest hit, nexon technologies saw its stock plummet by 7.8% after reporting lower-than-expected quarterly earnings. In contrast, Omega Energy posted a modest 2.1% gain, driven by rising oil prices.\n\nMeanwhile, commodity markets reflected a mixed sentiment. Gold futures rose by 1.5%, reaching $2,080 per ounce, as investors sought safe-haven assets. Crude oil prices continued their rally, climbing to $87.60 per barrel, supported by supply constraints and strong demand.\n\nFinancial experts are closely watching the Federal Reserve's next move, as speculation grows over potential rate hikes. The upcoming policy announcement is expected to influence investor confidence and overall market stability.\n```\n\n<Output>\nentity<|#|>Global Tech Index<|#|>category<|#|>The Global Tech Index tracks the performance of major technology stocks and experienced a 3.4% decline today.\nentity<|#|>Nexon Technologies<|#|>organization<|#|>Nexon Technologies is a tech company that saw its stock decline by 7.8% after disappointing earnings.\nentity<|#|>Omega Energy<|#|>organization<|#|>Omega Energy is an energy company that gained 2.1% in stock value due to rising oil prices.\nentity<|#|>Gold Futures<|#|>product<|#|>Gold futures rose by 1.5%, indicating increased investor interest in safe-haven assets.\nentity<|#|>Crude Oil<|#|>product<|#|>Crude oil prices rose to $87.60 per barrel due to supply constraints and strong demand.\nentity<|#|>Market Selloff<|#|>category<|#|>Market selloff refers to the significant decline in stock values due to investor concerns over interest rates and regulations.\nentity<|#|>Federal Reserve Policy Announcement<|#|>category<|#|>The Federal Reserve's upcoming policy announcement is expected to impact investor confidence and market stability.\nentity<|#|>3.4% Decline<|#|>category<|#|>The Global Tech Index experienced a 3.4% decline in midday trading.\nrelation<|#|>Global Tech Index<|#|>Market Selloff<|#|>market performance, investor sentiment<|#|>The decline in the Global Tech Index is part of the broader market selloff driven by investor concerns.\nrelation<|#|>Nexon Technologies<|#|>Global Tech Index<|#|>company impact, index movement<|#|>Nexon Technologies' stock decline contributed to the overall drop in the Global Tech Index.\nrelation<|#|>Gold Futures<|#|>Market Selloff<|#|>market reaction, safe-haven investment<|#|>Gold prices rose as investors sought safe-haven assets during the market selloff.\nrelation<|#|>Federal Reserve Policy Announcement<|#|>Market Selloff<|#|>interest rate impact, financial regulation<|#|>Speculation over Federal Reserve policy changes contributed to market volatility and investor selloff.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nAt the World Athletics Championship in Tokyo, Noah Carter broke the 100m sprint record using cutting-edge carbon-fiber spikes.\n```\n\n<Output>\nentity<|#|>World Athletics Championship<|#|>event<|#|>The World Athletics Championship is a global sports competition featuring top athletes in track and field.\nentity<|#|>Tokyo<|#|>location<|#|>Tokyo is the host city of the World Athletics Championship.\nentity<|#|>Noah Carter<|#|>person<|#|>Noah Carter is a sprinter who set a new record in the 100m sprint at the World Athletics Championship.\nentity<|#|>100m Sprint Record<|#|>category<|#|>The 100m sprint record is a benchmark in athletics, recently broken by Noah Carter.\nentity<|#|>Carbon-Fiber Spikes<|#|>equipment<|#|>Carbon-fiber spikes are advanced sprinting shoes that provide enhanced speed and traction.\nentity<|#|>World Athletics Federation<|#|>organization<|#|>The World Athletics Federation is the governing body overseeing the World Athletics Championship and record validations.\nrelation<|#|>World Athletics Championship<|#|>Tokyo<|#|>event location, international competition<|#|>The World Athletics Championship is being hosted in Tokyo.\nrelation<|#|>Noah Carter<|#|>100m Sprint Record<|#|>athlete achievement, record-breaking<|#|>Noah Carter set a new 100m sprint record at the championship.\nrelation<|#|>Noah Carter<|#|>Carbon-Fiber Spikes<|#|>athletic equipment, performance boost<|#|>Noah Carter used carbon-fiber spikes to enhance performance during the race.\nrelation<|#|>Noah Carter<|#|>World Athletics Championship<|#|>athlete participation, competition<|#|>Noah Carter is competing at the World Athletics Championship.\n<|COMPLETE|>\n\n\n\n---Real Data to be Processed---\n<Input>\nEntity_types: [Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject]\nText:\n```\nASurveyofReinforcementLearningforLargeReasoningModels\n8\nGPT-5\nLegend 6 Claude Opus 4.1\nDeepSeek-V3.1 671B\nMultimodal o3-pro gpt-oss 21/117B\nGemini 2.5 Flash GLM-4.5V 106B\nPublicly Unavailable\n4 Gemini 2.5 Pro InternVL3.5 1-241B\nPublicly Available Seed-Thinking 1.6 9\no3 Magistral 24B\no4-mini Minimax-M1 456B 7 Ring-mi ni-2.0 16B\nSeed-Thinking 1.5 Qwen 3-Next-80B-A3B-Thinking 80B\nPhi-4 Reasoning 14B Gemi ni2. 5Flash-Lite\nSkywork-R1V2 38B Gro k4\n2 InternVL3 1-78B 5 Intern-S 1241B\nMiMo 7B Kim ik 21T\nClaude 3.7 Sonnet Qwen3 0.6-235B Hunyuan-TurboS Ste p 3321B\no3-mini Claude 4 Qwen3-2507-Thinkin g4-235B\no1-preview Gemini 2.0 Flash Skywork OR-1 7/32B GLM-4.1V-Thinkin g9B\no1-mini\no1-2024-12-17 3 INTELLECT-2 32B GLM-4. 5355B\nLlama-Nemotron-Ultra 253B Skywor k-R1V3 38B\nQVQ-Max DeepSeek-R1-0528 671B\nORZ 0.5-32B\n1\nQWQ 32B\n2025 Kimi1.5\n2024 Deep Seek-R1671B\nFigure 4 | Timeline of representative open-source and closed-source reasoning models trained with\nRL, including language models, multimodal models, and agentic models.\na unified native multimodal pretraining phase, and later InternVL3.5 [Wang et al., 2025o] used a\ntwo-stage cascade RL framework, achieving improved efficiency and versatility. More recently, the\nIntern-S1 [Bai et al., 2025] model focused on multimodal scientific reasoning across diverse domains,\nbenefiting from a mixture-of-rewards design during online RL to facilitate simultaneous training on a\nwide range of tasks. Other recent models include Step3 [Wang et al., 2025a], designed for efficient\ntraining and minimizing decoding costs, and GLM-4.5V [Team et al., 2025a], with state-of-the-art\nperformance across most visual multimodal benchmarks.\nIn addition to the aforementioned models, we provide a comprehensive list of reasoning models\nin Figure 4 and detailed information on open-source models in Table 1.\n2.3. Related Surveys\nInthissubsection,wecomparerecentsurveysrelatedtoRLandLLMs. Severalsurveysfocusprimarily\non RL itself, covering both classical RL and its recent extensions. Ghasemi et al. [2024] present\na general RL survey covering algorithms and real-world challenges, Huh and Mohapatra [2023]\nfocuse on multi-agent RL, Zhang et al. [2024b] review self-play techniques, and Wu et al. [2025h]\nsurvey RL in computer vision tasks. While these works offer broad perspectives on RL, they do not\nexplicitlyaddressitsapplicationtoLLMs. Incontrast,othersurveyscenteronLLMsandtheiremerging\ncapabilities, such as long chain-of-thought reasoning [Chen et al., 2025m, Li et al., 2025w, Xia et al.,\n2024] and adaptive behaviors [Feng et al., 2025c, Sui et al., 2025], where RL is often introduced\nas a key method to support these advances. Zhao et al. [2023a] provide a broad overview of LLM\narchitecturesandapplications,whilemorerecentworksconcentratespecificallyonreasoningabilities.\nZhangetal.[2025a]surveyreplicationstudiesonreasoningLLMsinthewakeofDeepSeek-R1,Chen\netal.[2025m]examinelongchain-of-thoughtreasoning, andLietal.[2025w]analyzethetransition\nfrom System 1 to System 2 reasoning. These studies highlight RL-based methods such as RLHF and\nRLVR as useful tools, but treat them as only one element among a wide range of reasoning strategies.\nSun et al. [2025b] offer a broader, structured take on reasoning via foundation models. It highlights\nkey foundation models that are either proposed or adapted specifically for reasoning, as well as\n8\n\nTables:\n[\n  [\n    [\n      \"GPT-5\"\n    ],\n    [\n      \"Claude Opus 4.1\"\n    ],\n    [\n      \"DeepSeek-V3.1 671B\"\n    ],\n    [\n      \"gpt-oss 21/117B\\nGLM-4.5V 106B\"\n    ]\n  ],\n  [\n    [\n      \"o3-pro\\nGemini 2.5 Flash\"\n    ],\n    [\n      \"Gemini 2.5 Pro\\nSeed-Thinking 1.6\"\n    ]\n  ]\n]\n```\n[{\"role\": \"user\", \"content\": \"---Task---\\nExtract entities and relationships from the input text to be processed.\\n\\n---Instructions---\\n1.  **Strict Adherence to Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system prompt.\\n2.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\\n3.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant entities and relationships have been extracted and presented.\\n4.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\\n\\n<Output>\"}, {\"role\": \"assistant\", \"content\": \"entity<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Content<|#|>This document is a survey focusing on the application of Reinforcement Learning for Large Reasoning Models.\\nentity<|#|>Reinforcement Learning<|#|>Concept<|#|>Reinforcement Learning (RL) is a key method used in training various models, including large reasoning models.\\nentity<|#|>Large Reasoning Models<|#|>Concept<|#|>Large Reasoning Models are a category of models that benefit from Reinforcement Learning, including language, multimodal, and agentic models.\\nentity<|#|>LLMs<|#|>Concept<|#|>Large Language Models (LLMs) are a category of models whose architectures and applications are surveyed, and whose emerging capabilities include long chain-of-thought reasoning and adaptive behaviors.\\nentity<|#|>GPT-5<|#|>Artifact<|#|>GPT-5 is a representative reasoning model.\\nentity<|#|>Claude Opus 4.1<|#|>Artifact<|#|>Claude Opus 4.1 is a representative reasoning model.\\nentity<|#|>DeepSeek-V3.1 671B<|#|>Artifact<|#|>DeepSeek-V3.1 671B is a representative reasoning model.\\nentity<|#|>Multimodal o3-pro<|#|>Artifact<|#|>Multimodal o3-pro is a representative reasoning model.\\nentity<|#|>gpt-oss 21/117B<|#|>Artifact<|#|>gpt-oss 21/117B is a representative reasoning model.\\nentity<|#|>Gemini 2.5 Flash<|#|>Artifact<|#|>Gemini 2.5 Flash is a representative reasoning model.\\nentity<|#|>GLM-4.5V 106B<|#|>Artifact<|#|>GLM-4.5V 106B is a specific variant of the GLM-4.5V reasoning model.\\nentity<|#|>Gemini 2.5 Pro<|#|>Artifact<|#|>Gemini 2.5 Pro is a representative reasoning model.\\nentity<|#|>InternVL3.5 1-241B<|#|>Artifact<|#|>InternVL3.5 1-241B is a specific variant of the InternVL3.5 reasoning model.\\nentity<|#|>Seed-Thinking 1.6<|#|>Artifact<|#|>Seed-Thinking 1.6 is a representative reasoning model.\\nentity<|#|>o3 Magistral 24B<|#|>Artifact<|#|>o3 Magistral 24B is a representative reasoning model.\\nentity<|#|>o4-mini<|#|>Artifact<|#|>o4-mini is a representative reasoning model.\\nentity<|#|>Minimax-M1 456B<|#|>Artifact<|#|>Minimax-M1 456B is a representative reasoning model.\\nentity<|#|>Ring-mini-2.0 16B<|#|>Artifact<|#|>Ring-mini-2.0 16B is a representative reasoning model.\\nentity<|#|>Seed-Thinking 1.5<|#|>Artifact<|#|>Seed-Thinking 1.5 is a representative reasoning model.\\nentity<|#|>Qwen 3-Next-80B-A3B-Thinking 80B<|#|>Artifact<|#|>Qwen 3-Next-80B-A3B-Thinking 80B is a representative reasoning model.\\nentity<|#|>Phi-4 Reasoning 14B<|#|>Artifact<|#|>Phi-4 Reasoning 14B is a representative reasoning model.\\nentity<|#|>Gemini 2.5 Flash-Lite<|#|>Artifact<|#|>Gemini 2.5 Flash-Lite is a representative reasoning model.\\nentity<|#|>Skywork-R1V2 38B<|#|>Artifact<|#|>Skywork-R1V2 38B is a representative reasoning model.\\nentity<|#|>Grok4<|#|>Artifact<|#|>Grok4 is a representative reasoning model.\\nentity<|#|>InternVL3 1-78B<|#|>Artifact<|#|>InternVL3 1-78B is a representative reasoning model.\\nentity<|#|>Intern-S 1241B<|#|>Artifact<|#|>Intern-S 1241B is a specific variant of the Intern-S1 reasoning model.\\nentity<|#|>MiMo 7B<|#|>Artifact<|#|>MiMo 7B is a representative reasoning model.\\nentity<|#|>Kimik 21T<|#|>Artifact<|#|>Kimik 21T is a representative reasoning model.\\nentity<|#|>Claude 3.7 Sonnet<|#|>Artifact<|#|>Claude 3.7 Sonnet is a representative reasoning model.\\nentity<|#|>Qwen3 0.6-235B<|#|>Artifact<|#|>Qwen3 0.6-235B is a representative reasoning model.\\nentity<|#|>Hunyuan-TurboS Step 3321B<|#|>Artifact<|#|>Hunyuan-TurboS Step 3321B is a specific variant of the Step3 reasoning model.\\nentity<|#|>o3-mini<|#|>Artifact<|#|>o3-mini is a representative reasoning model.\\nentity<|#|>Claude 4<|#|>Artifact<|#|>Claude 4 is a representative reasoning model.\\nentity<|#|>Qwen3-2507-Thinking4-235B<|#|>Artifact<|#|>Qwen3-2507-Thinking4-235B is a representative reasoning model.\\nentity<|#|>o1-preview<|#|>Artifact<|#|>o1-preview is a representative reasoning model.\\nentity<|#|>Gemini 2.0 Flash<|#|>Artifact<|#|>Gemini 2.0 Flash is a representative reasoning model.\\nentity<|#|>Skywork OR-1 7/32B<|#|>Artifact<|#|>Skywork OR-1 7/32B is a representative reasoning model.\\nentity<|#|>GLM-4.1V-Thinking9B<|#|>Artifact<|#|>GLM-4.1V-Thinking9B is a representative reasoning model.\\nentity<|#|>o1-mini<|#|>Artifact<|#|>o1-mini is a representative reasoning model.\\nentity<|#|>o1-2024-12-17<|#|>Artifact<|#|>o1-2024-12-17 is a representative reasoning model.\\nentity<|#|>INTELLECT-2 32B<|#|>Artifact<|#|>INTELLECT-2 32B is a representative reasoning model.\\nentity<|#|>GLM-4.5355B<|#|>Artifact<|#|>GLM-4.5355B is a representative reasoning model.\\nentity<|#|>Llama-Nemotron-Ultra 253B<|#|>Artifact<|#|>Llama-Nemotron-Ultra 253B is a representative reasoning model.\\nentity<|#|>Skywork-R1V3 38B<|#|>Artifact<|#|>Skywork-R1V3 38B is a representative reasoning model.\\nentity<|#|>QVQ-Max<|#|>Artifact<|#|>QVQ-Max is a representative reasoning model.\\nentity<|#|>DeepSeek-R1-0528 671B<|#|>Artifact<|#|>DeepSeek-R1-0528 671B is a specific variant of the DeepSeek-R1 reasoning model.\\nentity<|#|>ORZ 0.5-32B<|#|>Artifact<|#|>ORZ 0.5-32B is a representative reasoning model.\\nentity<|#|>QWQ 32B<|#|>Artifact<|#|>QWQ 32B is a representative reasoning model.\\nentity<|#|>Kimi1.5<|#|>Artifact<|#|>Kimi1.5 is a representative reasoning model.\\nentity<|#|>DeepSeek-R1 671B<|#|>Artifact<|#|>DeepSeek-R1 671B is a specific variant of the DeepSeek-R1 reasoning model.\\nentity<|#|>InternVL3.5<|#|>Artifact<|#|>InternVL3.5 is a reasoning model that used a unified native multimodal pretraining phase and a two-stage cascade RL framework, achieving improved efficiency and versatility.\\nentity<|#|>Intern-S1<|#|>Artifact<|#|>Intern-S1 is a reasoning model that focused on multimodal scientific reasoning across diverse domains, benefiting from a mixture-of-rewards design during online RL.\\nentity<|#|>Step3<|#|>Artifact<|#|>Step3 is a reasoning model designed for efficient training and minimizing decoding costs.\\nentity<|#|>GLM-4.5V<|#|>Artifact<|#|>GLM-4.5V is a reasoning model with state-of-the-art performance across most visual multimodal benchmarks.\\nentity<|#|>DeepSeek-R1<|#|>Artifact<|#|>DeepSeek-R1 is a reasoning model that is the subject of replication studies.\\nentity<|#|>Figure 4<|#|>Content<|#|>Figure 4 is a timeline illustrating representative open-source and closed-source reasoning models trained with Reinforcement Learning.\\nentity<|#|>Table 1<|#|>Content<|#|>Table 1 provides detailed information on open-source models.\\nentity<|#|>Unified Native Multimodal Pretraining Phase<|#|>Method<|#|>This method was used by InternVL3.5 for improved efficiency and versatility.\\nentity<|#|>Two-Stage Cascade RL Framework<|#|>Method<|#|>This framework was used by InternVL3.5, achieving improved efficiency and versatility.\\nentity<#|>Mixture-of-Rewards Design<|#|>Method<|#|>This design was used during online RL for the Intern-S1 model to facilitate simultaneous training on a wide range of tasks.\\nentity<|#|>Online RL<|#|>Method<|#|>Online RL is a method of Reinforcement Learning used in models like Intern-S1.\\nentity<|#|>Efficient Training<|#|>Concept<|#|>Efficient training is a design goal for models like Step3.\\nentity<|#|>Minimizing Decoding Costs<|#|>Concept<|#|>Minimizing decoding costs is a design goal for models like Step3.\\nentity<|#|>Visual Multimodal Benchmarks<|#|>Concept<|#|>Visual multimodal benchmarks are used to evaluate the performance of models like GLM-4.5V.\\nentity<|#|>Classical RL<|#|>Concept<|#|>Classical RL refers to traditional Reinforcement Learning approaches.\\nentity<|#|>Multi-Agent RL<|#|>Concept<|#|>Multi-Agent RL is a specific area of Reinforcement Learning.\\nentity<|#|>Self-Play Techniques<|#|>Method<|#|>Self-play techniques are reviewed in the context of Reinforcement Learning.\\nentity<|#|>RL in Computer Vision Tasks<|#|>Concept<|#|>RL in Computer Vision Tasks is a specific application area of Reinforcement Learning.\\nentity<|#|>LLM Architectures<|#|>Concept<|#|>LLM Architectures refer to the structural designs of Large Language Models.\\nentity<|#|>Long Chain-of-Thought Reasoning<|#|>Concept<|#|>Long Chain-of-Thought Reasoning is an emerging capability of LLMs.\\nentity<|#|>Adaptive Behaviors<|#|>Concept<|#|>Adaptive behaviors are an emerging capability of LLMs.\\nentity<|#|>RLHF<|#|>Method<|#|>RLHF (Reinforcement Learning from Human Feedback) is an RL-based method useful for reasoning LLMs.\\nentity<|#|>RLVR<|#|>Method<|#|>RLVR (Reinforcement Learning from Visual Reasoning) is an RL-based method useful for reasoning LLMs.\\nentity<|#|>System 1 Reasoning<|#|>Concept<|#|>System 1 Reasoning is a type of reasoning analyzed in relation to LLMs.\\nentity<|#|>System 2 Reasoning<|#|>Concept<|#|>System 2 Reasoning is a type of reasoning analyzed in relation to LLMs.\\nentity<|#|>Reasoning via Foundation Models<|#|>Concept<|#|>Reasoning via Foundation Models is a structured approach to reasoning.\\nentity<|#|>Language Models<|#|>Concept<|#|>Language Models are a type of reasoning model trained with RL.\\nentity<|#|>Multimodal Models<|#|>Concept<|#|>Multimodal Models are a type of reasoning model trained with RL.\\nentity<|#|>Agentic Models<|#|>Concept<|#|>Agentic Models are a type of reasoning model trained with RL.\\nentity<|#|>Multimodal Scientific Reasoning<|#|>Concept<|#|>Multimodal scientific reasoning is a focus area for models like Intern-S1.\\nentity<|#|>Wang et al. [2025o]<|#|>Organization<|#|>Wang et al. [2025o] are researchers associated with the InternVL3.5 model.\\nentity<|#|>Bai et al. [2025]<|#|>Organization<|#|>Bai et al. [2025] are researchers associated with the Intern-S1 model.\\nentity<|#|>Wang et al. [2025a]<|#|>Organization<|#|>Wang et al. [2025a] are researchers associated with the Step3 model.\\nentity<|#|>Team et al. [2025a]<|#|>Organization<|#|>Team et al. [2025a] are researchers associated with the GLM-4.5V model.\\nentity<|#|>Ghasemi et al. [2024]<|#|>Organization<|#|>Ghasemi et al. [2024] authored a general RL survey.\\nentity<|#|>Huh and Mohapatra [2023]<|#|>Organization<|#|>Huh and Mohapatra [2023] authored a survey focusing on multi-agent RL.\\nentity<|#|>Zhang et al. [2024b]<|#|>Organization<|#|>Zhang et al. [2024b] authored a review on self-play techniques.\\nentity<|#|>Wu et al. [2025h]<|#|>Organization<|#|>Wu et al. [2025h] authored a survey on RL in computer vision tasks.\\nentity<|#|>Chen et al. [2025m]<|#|>Organization<|#|>Chen et al. [2025m] authored a survey examining long chain-of-thought reasoning.\\nentity<|#|>Li et al. [2025w]<|#|>Organization<|#|>Li et al. [2025w] authored a survey examining long chain-of-thought reasoning and analyzing the transition from System 1 to System 2 reasoning.\\nentity<|#|>Xia et al. [2024]<|#|>Organization<|#|>Xia et al. [2024] authored a survey examining long chain-of-thought reasoning.\\nentity<|#|>Feng et al. [2025c]<|#|>Organization<|#|>Feng et al. [2025c] authored a survey examining adaptive behaviors.\\nentity<|#|>Sui et al. [2025]<|#|>Organization<|#|>Sui et al. [2025] authored a survey examining adaptive behaviors.\\nentity<|#|>Zhao et al. [2023a]<|#|>Organization<|#|>Zhao et al. [2023a] authored a broad overview of LLM architectures and applications.\\nentity<|#|>Zhang et al. [2025a]<|#|>Organization<|#|>Zhang et al. [2025a] authored a survey on replication studies on reasoning LLMs.\\nentity<|#|>Sun et al. [2025b]<|#|>Organization<|#|>Sun et al. [2025b] authored a structured survey on reasoning via foundation models.\\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Reinforcement Learning<|#|>focuses on, application<|#|>This survey focuses on the application of Reinforcement Learning.\\nrelation<|#|>ASurveyofReinforcementLearningforLargeReasoningModels<|#|>Large Reasoning Models<|#|>focuses on, application<|#|>This survey focuses on Large Reasoning Models.\\nrelation<|#|>Figure 4<|#|>Large Reasoning Models<|#|>illustrates, categorizes<|#|>Figure 4 illustrates representative open-source and closed-source reasoning models.\\nrelation<|#|>Figure 4<|#|>Reinforcement Learning<|#|>training method<|#|>Models in Figure 4 are trained with Reinforcement Learning.\\nrelation<|#|>Figure 4<|#|>Language Models<|#|>categorizes<|#|>Figure 4 includes language models.\\nrelation<|#|>Figure 4<|#|>Multimodal Models<|#|>categorizes<|#|>Figure 4 includes multimodal models.\\nrelation<|#|>Figure 4<|#|>Agentic Models<|#|>categorizes<|#|>Figure 4 includes agentic models.\\nrelation<|#|>Table 1<|#|>Large Reasoning Models<|#|>provides information<|#|>Table 1 provides detailed information on open-source models.\\nrelation<|#|>InternVL3.5<|#|>Wang et al. [2025o]<|#|>developed by<|#|>InternVL3.5 was developed by Wang et al. [2025o].\\nrelation<|#|>InternVL3.5<|#|>Unified Native Multimodal Pretraining Phase<|#|>uses, method<|#|>InternVL3.5 used a unified native multimodal pretraining phase.\\nrelation<|#|>InternVL3.5<|#|>Two-Stage Cascade RL Framework<|#|>uses, method<|#|>InternVL3.5 used a two-stage cascade RL framework.\\nrelation<|#|>InternVL3.5 1-241B<|#|>InternVL3.5<|#|>variant of<|#|>InternVL3.5 1-241B is a variant of InternVL3.5.\\nrelation<|#|>Intern-S1<|#|>Bai et al. [2025]<|#|>developed by<|#|>Intern-S1 was developed by Bai et al. [2025].\\nrelation<|#|>Intern-S1<|#|>Multimodal Scientific Reasoning<|#|>focuses on<|#|>Intern-S1 focused on multimodal scientific reasoning.\\nrelation<|#|>Intern-S1<|#|>Mixture-of-Rewards Design<|#|>uses, method<|#|>Intern-S1 benefited from a mixture-of-rewards design during online RL.\\nrelation<|#|>Intern-S1<|#|>Online RL<|#|>uses, method<|#|>Intern-S1 used online RL.\\nrelation<|#|>Intern-S 1241B<|#|>Intern-S1<|#|>variant of<|#|>Intern-S 1241B is a variant of Intern-S1.\\nrelation<|#|>Step3<|#|>Wang et al. [2025a]<|#|>developed by<|#|>Step3 was developed by Wang et al. [2025a].\\nrelation<|#|>Step3<|#|>Efficient Training<|#|>designed for<|#|>Step3 was designed for efficient training.\\nrelation<|#|>Step3<|#|>Minimizing Decoding Costs<|#|>designed for<|#|>Step3 was designed for minimizing decoding costs.\\nrelation<#|>Hunyuan-TurboS Step 3321B<|#|>Step3<|#|>variant of<|#|>Hunyuan-TurboS Step 3321B is a variant of Step3.\\nrelation<|#|>GLM-4.5V<|#|>Team et al. [2025a]<|#|>developed by<|#|>GLM-4.5V was developed by Team et al. [2025a].\\nrelation<|#|>GLM-4.5V<|#|>Visual Multimodal Benchmarks<|#|>performance on<|#|>GLM-4.5V shows state-of-the-art performance across most visual multimodal benchmarks.\\nrelation<|#|>GLM-4.5V 106B<|#|>GLM-4.5V<|#|>variant of<|#|>GLM-4.5V 106B is a variant of GLM-4.5V.\\nrelation<|#|>Ghasemi et al. [2024]<|#|>Reinforcement Learning<|#|>survey topic<|#|>Ghasemi et al. [2024] present a general RL survey.\\nrelation<|#|>Huh and Mohapatra [2023]<|#|>Multi-Agent RL<|#|>survey topic<|#|>Huh and Mohapatra [2023] focus on multi-agent RL.\\nrelation<|#|>Zhang et al. [2024b]<|#|>Self-Play Techniques<|#|>review topic<|#|>Zhang et al. [2024b] review self-play techniques.\\nrelation<|#|>Wu et al. [2025h]<|#|>RL in Computer Vision Tasks<|#|>survey topic<|#|>Wu et al. [2025h] survey RL in computer vision tasks.\\nrelation<|#|>Chen et al. [2025m]<|#|>Long Chain-of-Thought Reasoning<|#|>survey topic<|#|>Chen et al. [2025m] examine long chain-of-thought reasoning.\\nrelation<|#|>Li et al. [2025w]<|#|>Long Chain-of-Thought Reasoning<|#|>survey topic<|#|>Li et al. [2025w] examine long chain-of-thought reasoning.\\nrelation<|#|>Xia et al. [2024]<|#|>Long Chain-of-Thought Reasoning<|#|>survey topic<|#|>Xia et al. [2024] examine long chain-of-thought reasoning.\\nrelation<|#|>Feng et al. [2025c]<|#|>Adaptive Behaviors<|#|>survey topic<|#|>Feng et al. [2025c] examine adaptive behaviors.\\nrelation<|#|>Sui et al. [2025]<|#|>Adaptive Behaviors<|#|>survey topic<|#|>Sui et al. [2025] examine adaptive behaviors.\\nrelation<|#|>Zhao et al. [2023a]<|#|>LLM Architectures<|#|>overview topic<|#|>Zhao et al. [2023a] provide a broad overview of LLM architectures.\\nrelation<|#|>Zhao et al. [2023a]<|#|>LLMs<|#|>overview topic<|#|>Zhao et al. [2023a] provide a broad overview of LLM applications.\\nrelation<|#|>Long Chain-of-Thought Reasoning<|#|>LLMs<|#|>capability of<|#|>Long chain-of-thought reasoning is an emerging capability of LLMs.\\nrelation<|#|>Adaptive Behaviors<|#|>LLMs<|#|>capability of<|#|>Adaptive behaviors are an emerging capability of LLMs.\\nrelation<|#|>Zhang et al. [2025a]<|#|>DeepSeek-R1<|#|>replication studies<|#|>Zhang et al. [2025a] survey replication studies on reasoning LLMs in the wake of DeepSeek-R1.\\nrelation<|#|>DeepSeek-R1 671B<|#|>DeepSeek-R1<|#|>variant of<|#|>DeepSeek-R1 671B is a variant of DeepSeek-R1.\\nrelation<|#|>DeepSeek-R1-0528 671B<|#|>DeepSeek-R1<|#|>variant of<|#|>DeepSeek-R1-0528 671B is a variant of DeepSeek-R1.\\nrelation<|#|>Li et al. [2025w]<|#|>System 1 Reasoning<|#|>analyzes<|#|>Li et al. [2025w] analyze the transition from System 1 to System 2 reasoning.\\nrelation<|#|>Li et al. [2025w]<|#|>System 2 Reasoning<|#|>analyzes<|#|>Li et al. [2025w] analyze the transition from System 1 to System 2 reasoning.\\nrelation<|#|>RLHF<|#|>LLMs<|#|>tool for<|#|>RLHF is a useful tool for reasoning LLMs.\\nrelation<|#|>RLVR<|#|>LLMs<|#|>tool for<|#|>RLVR is a useful tool for reasoning LLMs.\\nrelation<|#|>Sun et al. [2025b]<|#|>Reasoning via Foundation Models<|#|>survey topic<|#|>Sun et al. [2025b] offer a broader, structured take on reasoning via foundation models.\\nrelation<|#|>Sun et al. [2025b]<|#|>Large Reasoning Models<|#|>highlights<|#|>Sun et al. [2025b] highlight key foundation models for reasoning.\\n<|COMPLETE|>\"}]",
    "queryparam": null,
    "create_time": 1765219163,
    "update_time": 1765219163,
    "_id": "default:extract:63a04a67768cb786afc46f9752604d76"
  },
  "default:extract:1652c44ed750bb3051f42a533083dd2e": {
    "return": "entity<|#|>Reinforcement Learning<|#|>Concept<|#|>A machine learning paradigm used for training models, central to the survey of large reasoning models.\nentity<|#|>Large Reasoning Models<|#|>Concept<|#|>A category of models that are the subject of the survey, often trained with reinforcement learning.\nentity<|#|>Online Policy Mirror Descent<|#|>Concept<|#|>An algorithm used for training models, denoted as OPMD.\nentity<|#|>Mixed Preference Optimization<|#|>Concept<|#|>An algorithm used for training models, denoted as MPO.\nentity<|#|>Clipped IS-weight Policy Optimization<|#|>Concept<|#|>An algorithm used for training models, denoted as CISPO.\nentity<|#|>Text Modality<|#|>Concept<|#|>A type of data modality, indicated by 'T', supported by various models.\nentity<|#|>Image Modality<|#|>Concept<|#|>A type of data modality, indicated by 'I', supported by various models.\nentity<|#|>Video Modality<|#|>Concept<|#|>A type of data modality, indicated by 'V', supported by various models.\nentity<|#|>Open-Source Models<|#|>Concept<|#|>Representative models that are publicly available and trained with Reinforcement Learning.\nentity<|#|>GRPO<|#|>Concept<|#|>An algorithm used for training models.\nentity<|#|>PPO<|#|>Concept<|#|>An algorithm used for training models.\nentity<|#|>GSPO<|#|>Concept<|#|>An algorithm used for training models.\nentity<|#|>Mixture of Experts<|#|>Concept<|#|>A model architecture, denoted as MoE.\nentity<|#|>Multi-Layer Attention<|#|>Concept<|#|>A model architecture, denoted as MLA.\nentity<|#|>Dense Architecture<|#|>Concept<|#|>A model architecture.\nentity<|#|>Hybrid Mixture of Experts<|#|>Concept<|#|>A model architecture, denoted as HybridMoE.\nentity<|#|>DeepSeek<|#|>Organization<|#|>An organization that develops large reasoning models.\nentity<|#|>StepAI<|#|>Organization<|#|>An organization that develops large reasoning models.\nentity<|#|>AlibabaQwen<|#|>Organization<|#|>An organization that develops large reasoning models.\nentity<|#|>Microsoft<|#|>Organization<|#|>An organization that develops large reasoning models.\nentity<|#|>Skywork<|#|>Organization<|#|>An organization that develops large reasoning models.\nentity<|#|>ShanghaiAILab<|#|>Organization<|#|>An organization that develops large reasoning models.\nentity<|#|>Xiaomi<|#|>Organization<|#|>An organization that develops large reasoning models.\nentity<|#|>NVIDIA<|#|>Organization<|#|>An organization that develops large reasoning models.\nentity<|#|>IntellectAI<|#|>Organization<|#|>An organization that develops large reasoning models.\nentity<|#|>Tencent<|#|>Organization<|#|>An organization that develops large reasoning models.\nentity<|#|>MistralAI<|#|>Organization<|#|>An organization that develops large reasoning models.\nentity<|#|>Minimax<|#|>Organization<|#|>An organization that develops large reasoning models.\nentity<|#|>Kimi<|#|>Organization<|#|>An organization that develops large reasoning models.\nentity<|#|>ZhipuAI<|#|>Organization<|#|>An organization that develops large reasoning models.\nentity<|#|>OpenAI<|#|>Organization<|#|>An organization that develops large reasoning models.\nentity<|#|>BytedanceSeed<|#|>Organization<|#|>An organization that develops large reasoning models.\nentity<|#|>Baidu<|#|>Organization<|#|>An organization that develops large reasoning models.\nentity<|#|>DeepSeek-R1<|#|>Artifact<|#|>An open-source model developed by DeepSeek with MoE/MLA architecture and 671B parameters, trained with GRPO for Text modality.\nentity<|#|>ORZ<|#|>Artifact<|#|>An open-source model developed by StepAI with Dense architecture and 0.5-32B parameters, trained with PPO for Text modality.\nentity<|#|>QwQ<|#|>Artifact<|#|>An open-source model developed by AlibabaQwen with Dense architecture and 32B parameters for Text modality.\nentity<|#|>Phi-4Reasoning<|#|>Artifact<|#|>An open-source model developed by Microsoft with Dense architecture and 14B parameters, trained with GRPO for Text modality.\nentity<|#|>Skywork-R1V2<|#|>Artifact<|#|>An open-source model developed by Skywork with Dense architecture and 38B parameters, trained with MPO/GRPO for Text and Image modalities.\nentity<|#|>InternVL3<|#|>Artifact<|#|>An open-source model developed by ShanghaiAILab with Dense architecture and 1-78B parameters, trained with MPO for Text, Image, and Video modalities.\nentity<|#|>MiMo<|#|>Artifact<|#|>An open-source model developed by Xiaomi with Dense architecture and 7B parameters, trained with GRPO for Text modality.\nentity<|#|>Qwen3<|#|>Artifact<|#|>An open-source model developed by AlibabaQwen with MoE/Dense architecture and 0.6-235B parameters, trained with GRPO for Text modality.\nentity<|#|>Llama-Nemotron-Ultra<|#|>Artifact<|#|>An open-source model developed by NVIDIA with Dense architecture and 253B parameters, trained with GRPO for Text modality.\nentity<|#|>INTELLECT-2<|#|>Artifact<|#|>An open-source model developed by IntellectAI with Dense architecture and 32B parameters, trained with GRPO for Text modality.\nentity<|#|>Hunyuan-TurboS<|#|>Artifact<|#|>An open-source model developed by Tencent with HybridMoE architecture and 560B parameters, trained with GRPO for Text modality.\nentity<|#|>SkyworkOR-1<|#|>Artifact<|#|>An open-source model developed by Skywork with Dense architecture and 7B/32B parameters, trained with GRPO for Text modality.\nentity<|#|>DeepSeek-R1-0528<|#|>Artifact<|#|>An open-source model developed by DeepSeek with MoE/MLA architecture and 671B parameters, trained with GRPO for Text modality.\nentity<|#|>Magistral<|#|>Artifact<|#|>An open-source model developed by MistralAI with Dense architecture and 24B parameters, trained with GRPO for Text modality.\nentity<|#|>Minimax-M1<|#|>Artifact<|#|>An open-source model developed by Minimax with HybridMoE architecture and 456B parameters, trained with CISPO for Text modality.\nentity<|#|>Intern-S1<|#|>Artifact<|#|>An open-source model developed by ShanghaiAILab with MoE architecture and 241B parameters, trained with GRPO for Text, Image, and Video modalities.\nentity<|#|>KimiK2<|#|>Artifact<|#|>An open-source model developed by Kimi with MoE architecture and 1T parameters, trained with Online Policy Mirror Descent for Text modality.\nentity<|#|>Step3<|#|>Artifact<|#|>An open-source model developed by StepAI with MoE architecture and 321B parameters for Text, Image, and Video modalities.\nentity<|#|>Qwen3-2507<|#|>Artifact<|#|>An open-source model developed by AlibabaQwen with MoE/Dense architecture and 4-235B parameters, trained with GSPO for Text modality.\nentity<|#|>GLM-4.1V-Thinking<|#|>Artifact<|#|>An open-source model developed by ZhipuAI with Dense architecture and 9B parameters, trained with GRPO for Text, Image, and Video modalities.\nentity<|#|>GLM-4.5<|#|>Artifact<|#|>An open-source model developed by ZhipuAI with MoE architecture and 355B parameters, trained with GRPO for Text modality.\nentity<|#|>Skywork-R1V3<|#|>Artifact<|#|>An open-source model developed by Skywork with Dense architecture and 38B parameters, trained with GRPO for Text and Image modalities.\nentity<|#|>gpt-oss<|#|>Artifact<|#|>An open-source model developed by OpenAI with MoE architecture and 117B/21B parameters for Text modality.\nentity<|#|>Seed-OSS<|#|>Artifact<|#|>An open-source model developed by BytedanceSeed with Dense architecture and 36B parameters for Text modality.\nentity<|#|>GLM-4.5V<|#|>Artifact<|#|>An open-source model developed by ZhipuAI with MoE architecture and 106B parameters, trained with GRPO for Text, Image, and Video modalities.\nentity<|#|>InternVL3.5<|#|>Artifact<|#|>An open-source model developed by ShanghaiAILab with MoE/Dense architecture and 1-241B parameters, trained with MPO/GSPO for Text, Image, and Video modalities.\nentity<|#|>ERNIE-4.5-Thinking<|#|>Artifact<|#|>An open-source model developed by Baidu with MoE architecture and 21B-A3B parameters for Text modality.\nrelation<|#|>Open-Source Models<|#|>Reinforcement Learning<|#|>training method, application<|#|>Open-source models are trained using Reinforcement Learning.\nrelation<|#|>Online Policy Mirror Descent<|#|>Reinforcement Learning<|#|>algorithm type, category<|#|>Online Policy Mirror Descent is an algorithm used in Reinforcement Learning.\nrelation<|#|>Mixed Preference Optimization<|#|>Reinforcement Learning<|#|>algorithm type, category<|#|>Mixed Preference Optimization is an algorithm used in Reinforcement Learning.\nrelation<|#|>Clipped IS-weight Policy Optimization<|#|>Reinforcement Learning<|#|>algorithm type, category<|#|>Clipped IS-weight Policy Optimization is an algorithm used in Reinforcement Learning.\nrelation<|#|>GRPO<|#|>Reinforcement Learning<|#|>algorithm type, category<|#|>GRPO is an algorithm used in Reinforcement Learning.\nrelation<|#|>PPO<|#|>Reinforcement Learning<|#|>algorithm type, category<|#|>PPO is an algorithm used in Reinforcement Learning.\nrelation<|#|>GSPO<|#|>Reinforcement Learning<|#|>algorithm type, category<|#|>GSPO is an algorithm used in Reinforcement Learning.\nrelation<|#|>DeepSeek-R1<|#|>DeepSeek<|#|>development, affiliation<|#|>DeepSeek-R1 was developed by the organization DeepSeek.\nrelation<|#|>DeepSeek-R1<|#|>Mixture of Experts<|#|>architecture, design<|#|>DeepSeek-R1 uses the Mixture of Experts architecture.\nrelation<|#|>DeepSeek-R1<|#|>Multi-Layer Attention<|#|>architecture, design<|#|>DeepSeek-R1 uses the Multi-Layer Attention architecture.\nrelation<|#|>DeepSeek-R1<|#|>GRPO<|#|>training algorithm, method<|#|>DeepSeek-R1 is trained with the GRPO algorithm.\nrelation<|#|>DeepSeek-R1<|#|>Text Modality<|#|>data support, capability<|#|>DeepSeek-R1 supports the Text Modality.\nrelation<|#|>DeepSeek-R1<|#|>Open-Source Models<|#|>model type, classification<|#|>DeepSeek-R1 is a type of Open-Source Model.\nrelation<|#|>ORZ<|#|>StepAI<|#|>development, affiliation<|#|>ORZ was developed by the organization StepAI.\nrelation<|#|>ORZ<|#|>Dense Architecture<|#|>architecture, design<|#|>ORZ uses the Dense Architecture.\nrelation<|#|>ORZ<|#|>PPO<|#|>training algorithm, method<|#|>ORZ is trained with the PPO algorithm.\nrelation<#|>ORZ<|#|>Text Modality<|#|>data support, capability<|#|>ORZ supports the Text Modality.\nrelation<|#|>ORZ<|#|>Open-Source Models<|#|>model type, classification<|#|>ORZ is a type of Open-Source Model.\nrelation<|#|>QwQ<|#|>AlibabaQwen<|#|>development, affiliation<|#|>QwQ was developed by the organization AlibabaQwen.\nrelation<|#|>QwQ<|#|>Dense Architecture<|#|>architecture, design<|#|>QwQ uses the Dense Architecture.\nrelation<|#|>QwQ<|#|>Text Modality<|#|>data support, capability<|#|>QwQ supports the Text Modality.\nrelation<|#|>QwQ<|#|>Open-Source Models<|#|>model type, classification<|#|>QwQ is a type of Open-Source Model.\nrelation<|#|>Phi-4Reasoning<|#|>Microsoft<|#|>development, affiliation<|#|>Phi-4Reasoning was developed by the organization Microsoft.\nrelation<|#|>Phi-4Reasoning<|#|>Dense Architecture<|#|>architecture, design<|#|>Phi-4Reasoning uses the Dense Architecture.\nrelation<|#|>Phi-4Reasoning<|#|>GRPO<|#|>training algorithm, method<|#|>Phi-4Reasoning is trained with the GRPO algorithm.\nrelation<|#|>Phi-4Reasoning<|#|>Text Modality<|#|>data support, capability<|#|>Phi-4Reasoning supports the Text Modality.\nrelation<|#|>Phi-4Reasoning<|#|>Open-Source Models<|#|>model type, classification<|#|>Phi-4Reasoning is a type of Open-Source Model.\nrelation<|#|>Skywork-R1V2<|#|>Skywork<|#|>development, affiliation<|#|>Skywork-R1V2 was developed by the organization Skywork.\nrelation<|#|>Skywork-R1V2<|#|>Dense Architecture<|#|>architecture, design<|#|>Skywork-R1V2 uses the Dense Architecture.\nrelation<|#|>Skywork-R1V2<|#|>Mixed Preference Optimization<|#|>training algorithm, method<|#|>Skywork-R1V2 is trained with the Mixed Preference Optimization algorithm.\nrelation<|#|>Skywork-R1V2<|#|>GRPO<|#|>training algorithm, method<|#|>Skywork-R1V2 is trained with the GRPO algorithm.\nrelation<|#|>Skywork-R1V2<|#|>Text Modality<|#|>data support, capability<|#|>Skywork-R1V2 supports the Text Modality.\nrelation<|#|>Skywork-R1V2<|#|>Image Modality<|#|>data support, capability<|#|>Skywork-R1V2 supports the Image Modality.\nrelation<|#|>Skywork-R1V2<|#|>Open-Source Models<|#|>model type, classification<|#|>Skywork-R1V2 is a type of Open-Source Model.\nrelation<|#|>InternVL3<|#|>ShanghaiAILab<|#|>development, affiliation<|#|>InternVL3 was developed by the organization ShanghaiAILab.\nrelation<|#|>InternVL3<|#|>Dense Architecture<|#|>architecture, design<|#|>InternVL3 uses the Dense Architecture.\nrelation<|#|>InternVL3<|#|>Mixed Preference Optimization<|#|>training algorithm, method<|#|>InternVL3 is trained with the Mixed Preference Optimization algorithm.\nrelation<|#|>InternVL3<|#|>Text Modality<|#|>data support, capability<|#|>InternVL3 supports the Text Modality.\nrelation<|#|>InternVL3<|#|>Image Modality<|#|>data support, capability<|#|>InternVL3 supports the Image Modality.\nrelation<|#|>InternVL3<|#|>Video Modality<|#|>data support, capability<|#|>InternVL3 supports the Video Modality.\nrelation<|#|>InternVL3<|#|>Open-Source Models<|#|>model type, classification<|#|>InternVL3 is a type of Open-Source Model.\nrelation<|#|>MiMo<|#|>Xiaomi<|#|>development, affiliation<|#|>MiMo was developed by the organization Xiaomi.\nrelation<|#|>MiMo<|#|>Dense Architecture<|#|>architecture, design<|#|>MiMo uses the Dense Architecture.\nrelation<|#|>MiMo<|#|>GRPO<|#|>training algorithm, method<|#|>MiMo is trained with the GRPO algorithm.\nrelation<|#|>MiMo<|#|>Text Modality<|#|>data support, capability<|#|>MiMo supports the Text Modality.\nrelation<|#|>MiMo<|#|>Open-Source Models<|#|>model type, classification<|#|>MiMo is a type of Open-Source Model.\nrelation<|#|>Qwen3<|#|>AlibabaQwen<|#|>development, affiliation<|#|>Qwen3 was developed by the organization AlibabaQwen.\nrelation<|#|>Qwen3<|#|>Mixture of Experts<|#|>architecture, design<|#|>Qwen3 uses the Mixture of Experts architecture.\nrelation<|#|>Qwen3<|#|>Dense Architecture<|#|>architecture, design<|#|>Qwen3 uses the Dense Architecture.\nrelation<|#|>Qwen3<|#|>GRPO<|#|>training algorithm, method<|#|>Qwen3 is trained with the GRPO algorithm.\nrelation<|#|>Qwen3<|#|>Text Modality<|#|>data support, capability<|#|>Qwen3 supports the Text Modality.\nrelation<|#|>Qwen3<|#|>Open-Source Models<|#|>model type, classification<|#|>Qwen3 is a type of Open-Source Model.\nrelation<|#|>Llama-Nemotron-Ultra<|#|>NVIDIA<|#|>development, affiliation<|#|>Llama-Nemotron-Ultra was developed by the organization NVIDIA.\nrelation<|#|>Llama-Nemotron-Ultra<|#|>Dense Architecture<|#|>architecture, design<|#|>Llama-Nemotron-Ultra uses the Dense Architecture.\nrelation<|#|>Llama-Nemotron-Ultra<|#|>GRPO<|#|>training algorithm, method<|#|>Llama-Nemotron-Ultra is trained with the GRPO algorithm.\nrelation<|#|>Llama-Nemotron-Ultra<|#|>Text Modality<|#|>data support, capability<|#|>Llama-Nemotron-Ultra supports the Text Modality.\nrelation<|#|>Llama-Nemotron-Ultra<|#|>Open-Source Models<|#|>model type, classification<|#|>Llama-Nemotron-Ultra is a type of Open-Source Model.\nrelation<|#|>INTELLECT-2<|#|>IntellectAI<|#|>development, affiliation<|#|>INTELLECT-2 was developed by the organization IntellectAI.\nrelation<|#|>INTELLECT-2<|#|>Dense Architecture<|#|>architecture, design<|#|>INTELLECT-2 uses the Dense Architecture.\nrelation<|#|>INTELLECT-2<|#|>GRPO<|#|>training algorithm, method<|#|>INTELLECT-2 is trained with the GRPO algorithm.\nrelation<|#|>INTELLECT-2<|#|>Text Modality<|#|>data support, capability<|#|>INTELLECT-2 supports the Text Modality.\nrelation<|#|>INTELLECT-2<|#|>Open-Source Models<|#|>model type, classification<|#|>INTELLECT-2 is a type of Open-Source Model.\nrelation<|#|>Hunyuan-TurboS<|#|>Tencent<|#|>development, affiliation<|#|>Hunyuan-TurboS was developed by the organization Tencent.\nrelation<|#|>Hunyuan-TurboS<|#|>Hybrid Mixture of Experts<|#|>architecture, design<|#|>Hunyuan-TurboS uses the Hybrid Mixture of Experts architecture.\nrelation<|#|>Hunyuan-TurboS<|#|>GRPO<|#|>training algorithm, method<|#|>Hunyuan-TurboS is trained with the GRPO algorithm.\nrelation<|#|>Hunyuan-TurboS<|#|>Text Modality<|#|>data support, capability<|#|>Hunyuan-TurboS supports the Text Modality.\nrelation<|#|>Hunyuan-TurboS<|#|>Open-Source Models<|#|>model type, classification<|#|>Hunyuan-TurboS is a type of Open-Source Model.\nrelation<|#|>SkyworkOR-1<|#|>Skywork<|#|>development, affiliation<|#|>SkyworkOR-1 was developed by the organization Skywork.\nrelation<|#|>SkyworkOR-1<|#|>Dense Architecture<|#|>architecture, design<|#|>SkyworkOR-1 uses the Dense Architecture.\nrelation<|#|>SkyworkOR-1<|#|>GRPO<|#|>training algorithm, method<|#|>SkyworkOR-1 is trained with the GRPO algorithm.\nrelation<|#|>SkyworkOR-1<|#|>Text Modality<|#|>data support, capability<|#|>SkyworkOR-1 supports the Text Modality.\nrelation<|#|>SkyworkOR-1<|#|>Open-Source Models<|#|>model type, classification<|#|>SkyworkOR-1 is a type of Open-Source Model.\nrelation<|#|>DeepSeek-R1-0528<|#|>DeepSeek<|#|>development, affiliation<|#|>DeepSeek-R1-0528 was developed by the organization DeepSeek.\nrelation<|#|>DeepSeek-R1-0528<|#|>Mixture of Experts<|#|>architecture, design<|#|>DeepSeek-R1-0528 uses the Mixture of Experts architecture.\nrelation<|#|>DeepSeek-R1-0528<|#|>Multi-Layer Attention<|#|>architecture, design<|#|>DeepSeek-R1-0528 uses the Multi-Layer Attention architecture.\nrelation<|#|>DeepSeek-R1-0528<|#|>GRPO<|#|>training algorithm, method<|#|>DeepSeek-R1-0528 is trained with the GRPO algorithm.\nrelation<|#|>DeepSeek-R1-0528<|#|>Text Modality<|#|>data support, capability<|#|>DeepSeek-R1-0528 supports the Text Modality.\nrelation<|#|>DeepSeek-R1-0528<|#|>Open-Source Models<|#|>model type, classification<|#|>DeepSeek-R1-0528 is a type of Open-Source Model.\nrelation<|#|>Magistral<|#|>MistralAI<|#|>development, affiliation<|#|>Magistral was developed by the organization MistralAI.\nrelation<|#|>Magistral<|#|>Dense Architecture<|#|>architecture, design<|#|>Magistral uses the Dense Architecture.\nrelation<|#|>Magistral<|#|>GRPO<|#|>training algorithm, method<|#|>Magistral is trained with the GRPO algorithm.\nrelation<|#|>Magistral<|#|>Text Modality<|#|>data support, capability<|#|>Magistral supports the Text Modality.\nrelation<|#|>Magistral<|#|>Open-Source Models<|#|>model type, classification<|#|>Magistral is a type of Open-Source Model.\nrelation<|#|>Minimax-M1<|#|>Minimax<|#|>development, affiliation<|#|>Minimax-M1 was developed by the organization Minimax.\nrelation<|#|>Minimax-M1<|#|>Hybrid Mixture of Experts<|#|>architecture, design<|#|>Minimax-M1 uses the Hybrid Mixture of Experts architecture.\nrelation<|#|>Minimax-M1<|#|>Clipped IS-weight Policy Optimization<|#|>training algorithm, method<|#|>Minimax-M1 is trained with the Clipped IS-weight Policy Optimization algorithm.\nrelation<|#|>Minimax-M1<|#|>Text Modality<|#|>data support, capability<|#|>Minimax-M1 supports the Text Modality.\nrelation<|#|>Minimax-M1<|#|>Open-Source Models<|#|>model type, classification<|#|>Minimax-M1 is a type of Open-Source Model.\nrelation<|#|>Intern-S1<|#|>ShanghaiAILab<|#|>development, affiliation<|#|>Intern-S1 was developed by the organization ShanghaiAILab.\nrelation<|#|>Intern-S1<|#|>Mixture of Experts<|#|>architecture, design<|#|>Intern-S1 uses the Mixture of Experts architecture.\nrelation<|#|>Intern-S1<|#|>GRPO<|#|>training algorithm, method<|#|>Intern-S1 is trained with the GRPO algorithm.\nrelation<|#|>Intern-S1<|#|>Text Modality<|#|>data support, capability<|#|>Intern-S1 supports the Text Modality.\nrelation<|#|>Intern-S1<|#|>Image Modality<|#|>data support, capability<|#|>Intern-S1 supports the Image Modality.\nrelation<|#|>Intern-S1<|#|>Video Modality<|#|>data support, capability<|#|>Intern-S1 supports the Video Modality.\nrelation<|#|>Intern-S1<|#|>Open-Source Models<|#|>model type, classification<|#|>Intern-S1 is a type of Open-Source Model.\nrelation<|#|>KimiK2<|#|>Kimi<|#|>development, affiliation<|#|>KimiK2 was developed by the organization Kimi.\nrelation<|#|>KimiK2<|#|>Mixture of Experts<|#|>architecture, design<|#|>KimiK2 uses the Mixture of Experts architecture.\nrelation<|#|>KimiK2<|#|>Online Policy Mirror Descent<|#|>training algorithm, method<|#|>KimiK2 is trained with the Online Policy Mirror Descent algorithm.\nrelation<|#|>KimiK2<|#|>Text Modality<|#|>data support, capability<|#|>KimiK2 supports the Text Modality.\nrelation<|#|>KimiK2<|#|>Open-Source Models<|#|>model type, classification<|#|>KimiK2 is a type of Open-Source Model.\nrelation<|#|>Step3<|#|>StepAI<|#|>development, affiliation<|#|>Step3 was developed by the organization StepAI.\nrelation<|#|>Step3<|#|>Mixture of Experts<|#|>architecture, design<|#|>Step3 uses the Mixture of Experts architecture.\nrelation<|#|>Step3<|#|>Text Modality<|#|>data support, capability<|#|>Step3 supports the Text Modality.\nrelation<|#|>Step3<|#|>Image Modality<|#|>data support, capability<|#|>Step3 supports the Image Modality.\nrelation<|#|>Step3<|#|>Video Modality<|#|>data support, capability<|#|>Step3 supports the Video Modality.\nrelation<|#|>Step3<|#|>Open-Source Models<|#|>model type, classification<|#|>Step3 is a type of Open-Source Model.\nrelation<|#|>Qwen3-2507<|#|>AlibabaQwen<|#|>development, affiliation<|#|>Qwen3-2507 was developed by the organization AlibabaQwen.\nrelation<|#|>Qwen3-2507<|#|>Mixture of Experts<|#|>architecture, design<|#|>Qwen3-2507 uses the Mixture of Experts architecture.\nrelation<|#|>Qwen3-2507<|#|>Dense Architecture<|#|>architecture, design<|#|>Qwen3-2507 uses the Dense Architecture.\nrelation<|#|>Qwen3-2507<|#|>GSPO<|#|>training algorithm, method<|#|>Qwen3-2507 is trained with the GSPO algorithm.\nrelation<|#|>Qwen3-2507<|#|>Text Modality<|#|>data support, capability<|#|>Qwen3-2507 supports the Text Modality.\nrelation<|#|>Qwen3-2507<|#|>Open-Source Models<|#|>model type, classification<|#|>Qwen3-2507 is a type of Open-Source Model.\nrelation<|#|>GLM-4.1V-Thinking<|#|>ZhipuAI<|#|>development, affiliation<|#|>GLM-4.1V-Thinking was developed by the organization ZhipuAI.\nrelation<|#|>GLM-4.1V-Thinking<|#|>Dense Architecture<|#|>architecture, design<|#|>GLM-4.1V-Thinking uses the Dense Architecture.\nrelation<|#|>GLM-4.1V-Thinking<|#|>GRPO<|#|>training algorithm, method<|#|>GLM-4.1V-Thinking is trained with the GRPO algorithm.\nrelation<|#|>GLM-4.1V-Thinking<|#|>Text Modality<|#|>data support, capability<|#|>GLM-4.1V-Thinking supports the Text Modality.\nrelation<|#|>GLM-4.1V-Thinking<|#|>Image Modality<|#|>data support, capability<|#|>GLM-4.1V-Thinking supports the Image Modality.\nrelation<|#|>GLM-4.1V-Thinking<|#|>Video Modality<|#|>data support, capability<|#|>GLM-4.1V-Thinking supports the Video Modality.\nrelation<|#|>GLM-4.1V-Thinking<|#|>Open-Source Models<|#|>model type, classification<|#|>GLM-4.1V-Thinking is a type of Open-Source Model.\nrelation<|#|>GLM-4.5<|#|>ZhipuAI<|#|>development, affiliation<|#|>GLM-4.5 was developed by the organization ZhipuAI.\nrelation<|#|>GLM-4.5<|#|>Mixture of Experts<|#|>architecture, design<|#|>GLM-4.5 uses the Mixture of Experts architecture.\nrelation<|#|>GLM-4.5<|#|>GRPO<|#|>training algorithm, method<|#|>GLM-4.5 is trained with the GRPO algorithm.\nrelation<|#|>GLM-4.5<|#|>Text Modality<|#|>data support, capability<|#|>GLM-4.5 supports the Text Modality.\nrelation<|#|>GLM-4.5<|#|>Open-Source Models<|#|>model type, classification<|#|>GLM-4.5 is a type of Open-Source Model.\nrelation<|#|>Skywork-R1V3<|#|>Skywork<|#|>development, affiliation<|#|>Skywork-R1V3 was developed by the organization Skywork.\nrelation<|#|>Skywork-R1V3<|#|>Dense Architecture<|#|>architecture, design<|#|>Skywork-R1V3 uses the Dense Architecture.\nrelation<|#|>Skywork-R1V3<|#|>GRPO<|#|>training algorithm, method<|#|>Skywork-R1V3 is trained with the GRPO algorithm.\nrelation<|#|>Skywork-R1V3<|#|>Text Modality<|#|>data support, capability<|#|>Skywork-R1V3 supports the Text Modality.\nrelation<|#|>Skywork-R1V3<|#|>Image Modality<|#|>data support, capability<|#|>Skywork-R1V3 supports the Image Modality.\nrelation<|#|>Skywork-R1V3<|#|>Open-Source Models<|#|>model type, classification<|#|>Skywork-R1V3 is a type of Open-Source Model.\nrelation<|#|>gpt-oss<|#|>OpenAI<|#|>development, affiliation<|#|>gpt-oss was developed by the organization OpenAI.\nrelation<|#|>gpt-oss<|#|>Mixture of Experts<|#|>architecture, design<|#|>gpt-oss uses the Mixture of Experts architecture.\nrelation<|#|>gpt-oss<|#|>Text Modality<|#|>data support, capability<|#|>gpt-oss supports the Text Modality.\nrelation<|#|>gpt-oss<|#|>Open-Source Models<|#|>model type, classification<|#|>gpt-oss is a type of Open-Source Model.\nrelation<|#|>Seed-OSS<|#|>BytedanceSeed<|#|>development, affiliation<|#|>Seed-OSS was developed by the organization BytedanceSeed.\nrelation<|#|>Seed-OSS<|#|>Dense Architecture<|#|>architecture, design<|#|>Seed-OSS uses the Dense Architecture.\nrelation<|#|>Seed-OSS<|#|>Text Modality<|#|>data support, capability<|#|>Seed-OSS supports the Text Modality.\nrelation<|#|>Seed-OSS<|#|>Open-Source Models<|#|>model type, classification<|#|>Seed-OSS is a type of Open-Source Model.\nrelation<|#|>GLM-4.5V<|#|>ZhipuAI<|#|>development, affiliation<|#|>GLM-4.5V was developed by the organization ZhipuAI.\nrelation<|#|>GLM-4.5V<|#|>Mixture of Experts<|#|>architecture, design<|#|>GLM-4.5V uses the Mixture of Experts architecture.\nrelation<|#|>GLM-4.5V<|#|>GRPO<|#|>training algorithm, method<|#|>GLM-4.5V is trained with the GRPO algorithm.\nrelation<|#|>GLM-4.5V<|#|>Text Modality<|#|>data support, capability<|#|>GLM-4.5V supports the Text Modality.\nrelation<|#|>GLM-4.5V<|#|>Image Modality<|#|>data support, capability<|#|>GLM-4.5V supports the Image Modality.\nrelation<|#|>GLM-4.5V<|#|>Video Modality<|#|>data support, capability<|#|>GLM-4.5V supports the Video Modality.\nrelation<|#|>GLM-4.5V<|#|>Open-Source Models<|#|>model type, classification<|#|>GLM-4.5V is a type of Open-Source Model.\nrelation<|#|>InternVL3.5<|#|>ShanghaiAILab<|#|>development, affiliation<|#|>InternVL3.5 was developed by the organization ShanghaiAILab.\nrelation<|#|>InternVL3.5<|#|>Mixture of Experts<|#|>architecture, design<|#|>InternVL3.5 uses the Mixture of Experts architecture.\nrelation<|#|>InternVL3.5<|#|>Dense Architecture<|#|>architecture, design<|#|>InternVL3.5 uses the Dense Architecture.\nrelation<|#|>InternVL3.5<|#|>Mixed Preference Optimization<|#|>training algorithm, method<|#|>InternVL3.5 is trained with the Mixed Preference Optimization algorithm.\nrelation<|#|>InternVL3.5<|#|>GSPO<|#|>training algorithm, method<|#|>InternVL3.5 is trained with the GSPO algorithm.\nrelation<|#|>InternVL3.5<|#|>Text Modality<|#|>data support, capability<|#|>InternVL3.5 supports the Text Modality.\nrelation<|#|>InternVL3.5<|#|>Image Modality<|#|>data support, capability<|#|>InternVL3.5 supports the Image Modality.\nrelation<|#|>InternVL3.5<|#|>Video Modality<|#|>data support, capability<|#|>InternVL3.5 supports the Video Modality.\nrelation<|#|>InternVL3.5<|#|>Open-Source Models<|#|>model type, classification<|#|>InternVL3.5 is a type of Open-Source Model.\nrelation<|#|>ERNIE-4.5-Thinking<|#|>Baidu<|#|>development, affiliation<|#|>ERNIE-4.5-Thinking was developed by the organization Baidu.\nrelation<|#|>ERNIE-4.5-Thinking<|#|>Mixture of Experts<|#|>architecture, design<|#|>ERNIE-4.5-Thinking uses the Mixture of Experts architecture.\nrelation<|#|>ERNIE-4.5-Thinking<|#|>Text Modality<|#|>data support, capability<|#|>ERNIE-4.5-Thinking supports the Text Modality.\nrelation<|#|>ERNIE-4.5-Thinking<|#|>Open-Source Models<|#|>model type, classification<|#|>ERNIE-4.5-Thinking is a type of Open-Source Model.\n<|COMPLETE|>",
    "cache_type": "extract",
    "chunk_id": "chunk-9f6704aa27cd8e9483d9f72f2cf4c3fe",
    "original_prompt": "---Task---\nExtract entities and relationships from the input text to be processed.\n\n---Instructions---\n1.  **Strict Adherence to Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system prompt.\n2.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\n3.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant entities and relationships have been extracted and presented.\n4.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\n\n<Output>\n---Role---\nYou are a Knowledge Graph Specialist responsible for extracting entities and relationships from the input text.\n\n---Instructions---\n1.  **Entity Extraction & Output:**\n    *   **Identification:** Identify clearly defined and meaningful entities in the input text.\n    *   **Entity Details:** For each identified entity, extract the following information:\n        *   `entity_name`: The name of the entity. If the entity name is case-insensitive, capitalize the first letter of each significant word (title case). Ensure **consistent naming** across the entire extraction process.\n        *   `entity_type`: Categorize the entity using one of the following types: `Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject`. If none of the provided entity types apply, do not add new entity type and classify it as `Other`.\n        *   `entity_description`: Provide a concise yet comprehensive description of the entity's attributes and activities, based *solely* on the information present in the input text.\n    *   **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n        *   Format: `entity<|#|>entity_name<|#|>entity_type<|#|>entity_description`\n\n2.  **Relationship Extraction & Output:**\n    *   **Identification:** Identify direct, clearly stated, and meaningful relationships between previously extracted entities.\n    *   **N-ary Relationship Decomposition:** If a single statement describes a relationship involving more than two entities (an N-ary relationship), decompose it into multiple binary (two-entity) relationship pairs for separate description.\n        *   **Example:** For \"Alice, Bob, and Carol collaborated on Project X,\" extract binary relationships such as \"Alice collaborated with Project X,\" \"Bob collaborated with Project X,\" and \"Carol collaborated with Project X,\" or \"Alice collaborated with Bob,\" based on the most reasonable binary interpretations.\n    *   **Relationship Details:** For each binary relationship, extract the following fields:\n        *   `source_entity`: The name of the source entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `target_entity`: The name of the target entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `relationship_keywords`: One or more high-level keywords summarizing the overarching nature, concepts, or themes of the relationship. Multiple keywords within this field must be separated by a comma `,`. **DO NOT use `<|#|>` for separating multiple keywords within this field.**\n        *   `relationship_description`: A concise explanation of the nature of the relationship between the source and target entities, providing a clear rationale for their connection.\n    *   **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n        *   Format: `relation<|#|>source_entity<|#|>target_entity<|#|>relationship_keywords<|#|>relationship_description`\n\n3.  **Delimiter Usage Protocol:**\n    *   The `<|#|>` is a complete, atomic marker and **must not be filled with content**. It serves strictly as a field separator.\n    *   **Incorrect Example:** `entity<|#|>Tokyo<|location|>Tokyo is the capital of Japan.`\n    *   **Correct Example:** `entity<|#|>Tokyo<|#|>location<|#|>Tokyo is the capital of Japan.`\n\n4.  **Relationship Direction & Duplication:**\n    *   Treat all relationships as **undirected** unless explicitly stated otherwise. Swapping the source and target entities for an undirected relationship does not constitute a new relationship.\n    *   Avoid outputting duplicate relationships.\n\n5.  **Output Order & Prioritization:**\n    *   Output all extracted entities first, followed by all extracted relationships.\n    *   Within the list of relationships, prioritize and output those relationships that are **most significant** to the core meaning of the input text first.\n\n6.  **Context & Objectivity:**\n    *   Ensure all entity names and descriptions are written in the **third person**.\n    *   Explicitly name the subject or object; **avoid using pronouns** such as `this article`, `this paper`, `our company`, `I`, `you`, and `he/she`.\n\n7.  **Language & Proper Nouns:**\n    *   The entire output (entity names, keywords, and descriptions) must be written in `English`.\n    *   Proper nouns (e.g., personal names, place names, organization names) should be retained in their original language if a proper, widely accepted translation is not available or would cause ambiguity.\n\n8.  **Completion Signal:** Output the literal string `<|COMPLETE|>` only after all entities and relationships, following all criteria, have been completely extracted and outputted.\n\n---Examples---\n<Input Text>\n```\nwhile Alex clenched his jaw, the buzz of frustration dull against the backdrop of Taylor's authoritarian certainty. It was this competitive undercurrent that kept him alert, the sense that his and Jordan's shared commitment to discovery was an unspoken rebellion against Cruz's narrowing vision of control and order.\n\nThen Taylor did something unexpected. They paused beside Jordan and, for a moment, observed the device with something akin to reverence. \"If this tech can be understood...\" Taylor said, their voice quieter, \"It could change the game for us. For all of us.\"\n\nThe underlying dismissal earlier seemed to falter, replaced by a glimpse of reluctant respect for the gravity of what lay in their hands. Jordan looked up, and for a fleeting heartbeat, their eyes locked with Taylor's, a wordless clash of wills softening into an uneasy truce.\n\nIt was a small transformation, barely perceptible, but one that Alex noted with an inward nod. They had all been brought here by different paths\n```\n\n<Output>\nentity<|#|>Alex<|#|>person<|#|>Alex is a character who experiences frustration and is observant of the dynamics among other characters.\nentity<|#|>Taylor<|#|>person<|#|>Taylor is portrayed with authoritarian certainty and shows a moment of reverence towards a device, indicating a change in perspective.\nentity<|#|>Jordan<|#|>person<|#|>Jordan shares a commitment to discovery and has a significant interaction with Taylor regarding a device.\nentity<|#|>Cruz<|#|>person<|#|>Cruz is associated with a vision of control and order, influencing the dynamics among other characters.\nentity<|#|>The Device<|#|>equipment<|#|>The Device is central to the story, with potential game-changing implications, and is revered by Taylor.\nrelation<|#|>Alex<|#|>Taylor<|#|>power dynamics, observation<|#|>Alex observes Taylor's authoritarian behavior and notes changes in Taylor's attitude toward the device.\nrelation<|#|>Alex<|#|>Jordan<|#|>shared goals, rebellion<|#|>Alex and Jordan share a commitment to discovery, which contrasts with Cruz's vision.)\nrelation<|#|>Taylor<|#|>Jordan<|#|>conflict resolution, mutual respect<|#|>Taylor and Jordan interact directly regarding the device, leading to a moment of mutual respect and an uneasy truce.\nrelation<|#|>Jordan<|#|>Cruz<|#|>ideological conflict, rebellion<|#|>Jordan's commitment to discovery is in rebellion against Cruz's vision of control and order.\nrelation<|#|>Taylor<|#|>The Device<|#|>reverence, technological significance<|#|>Taylor shows reverence towards the device, indicating its importance and potential impact.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nStock markets faced a sharp downturn today as tech giants saw significant declines, with the global tech index dropping by 3.4% in midday trading. Analysts attribute the selloff to investor concerns over rising interest rates and regulatory uncertainty.\n\nAmong the hardest hit, nexon technologies saw its stock plummet by 7.8% after reporting lower-than-expected quarterly earnings. In contrast, Omega Energy posted a modest 2.1% gain, driven by rising oil prices.\n\nMeanwhile, commodity markets reflected a mixed sentiment. Gold futures rose by 1.5%, reaching $2,080 per ounce, as investors sought safe-haven assets. Crude oil prices continued their rally, climbing to $87.60 per barrel, supported by supply constraints and strong demand.\n\nFinancial experts are closely watching the Federal Reserve's next move, as speculation grows over potential rate hikes. The upcoming policy announcement is expected to influence investor confidence and overall market stability.\n```\n\n<Output>\nentity<|#|>Global Tech Index<|#|>category<|#|>The Global Tech Index tracks the performance of major technology stocks and experienced a 3.4% decline today.\nentity<|#|>Nexon Technologies<|#|>organization<|#|>Nexon Technologies is a tech company that saw its stock decline by 7.8% after disappointing earnings.\nentity<|#|>Omega Energy<|#|>organization<|#|>Omega Energy is an energy company that gained 2.1% in stock value due to rising oil prices.\nentity<|#|>Gold Futures<|#|>product<|#|>Gold futures rose by 1.5%, indicating increased investor interest in safe-haven assets.\nentity<|#|>Crude Oil<|#|>product<|#|>Crude oil prices rose to $87.60 per barrel due to supply constraints and strong demand.\nentity<|#|>Market Selloff<|#|>category<|#|>Market selloff refers to the significant decline in stock values due to investor concerns over interest rates and regulations.\nentity<|#|>Federal Reserve Policy Announcement<|#|>category<|#|>The Federal Reserve's upcoming policy announcement is expected to impact investor confidence and market stability.\nentity<|#|>3.4% Decline<|#|>category<|#|>The Global Tech Index experienced a 3.4% decline in midday trading.\nrelation<|#|>Global Tech Index<|#|>Market Selloff<|#|>market performance, investor sentiment<|#|>The decline in the Global Tech Index is part of the broader market selloff driven by investor concerns.\nrelation<|#|>Nexon Technologies<|#|>Global Tech Index<|#|>company impact, index movement<|#|>Nexon Technologies' stock decline contributed to the overall drop in the Global Tech Index.\nrelation<|#|>Gold Futures<|#|>Market Selloff<|#|>market reaction, safe-haven investment<|#|>Gold prices rose as investors sought safe-haven assets during the market selloff.\nrelation<|#|>Federal Reserve Policy Announcement<|#|>Market Selloff<|#|>interest rate impact, financial regulation<|#|>Speculation over Federal Reserve policy changes contributed to market volatility and investor selloff.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nAt the World Athletics Championship in Tokyo, Noah Carter broke the 100m sprint record using cutting-edge carbon-fiber spikes.\n```\n\n<Output>\nentity<|#|>World Athletics Championship<|#|>event<|#|>The World Athletics Championship is a global sports competition featuring top athletes in track and field.\nentity<|#|>Tokyo<|#|>location<|#|>Tokyo is the host city of the World Athletics Championship.\nentity<|#|>Noah Carter<|#|>person<|#|>Noah Carter is a sprinter who set a new record in the 100m sprint at the World Athletics Championship.\nentity<|#|>100m Sprint Record<|#|>category<|#|>The 100m sprint record is a benchmark in athletics, recently broken by Noah Carter.\nentity<|#|>Carbon-Fiber Spikes<|#|>equipment<|#|>Carbon-fiber spikes are advanced sprinting shoes that provide enhanced speed and traction.\nentity<|#|>World Athletics Federation<|#|>organization<|#|>The World Athletics Federation is the governing body overseeing the World Athletics Championship and record validations.\nrelation<|#|>World Athletics Championship<|#|>Tokyo<|#|>event location, international competition<|#|>The World Athletics Championship is being hosted in Tokyo.\nrelation<|#|>Noah Carter<|#|>100m Sprint Record<|#|>athlete achievement, record-breaking<|#|>Noah Carter set a new 100m sprint record at the championship.\nrelation<|#|>Noah Carter<|#|>Carbon-Fiber Spikes<|#|>athletic equipment, performance boost<|#|>Noah Carter used carbon-fiber spikes to enhance performance during the race.\nrelation<|#|>Noah Carter<|#|>World Athletics Championship<|#|>athlete participation, competition<|#|>Noah Carter is competing at the World Athletics Championship.\n<|COMPLETE|>\n\n\n\n---Real Data to be Processed---\n<Input>\nEntity_types: [Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject]\nText:\n```\nASurveyofReinforcementLearningforLargeReasoningModels\nTable 1 | Comparison of representative open-source models trained with RL. OPMD denotes Online\nPolicy Mirror Descent; MPO denotes Mixed Preference Optimization; CISPO denotes Clipped IS-\nweight Policy Optimization. T, I, and V indicate Text, Image, and Video modalities, respectively.\nDate Model Organization Architecture Parameters Algorithm Modal Link\nDeepSeek-R1\n2025.01 DeepSeek MoE/MLA 671B GRPO Text \n[Guoetal.,2025a]\nORZ\n2025.03 StepAI Dense 0.5-32B PPO Text \n[Huetal.,2025b]\nQwQ\n2025.03 AlibabaQwen Dense 32B - Text \n[Team,2025g]\nPhi-4Reasoning\n2025.04 Microsoft Dense 14B GRPO Text \n[Abdinetal.,2025]\nSkywork-R1V2\n2025.04 Skywork Dense 38B MPO/GRPO T/I \n[Wangetal.,2025k]\nInternVL3\n2025.04 ShanghaiAILab Dense 1-78B MPO T/I/V \n[Zhuetal.,2025c]\nMiMo\n2025.04 Xiaomi Dense 7B GRPO Text \n[Xiaomietal.,2025]\nQwen3\n2025.04 AlibabaQwen MoE/Dense 0.6-235B GRPO Text \n[Yangetal.,2025a]\nLlama-Nemotron-Ultra\n2025.05 NVIDIA Dense 253B GRPO Text \n[Bercovichetal.,2025]\nINTELLECT-2\n2025.05 IntellectAI Dense 32B GRPO Text\n[Teametal.,2025b]\nHunyuan-TurboS\n2025.05 Tencent HybridMoE 560B GRPO Text \n[Teametal.,2025c]\nSkyworkOR-1\n2025.05 Skywork Dense 7B/32B GRPO Text \n[Heetal.,2025d]\nDeepSeek-R1-0528\n2025.05 DeepSeek MoE/MLA 671B GRPO Text \n[Guoetal.,2025a]\nMagistral\n2025.06 MistralAI Dense 24B GRPO Text\n[Rastogietal.,2025]\nMinimax-M1\n2025.06 Minimax HybridMoE 456B CISPO Text \n[Chenetal.,2025a]\nIntern-S1\n2025.07 ShanghaiAILab MoE 241B GRPO T/I/V \n[Baietal.,2025]\nKimiK2\n2025.07 Kimi MoE 1T OPMD Text \n[Team,2025c]\nStep3\n2025.07 StepAI MoE 321B - T/I/V \n[Wangetal.,2025a]\nQwen3-2507\n2025.07 AlibabaQwen MoE/Dense 4-235B GSPO Text \n[Yangetal.,2025a]\nGLM-4.1V-Thinking\n2025.07 ZhipuAI Dense 9B GRPO T/I/V \n[Teametal.,2025a]\nGLM-4.5\n2025.07 ZhipuAI MoE 355B GRPO Text \n[Zengetal.,2025a]\nSkywork-R1V3\n2025.07 Skywork Dense 38B GRPO T/I \n[Shenetal.,2025b]\ngpt-oss\n2025.08 OpenAI MoE 117B/21B - Text \n[Agarwaletal.,2025a]\nSeed-OSS\n2025.08 BytedanceSeed Dense 36B - Text \n[Team,2025a]\nGLM-4.5V\n2025.08 ZhipuAI MoE 106B GRPO T/I/V \n[Teametal.,2025a]\nInternVL3.5\n2025.08 ShanghaiAILab MoE/Dense 1-241B MPO/GSPO T/I/V \n[Wangetal.,2025o]\nERNIE-4.5-Thinking\n2025.09 Baidu MoE 21B-A3B - Text\n[Baidu-ERNIE-Team,2025]\n9\n```",
    "queryparam": null,
    "create_time": 1765219247,
    "update_time": 1765219247,
    "_id": "default:extract:1652c44ed750bb3051f42a533083dd2e"
  },
  "default:extract:af0b6e687e220b0cf3245a40a244f515": {
    "return": "entity<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>Content<|#|>The title of the survey document, which focuses on the application of reinforcement learning to large reasoning models.\nentity<|#|>Table 1<|#|>Data<|#|>A table presenting a comparison of representative open-source models trained with Reinforcement Learning.\nrelation<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>Reinforcement Learning<|#|>focus, subject matter<|#|>The survey document focuses on the topic of Reinforcement Learning.\nrelation<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>Large Reasoning Models<|#|>focus, subject matter<|#|>The survey document focuses on the topic of Large Reasoning Models.\nrelation<|#|>Table 1<|#|>A Survey of Reinforcement Learning for Large Reasoning Models<|#|>part of, content<|#|>Table 1 is a component of the survey document.\nrelation<|#|>Table 1<|#|>Open-Source Models<|#|>comparison, data presentation<|#|>Table 1 provides a comparison of open-source models.\n<|COMPLETE|>",
    "cache_type": "extract",
    "chunk_id": "chunk-9f6704aa27cd8e9483d9f72f2cf4c3fe",
    "original_prompt": "---Task---\nBased on the last extraction task, identify and extract any **missed or incorrectly formatted** entities and relationships from the input text.\n\n---Instructions---\n1.  **Strict Adherence to System Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system instructions.\n2.  **Focus on Corrections/Additions:**\n    *   **Do NOT** re-output entities and relationships that were **correctly and fully** extracted in the last task.\n    *   If an entity or relationship was **missed** in the last task, extract and output it now according to the system format.\n    *   If an entity or relationship was **truncated, had missing fields, or was otherwise incorrectly formatted** in the last task, re-output the *corrected and complete* version in the specified format.\n3.  **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n4.  **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n5.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\n6.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant missing or corrected entities and relationships have been extracted and presented.\n7.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\n\n<Output>\n---Role---\nYou are a Knowledge Graph Specialist responsible for extracting entities and relationships from the input text.\n\n---Instructions---\n1.  **Entity Extraction & Output:**\n    *   **Identification:** Identify clearly defined and meaningful entities in the input text.\n    *   **Entity Details:** For each identified entity, extract the following information:\n        *   `entity_name`: The name of the entity. If the entity name is case-insensitive, capitalize the first letter of each significant word (title case). Ensure **consistent naming** across the entire extraction process.\n        *   `entity_type`: Categorize the entity using one of the following types: `Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject`. If none of the provided entity types apply, do not add new entity type and classify it as `Other`.\n        *   `entity_description`: Provide a concise yet comprehensive description of the entity's attributes and activities, based *solely* on the information present in the input text.\n    *   **Output Format - Entities:** Output a total of 4 fields for each entity, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `entity`.\n        *   Format: `entity<|#|>entity_name<|#|>entity_type<|#|>entity_description`\n\n2.  **Relationship Extraction & Output:**\n    *   **Identification:** Identify direct, clearly stated, and meaningful relationships between previously extracted entities.\n    *   **N-ary Relationship Decomposition:** If a single statement describes a relationship involving more than two entities (an N-ary relationship), decompose it into multiple binary (two-entity) relationship pairs for separate description.\n        *   **Example:** For \"Alice, Bob, and Carol collaborated on Project X,\" extract binary relationships such as \"Alice collaborated with Project X,\" \"Bob collaborated with Project X,\" and \"Carol collaborated with Project X,\" or \"Alice collaborated with Bob,\" based on the most reasonable binary interpretations.\n    *   **Relationship Details:** For each binary relationship, extract the following fields:\n        *   `source_entity`: The name of the source entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `target_entity`: The name of the target entity. Ensure **consistent naming** with entity extraction. Capitalize the first letter of each significant word (title case) if the name is case-insensitive.\n        *   `relationship_keywords`: One or more high-level keywords summarizing the overarching nature, concepts, or themes of the relationship. Multiple keywords within this field must be separated by a comma `,`. **DO NOT use `<|#|>` for separating multiple keywords within this field.**\n        *   `relationship_description`: A concise explanation of the nature of the relationship between the source and target entities, providing a clear rationale for their connection.\n    *   **Output Format - Relationships:** Output a total of 5 fields for each relationship, delimited by `<|#|>`, on a single line. The first field *must* be the literal string `relation`.\n        *   Format: `relation<|#|>source_entity<|#|>target_entity<|#|>relationship_keywords<|#|>relationship_description`\n\n3.  **Delimiter Usage Protocol:**\n    *   The `<|#|>` is a complete, atomic marker and **must not be filled with content**. It serves strictly as a field separator.\n    *   **Incorrect Example:** `entity<|#|>Tokyo<|location|>Tokyo is the capital of Japan.`\n    *   **Correct Example:** `entity<|#|>Tokyo<|#|>location<|#|>Tokyo is the capital of Japan.`\n\n4.  **Relationship Direction & Duplication:**\n    *   Treat all relationships as **undirected** unless explicitly stated otherwise. Swapping the source and target entities for an undirected relationship does not constitute a new relationship.\n    *   Avoid outputting duplicate relationships.\n\n5.  **Output Order & Prioritization:**\n    *   Output all extracted entities first, followed by all extracted relationships.\n    *   Within the list of relationships, prioritize and output those relationships that are **most significant** to the core meaning of the input text first.\n\n6.  **Context & Objectivity:**\n    *   Ensure all entity names and descriptions are written in the **third person**.\n    *   Explicitly name the subject or object; **avoid using pronouns** such as `this article`, `this paper`, `our company`, `I`, `you`, and `he/she`.\n\n7.  **Language & Proper Nouns:**\n    *   The entire output (entity names, keywords, and descriptions) must be written in `English`.\n    *   Proper nouns (e.g., personal names, place names, organization names) should be retained in their original language if a proper, widely accepted translation is not available or would cause ambiguity.\n\n8.  **Completion Signal:** Output the literal string `<|COMPLETE|>` only after all entities and relationships, following all criteria, have been completely extracted and outputted.\n\n---Examples---\n<Input Text>\n```\nwhile Alex clenched his jaw, the buzz of frustration dull against the backdrop of Taylor's authoritarian certainty. It was this competitive undercurrent that kept him alert, the sense that his and Jordan's shared commitment to discovery was an unspoken rebellion against Cruz's narrowing vision of control and order.\n\nThen Taylor did something unexpected. They paused beside Jordan and, for a moment, observed the device with something akin to reverence. \"If this tech can be understood...\" Taylor said, their voice quieter, \"It could change the game for us. For all of us.\"\n\nThe underlying dismissal earlier seemed to falter, replaced by a glimpse of reluctant respect for the gravity of what lay in their hands. Jordan looked up, and for a fleeting heartbeat, their eyes locked with Taylor's, a wordless clash of wills softening into an uneasy truce.\n\nIt was a small transformation, barely perceptible, but one that Alex noted with an inward nod. They had all been brought here by different paths\n```\n\n<Output>\nentity<|#|>Alex<|#|>person<|#|>Alex is a character who experiences frustration and is observant of the dynamics among other characters.\nentity<|#|>Taylor<|#|>person<|#|>Taylor is portrayed with authoritarian certainty and shows a moment of reverence towards a device, indicating a change in perspective.\nentity<|#|>Jordan<|#|>person<|#|>Jordan shares a commitment to discovery and has a significant interaction with Taylor regarding a device.\nentity<|#|>Cruz<|#|>person<|#|>Cruz is associated with a vision of control and order, influencing the dynamics among other characters.\nentity<|#|>The Device<|#|>equipment<|#|>The Device is central to the story, with potential game-changing implications, and is revered by Taylor.\nrelation<|#|>Alex<|#|>Taylor<|#|>power dynamics, observation<|#|>Alex observes Taylor's authoritarian behavior and notes changes in Taylor's attitude toward the device.\nrelation<|#|>Alex<|#|>Jordan<|#|>shared goals, rebellion<|#|>Alex and Jordan share a commitment to discovery, which contrasts with Cruz's vision.)\nrelation<|#|>Taylor<|#|>Jordan<|#|>conflict resolution, mutual respect<|#|>Taylor and Jordan interact directly regarding the device, leading to a moment of mutual respect and an uneasy truce.\nrelation<|#|>Jordan<|#|>Cruz<|#|>ideological conflict, rebellion<|#|>Jordan's commitment to discovery is in rebellion against Cruz's vision of control and order.\nrelation<|#|>Taylor<|#|>The Device<|#|>reverence, technological significance<|#|>Taylor shows reverence towards the device, indicating its importance and potential impact.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nStock markets faced a sharp downturn today as tech giants saw significant declines, with the global tech index dropping by 3.4% in midday trading. Analysts attribute the selloff to investor concerns over rising interest rates and regulatory uncertainty.\n\nAmong the hardest hit, nexon technologies saw its stock plummet by 7.8% after reporting lower-than-expected quarterly earnings. In contrast, Omega Energy posted a modest 2.1% gain, driven by rising oil prices.\n\nMeanwhile, commodity markets reflected a mixed sentiment. Gold futures rose by 1.5%, reaching $2,080 per ounce, as investors sought safe-haven assets. Crude oil prices continued their rally, climbing to $87.60 per barrel, supported by supply constraints and strong demand.\n\nFinancial experts are closely watching the Federal Reserve's next move, as speculation grows over potential rate hikes. The upcoming policy announcement is expected to influence investor confidence and overall market stability.\n```\n\n<Output>\nentity<|#|>Global Tech Index<|#|>category<|#|>The Global Tech Index tracks the performance of major technology stocks and experienced a 3.4% decline today.\nentity<|#|>Nexon Technologies<|#|>organization<|#|>Nexon Technologies is a tech company that saw its stock decline by 7.8% after disappointing earnings.\nentity<|#|>Omega Energy<|#|>organization<|#|>Omega Energy is an energy company that gained 2.1% in stock value due to rising oil prices.\nentity<|#|>Gold Futures<|#|>product<|#|>Gold futures rose by 1.5%, indicating increased investor interest in safe-haven assets.\nentity<|#|>Crude Oil<|#|>product<|#|>Crude oil prices rose to $87.60 per barrel due to supply constraints and strong demand.\nentity<|#|>Market Selloff<|#|>category<|#|>Market selloff refers to the significant decline in stock values due to investor concerns over interest rates and regulations.\nentity<|#|>Federal Reserve Policy Announcement<|#|>category<|#|>The Federal Reserve's upcoming policy announcement is expected to impact investor confidence and market stability.\nentity<|#|>3.4% Decline<|#|>category<|#|>The Global Tech Index experienced a 3.4% decline in midday trading.\nrelation<|#|>Global Tech Index<|#|>Market Selloff<|#|>market performance, investor sentiment<|#|>The decline in the Global Tech Index is part of the broader market selloff driven by investor concerns.\nrelation<|#|>Nexon Technologies<|#|>Global Tech Index<|#|>company impact, index movement<|#|>Nexon Technologies' stock decline contributed to the overall drop in the Global Tech Index.\nrelation<|#|>Gold Futures<|#|>Market Selloff<|#|>market reaction, safe-haven investment<|#|>Gold prices rose as investors sought safe-haven assets during the market selloff.\nrelation<|#|>Federal Reserve Policy Announcement<|#|>Market Selloff<|#|>interest rate impact, financial regulation<|#|>Speculation over Federal Reserve policy changes contributed to market volatility and investor selloff.\n<|COMPLETE|>\n\n\n<Input Text>\n```\nAt the World Athletics Championship in Tokyo, Noah Carter broke the 100m sprint record using cutting-edge carbon-fiber spikes.\n```\n\n<Output>\nentity<|#|>World Athletics Championship<|#|>event<|#|>The World Athletics Championship is a global sports competition featuring top athletes in track and field.\nentity<|#|>Tokyo<|#|>location<|#|>Tokyo is the host city of the World Athletics Championship.\nentity<|#|>Noah Carter<|#|>person<|#|>Noah Carter is a sprinter who set a new record in the 100m sprint at the World Athletics Championship.\nentity<|#|>100m Sprint Record<|#|>category<|#|>The 100m sprint record is a benchmark in athletics, recently broken by Noah Carter.\nentity<|#|>Carbon-Fiber Spikes<|#|>equipment<|#|>Carbon-fiber spikes are advanced sprinting shoes that provide enhanced speed and traction.\nentity<|#|>World Athletics Federation<|#|>organization<|#|>The World Athletics Federation is the governing body overseeing the World Athletics Championship and record validations.\nrelation<|#|>World Athletics Championship<|#|>Tokyo<|#|>event location, international competition<|#|>The World Athletics Championship is being hosted in Tokyo.\nrelation<|#|>Noah Carter<|#|>100m Sprint Record<|#|>athlete achievement, record-breaking<|#|>Noah Carter set a new 100m sprint record at the championship.\nrelation<|#|>Noah Carter<|#|>Carbon-Fiber Spikes<|#|>athletic equipment, performance boost<|#|>Noah Carter used carbon-fiber spikes to enhance performance during the race.\nrelation<|#|>Noah Carter<|#|>World Athletics Championship<|#|>athlete participation, competition<|#|>Noah Carter is competing at the World Athletics Championship.\n<|COMPLETE|>\n\n\n\n---Real Data to be Processed---\n<Input>\nEntity_types: [Person,Creature,Organization,Location,Event,Concept,Method,Content,Data,Artifact,NaturalObject]\nText:\n```\nASurveyofReinforcementLearningforLargeReasoningModels\nTable 1 | Comparison of representative open-source models trained with RL. OPMD denotes Online\nPolicy Mirror Descent; MPO denotes Mixed Preference Optimization; CISPO denotes Clipped IS-\nweight Policy Optimization. T, I, and V indicate Text, Image, and Video modalities, respectively.\nDate Model Organization Architecture Parameters Algorithm Modal Link\nDeepSeek-R1\n2025.01 DeepSeek MoE/MLA 671B GRPO Text \n[Guoetal.,2025a]\nORZ\n2025.03 StepAI Dense 0.5-32B PPO Text \n[Huetal.,2025b]\nQwQ\n2025.03 AlibabaQwen Dense 32B - Text \n[Team,2025g]\nPhi-4Reasoning\n2025.04 Microsoft Dense 14B GRPO Text \n[Abdinetal.,2025]\nSkywork-R1V2\n2025.04 Skywork Dense 38B MPO/GRPO T/I \n[Wangetal.,2025k]\nInternVL3\n2025.04 ShanghaiAILab Dense 1-78B MPO T/I/V \n[Zhuetal.,2025c]\nMiMo\n2025.04 Xiaomi Dense 7B GRPO Text \n[Xiaomietal.,2025]\nQwen3\n2025.04 AlibabaQwen MoE/Dense 0.6-235B GRPO Text \n[Yangetal.,2025a]\nLlama-Nemotron-Ultra\n2025.05 NVIDIA Dense 253B GRPO Text \n[Bercovichetal.,2025]\nINTELLECT-2\n2025.05 IntellectAI Dense 32B GRPO Text\n[Teametal.,2025b]\nHunyuan-TurboS\n2025.05 Tencent HybridMoE 560B GRPO Text \n[Teametal.,2025c]\nSkyworkOR-1\n2025.05 Skywork Dense 7B/32B GRPO Text \n[Heetal.,2025d]\nDeepSeek-R1-0528\n2025.05 DeepSeek MoE/MLA 671B GRPO Text \n[Guoetal.,2025a]\nMagistral\n2025.06 MistralAI Dense 24B GRPO Text\n[Rastogietal.,2025]\nMinimax-M1\n2025.06 Minimax HybridMoE 456B CISPO Text \n[Chenetal.,2025a]\nIntern-S1\n2025.07 ShanghaiAILab MoE 241B GRPO T/I/V \n[Baietal.,2025]\nKimiK2\n2025.07 Kimi MoE 1T OPMD Text \n[Team,2025c]\nStep3\n2025.07 StepAI MoE 321B - T/I/V \n[Wangetal.,2025a]\nQwen3-2507\n2025.07 AlibabaQwen MoE/Dense 4-235B GSPO Text \n[Yangetal.,2025a]\nGLM-4.1V-Thinking\n2025.07 ZhipuAI Dense 9B GRPO T/I/V \n[Teametal.,2025a]\nGLM-4.5\n2025.07 ZhipuAI MoE 355B GRPO Text \n[Zengetal.,2025a]\nSkywork-R1V3\n2025.07 Skywork Dense 38B GRPO T/I \n[Shenetal.,2025b]\ngpt-oss\n2025.08 OpenAI MoE 117B/21B - Text \n[Agarwaletal.,2025a]\nSeed-OSS\n2025.08 BytedanceSeed Dense 36B - Text \n[Team,2025a]\nGLM-4.5V\n2025.08 ZhipuAI MoE 106B GRPO T/I/V \n[Teametal.,2025a]\nInternVL3.5\n2025.08 ShanghaiAILab MoE/Dense 1-241B MPO/GSPO T/I/V \n[Wangetal.,2025o]\nERNIE-4.5-Thinking\n2025.09 Baidu MoE 21B-A3B - Text\n[Baidu-ERNIE-Team,2025]\n9\n```\n[{\"role\": \"user\", \"content\": \"---Task---\\nExtract entities and relationships from the input text to be processed.\\n\\n---Instructions---\\n1.  **Strict Adherence to Format:** Strictly adhere to all format requirements for entity and relationship lists, including output order, field delimiters, and proper noun handling, as specified in the system prompt.\\n2.  **Output Content Only:** Output *only* the extracted list of entities and relationships. Do not include any introductory or concluding remarks, explanations, or additional text before or after the list.\\n3.  **Completion Signal:** Output `<|COMPLETE|>` as the final line after all relevant entities and relationships have been extracted and presented.\\n4.  **Output Language:** Ensure the output language is English. Proper nouns (e.g., personal names, place names, organization names) must be kept in their original language and not translated.\\n\\n<Output>\"}, {\"role\": \"assistant\", \"content\": \"entity<|#|>Reinforcement Learning<|#|>Concept<|#|>A machine learning paradigm used for training models, central to the survey of large reasoning models.\\nentity<|#|>Large Reasoning Models<|#|>Concept<|#|>A category of models that are the subject of the survey, often trained with reinforcement learning.\\nentity<|#|>Online Policy Mirror Descent<|#|>Concept<|#|>An algorithm used for training models, denoted as OPMD.\\nentity<|#|>Mixed Preference Optimization<|#|>Concept<|#|>An algorithm used for training models, denoted as MPO.\\nentity<|#|>Clipped IS-weight Policy Optimization<|#|>Concept<|#|>An algorithm used for training models, denoted as CISPO.\\nentity<|#|>Text Modality<|#|>Concept<|#|>A type of data modality, indicated by 'T', supported by various models.\\nentity<|#|>Image Modality<|#|>Concept<|#|>A type of data modality, indicated by 'I', supported by various models.\\nentity<|#|>Video Modality<|#|>Concept<|#|>A type of data modality, indicated by 'V', supported by various models.\\nentity<|#|>Open-Source Models<|#|>Concept<|#|>Representative models that are publicly available and trained with Reinforcement Learning.\\nentity<|#|>GRPO<|#|>Concept<|#|>An algorithm used for training models.\\nentity<|#|>PPO<|#|>Concept<|#|>An algorithm used for training models.\\nentity<|#|>GSPO<|#|>Concept<|#|>An algorithm used for training models.\\nentity<|#|>Mixture of Experts<|#|>Concept<|#|>A model architecture, denoted as MoE.\\nentity<|#|>Multi-Layer Attention<|#|>Concept<|#|>A model architecture, denoted as MLA.\\nentity<|#|>Dense Architecture<|#|>Concept<|#|>A model architecture.\\nentity<|#|>Hybrid Mixture of Experts<|#|>Concept<|#|>A model architecture, denoted as HybridMoE.\\nentity<|#|>DeepSeek<|#|>Organization<|#|>An organization that develops large reasoning models.\\nentity<|#|>StepAI<|#|>Organization<|#|>An organization that develops large reasoning models.\\nentity<|#|>AlibabaQwen<|#|>Organization<|#|>An organization that develops large reasoning models.\\nentity<|#|>Microsoft<|#|>Organization<|#|>An organization that develops large reasoning models.\\nentity<|#|>Skywork<|#|>Organization<|#|>An organization that develops large reasoning models.\\nentity<|#|>ShanghaiAILab<|#|>Organization<|#|>An organization that develops large reasoning models.\\nentity<|#|>Xiaomi<|#|>Organization<|#|>An organization that develops large reasoning models.\\nentity<|#|>NVIDIA<|#|>Organization<|#|>An organization that develops large reasoning models.\\nentity<|#|>IntellectAI<|#|>Organization<|#|>An organization that develops large reasoning models.\\nentity<|#|>Tencent<|#|>Organization<|#|>An organization that develops large reasoning models.\\nentity<|#|>MistralAI<|#|>Organization<|#|>An organization that develops large reasoning models.\\nentity<|#|>Minimax<|#|>Organization<|#|>An organization that develops large reasoning models.\\nentity<|#|>Kimi<|#|>Organization<|#|>An organization that develops large reasoning models.\\nentity<|#|>ZhipuAI<|#|>Organization<|#|>An organization that develops large reasoning models.\\nentity<|#|>OpenAI<|#|>Organization<|#|>An organization that develops large reasoning models.\\nentity<|#|>BytedanceSeed<|#|>Organization<|#|>An organization that develops large reasoning models.\\nentity<|#|>Baidu<|#|>Organization<|#|>An organization that develops large reasoning models.\\nentity<|#|>DeepSeek-R1<|#|>Artifact<|#|>An open-source model developed by DeepSeek with MoE/MLA architecture and 671B parameters, trained with GRPO for Text modality.\\nentity<|#|>ORZ<|#|>Artifact<|#|>An open-source model developed by StepAI with Dense architecture and 0.5-32B parameters, trained with PPO for Text modality.\\nentity<|#|>QwQ<|#|>Artifact<|#|>An open-source model developed by AlibabaQwen with Dense architecture and 32B parameters for Text modality.\\nentity<|#|>Phi-4Reasoning<|#|>Artifact<|#|>An open-source model developed by Microsoft with Dense architecture and 14B parameters, trained with GRPO for Text modality.\\nentity<|#|>Skywork-R1V2<|#|>Artifact<|#|>An open-source model developed by Skywork with Dense architecture and 38B parameters, trained with MPO/GRPO for Text and Image modalities.\\nentity<|#|>InternVL3<|#|>Artifact<|#|>An open-source model developed by ShanghaiAILab with Dense architecture and 1-78B parameters, trained with MPO for Text, Image, and Video modalities.\\nentity<|#|>MiMo<|#|>Artifact<|#|>An open-source model developed by Xiaomi with Dense architecture and 7B parameters, trained with GRPO for Text modality.\\nentity<|#|>Qwen3<|#|>Artifact<|#|>An open-source model developed by AlibabaQwen with MoE/Dense architecture and 0.6-235B parameters, trained with GRPO for Text modality.\\nentity<|#|>Llama-Nemotron-Ultra<|#|>Artifact<|#|>An open-source model developed by NVIDIA with Dense architecture and 253B parameters, trained with GRPO for Text modality.\\nentity<|#|>INTELLECT-2<|#|>Artifact<|#|>An open-source model developed by IntellectAI with Dense architecture and 32B parameters, trained with GRPO for Text modality.\\nentity<|#|>Hunyuan-TurboS<|#|>Artifact<|#|>An open-source model developed by Tencent with HybridMoE architecture and 560B parameters, trained with GRPO for Text modality.\\nentity<|#|>SkyworkOR-1<|#|>Artifact<|#|>An open-source model developed by Skywork with Dense architecture and 7B/32B parameters, trained with GRPO for Text modality.\\nentity<|#|>DeepSeek-R1-0528<|#|>Artifact<|#|>An open-source model developed by DeepSeek with MoE/MLA architecture and 671B parameters, trained with GRPO for Text modality.\\nentity<|#|>Magistral<|#|>Artifact<|#|>An open-source model developed by MistralAI with Dense architecture and 24B parameters, trained with GRPO for Text modality.\\nentity<|#|>Minimax-M1<|#|>Artifact<|#|>An open-source model developed by Minimax with HybridMoE architecture and 456B parameters, trained with CISPO for Text modality.\\nentity<|#|>Intern-S1<|#|>Artifact<|#|>An open-source model developed by ShanghaiAILab with MoE architecture and 241B parameters, trained with GRPO for Text, Image, and Video modalities.\\nentity<|#|>KimiK2<|#|>Artifact<|#|>An open-source model developed by Kimi with MoE architecture and 1T parameters, trained with Online Policy Mirror Descent for Text modality.\\nentity<|#|>Step3<|#|>Artifact<|#|>An open-source model developed by StepAI with MoE architecture and 321B parameters for Text, Image, and Video modalities.\\nentity<|#|>Qwen3-2507<|#|>Artifact<|#|>An open-source model developed by AlibabaQwen with MoE/Dense architecture and 4-235B parameters, trained with GSPO for Text modality.\\nentity<|#|>GLM-4.1V-Thinking<|#|>Artifact<|#|>An open-source model developed by ZhipuAI with Dense architecture and 9B parameters, trained with GRPO for Text, Image, and Video modalities.\\nentity<|#|>GLM-4.5<|#|>Artifact<|#|>An open-source model developed by ZhipuAI with MoE architecture and 355B parameters, trained with GRPO for Text modality.\\nentity<|#|>Skywork-R1V3<|#|>Artifact<|#|>An open-source model developed by Skywork with Dense architecture and 38B parameters, trained with GRPO for Text and Image modalities.\\nentity<|#|>gpt-oss<|#|>Artifact<|#|>An open-source model developed by OpenAI with MoE architecture and 117B/21B parameters for Text modality.\\nentity<|#|>Seed-OSS<|#|>Artifact<|#|>An open-source model developed by BytedanceSeed with Dense architecture and 36B parameters for Text modality.\\nentity<|#|>GLM-4.5V<|#|>Artifact<|#|>An open-source model developed by ZhipuAI with MoE architecture and 106B parameters, trained with GRPO for Text, Image, and Video modalities.\\nentity<|#|>InternVL3.5<|#|>Artifact<|#|>An open-source model developed by ShanghaiAILab with MoE/Dense architecture and 1-241B parameters, trained with MPO/GSPO for Text, Image, and Video modalities.\\nentity<|#|>ERNIE-4.5-Thinking<|#|>Artifact<|#|>An open-source model developed by Baidu with MoE architecture and 21B-A3B parameters for Text modality.\\nrelation<|#|>Open-Source Models<|#|>Reinforcement Learning<|#|>training method, application<|#|>Open-source models are trained using Reinforcement Learning.\\nrelation<|#|>Online Policy Mirror Descent<|#|>Reinforcement Learning<|#|>algorithm type, category<|#|>Online Policy Mirror Descent is an algorithm used in Reinforcement Learning.\\nrelation<|#|>Mixed Preference Optimization<|#|>Reinforcement Learning<|#|>algorithm type, category<|#|>Mixed Preference Optimization is an algorithm used in Reinforcement Learning.\\nrelation<|#|>Clipped IS-weight Policy Optimization<|#|>Reinforcement Learning<|#|>algorithm type, category<|#|>Clipped IS-weight Policy Optimization is an algorithm used in Reinforcement Learning.\\nrelation<|#|>GRPO<|#|>Reinforcement Learning<|#|>algorithm type, category<|#|>GRPO is an algorithm used in Reinforcement Learning.\\nrelation<|#|>PPO<|#|>Reinforcement Learning<|#|>algorithm type, category<|#|>PPO is an algorithm used in Reinforcement Learning.\\nrelation<|#|>GSPO<|#|>Reinforcement Learning<|#|>algorithm type, category<|#|>GSPO is an algorithm used in Reinforcement Learning.\\nrelation<|#|>DeepSeek-R1<|#|>DeepSeek<|#|>development, affiliation<|#|>DeepSeek-R1 was developed by the organization DeepSeek.\\nrelation<|#|>DeepSeek-R1<|#|>Mixture of Experts<|#|>architecture, design<|#|>DeepSeek-R1 uses the Mixture of Experts architecture.\\nrelation<|#|>DeepSeek-R1<|#|>Multi-Layer Attention<|#|>architecture, design<|#|>DeepSeek-R1 uses the Multi-Layer Attention architecture.\\nrelation<|#|>DeepSeek-R1<|#|>GRPO<|#|>training algorithm, method<|#|>DeepSeek-R1 is trained with the GRPO algorithm.\\nrelation<|#|>DeepSeek-R1<|#|>Text Modality<|#|>data support, capability<|#|>DeepSeek-R1 supports the Text Modality.\\nrelation<|#|>DeepSeek-R1<|#|>Open-Source Models<|#|>model type, classification<|#|>DeepSeek-R1 is a type of Open-Source Model.\\nrelation<|#|>ORZ<|#|>StepAI<|#|>development, affiliation<|#|>ORZ was developed by the organization StepAI.\\nrelation<|#|>ORZ<|#|>Dense Architecture<|#|>architecture, design<|#|>ORZ uses the Dense Architecture.\\nrelation<|#|>ORZ<|#|>PPO<|#|>training algorithm, method<|#|>ORZ is trained with the PPO algorithm.\\nrelation<#|>ORZ<|#|>Text Modality<|#|>data support, capability<|#|>ORZ supports the Text Modality.\\nrelation<|#|>ORZ<|#|>Open-Source Models<|#|>model type, classification<|#|>ORZ is a type of Open-Source Model.\\nrelation<|#|>QwQ<|#|>AlibabaQwen<|#|>development, affiliation<|#|>QwQ was developed by the organization AlibabaQwen.\\nrelation<|#|>QwQ<|#|>Dense Architecture<|#|>architecture, design<|#|>QwQ uses the Dense Architecture.\\nrelation<|#|>QwQ<|#|>Text Modality<|#|>data support, capability<|#|>QwQ supports the Text Modality.\\nrelation<|#|>QwQ<|#|>Open-Source Models<|#|>model type, classification<|#|>QwQ is a type of Open-Source Model.\\nrelation<|#|>Phi-4Reasoning<|#|>Microsoft<|#|>development, affiliation<|#|>Phi-4Reasoning was developed by the organization Microsoft.\\nrelation<|#|>Phi-4Reasoning<|#|>Dense Architecture<|#|>architecture, design<|#|>Phi-4Reasoning uses the Dense Architecture.\\nrelation<|#|>Phi-4Reasoning<|#|>GRPO<|#|>training algorithm, method<|#|>Phi-4Reasoning is trained with the GRPO algorithm.\\nrelation<|#|>Phi-4Reasoning<|#|>Text Modality<|#|>data support, capability<|#|>Phi-4Reasoning supports the Text Modality.\\nrelation<|#|>Phi-4Reasoning<|#|>Open-Source Models<|#|>model type, classification<|#|>Phi-4Reasoning is a type of Open-Source Model.\\nrelation<|#|>Skywork-R1V2<|#|>Skywork<|#|>development, affiliation<|#|>Skywork-R1V2 was developed by the organization Skywork.\\nrelation<|#|>Skywork-R1V2<|#|>Dense Architecture<|#|>architecture, design<|#|>Skywork-R1V2 uses the Dense Architecture.\\nrelation<|#|>Skywork-R1V2<|#|>Mixed Preference Optimization<|#|>training algorithm, method<|#|>Skywork-R1V2 is trained with the Mixed Preference Optimization algorithm.\\nrelation<|#|>Skywork-R1V2<|#|>GRPO<|#|>training algorithm, method<|#|>Skywork-R1V2 is trained with the GRPO algorithm.\\nrelation<|#|>Skywork-R1V2<|#|>Text Modality<|#|>data support, capability<|#|>Skywork-R1V2 supports the Text Modality.\\nrelation<|#|>Skywork-R1V2<|#|>Image Modality<|#|>data support, capability<|#|>Skywork-R1V2 supports the Image Modality.\\nrelation<|#|>Skywork-R1V2<|#|>Open-Source Models<|#|>model type, classification<|#|>Skywork-R1V2 is a type of Open-Source Model.\\nrelation<|#|>InternVL3<|#|>ShanghaiAILab<|#|>development, affiliation<|#|>InternVL3 was developed by the organization ShanghaiAILab.\\nrelation<|#|>InternVL3<|#|>Dense Architecture<|#|>architecture, design<|#|>InternVL3 uses the Dense Architecture.\\nrelation<|#|>InternVL3<|#|>Mixed Preference Optimization<|#|>training algorithm, method<|#|>InternVL3 is trained with the Mixed Preference Optimization algorithm.\\nrelation<|#|>InternVL3<|#|>Text Modality<|#|>data support, capability<|#|>InternVL3 supports the Text Modality.\\nrelation<|#|>InternVL3<|#|>Image Modality<|#|>data support, capability<|#|>InternVL3 supports the Image Modality.\\nrelation<|#|>InternVL3<|#|>Video Modality<|#|>data support, capability<|#|>InternVL3 supports the Video Modality.\\nrelation<|#|>InternVL3<|#|>Open-Source Models<|#|>model type, classification<|#|>InternVL3 is a type of Open-Source Model.\\nrelation<|#|>MiMo<|#|>Xiaomi<|#|>development, affiliation<|#|>MiMo was developed by the organization Xiaomi.\\nrelation<|#|>MiMo<|#|>Dense Architecture<|#|>architecture, design<|#|>MiMo uses the Dense Architecture.\\nrelation<|#|>MiMo<|#|>GRPO<|#|>training algorithm, method<|#|>MiMo is trained with the GRPO algorithm.\\nrelation<|#|>MiMo<|#|>Text Modality<|#|>data support, capability<|#|>MiMo supports the Text Modality.\\nrelation<|#|>MiMo<|#|>Open-Source Models<|#|>model type, classification<|#|>MiMo is a type of Open-Source Model.\\nrelation<|#|>Qwen3<|#|>AlibabaQwen<|#|>development, affiliation<|#|>Qwen3 was developed by the organization AlibabaQwen.\\nrelation<|#|>Qwen3<|#|>Mixture of Experts<|#|>architecture, design<|#|>Qwen3 uses the Mixture of Experts architecture.\\nrelation<|#|>Qwen3<|#|>Dense Architecture<|#|>architecture, design<|#|>Qwen3 uses the Dense Architecture.\\nrelation<|#|>Qwen3<|#|>GRPO<|#|>training algorithm, method<|#|>Qwen3 is trained with the GRPO algorithm.\\nrelation<|#|>Qwen3<|#|>Text Modality<|#|>data support, capability<|#|>Qwen3 supports the Text Modality.\\nrelation<|#|>Qwen3<|#|>Open-Source Models<|#|>model type, classification<|#|>Qwen3 is a type of Open-Source Model.\\nrelation<|#|>Llama-Nemotron-Ultra<|#|>NVIDIA<|#|>development, affiliation<|#|>Llama-Nemotron-Ultra was developed by the organization NVIDIA.\\nrelation<|#|>Llama-Nemotron-Ultra<|#|>Dense Architecture<|#|>architecture, design<|#|>Llama-Nemotron-Ultra uses the Dense Architecture.\\nrelation<|#|>Llama-Nemotron-Ultra<|#|>GRPO<|#|>training algorithm, method<|#|>Llama-Nemotron-Ultra is trained with the GRPO algorithm.\\nrelation<|#|>Llama-Nemotron-Ultra<|#|>Text Modality<|#|>data support, capability<|#|>Llama-Nemotron-Ultra supports the Text Modality.\\nrelation<|#|>Llama-Nemotron-Ultra<|#|>Open-Source Models<|#|>model type, classification<|#|>Llama-Nemotron-Ultra is a type of Open-Source Model.\\nrelation<|#|>INTELLECT-2<|#|>IntellectAI<|#|>development, affiliation<|#|>INTELLECT-2 was developed by the organization IntellectAI.\\nrelation<|#|>INTELLECT-2<|#|>Dense Architecture<|#|>architecture, design<|#|>INTELLECT-2 uses the Dense Architecture.\\nrelation<|#|>INTELLECT-2<|#|>GRPO<|#|>training algorithm, method<|#|>INTELLECT-2 is trained with the GRPO algorithm.\\nrelation<|#|>INTELLECT-2<|#|>Text Modality<|#|>data support, capability<|#|>INTELLECT-2 supports the Text Modality.\\nrelation<|#|>INTELLECT-2<|#|>Open-Source Models<|#|>model type, classification<|#|>INTELLECT-2 is a type of Open-Source Model.\\nrelation<|#|>Hunyuan-TurboS<|#|>Tencent<|#|>development, affiliation<|#|>Hunyuan-TurboS was developed by the organization Tencent.\\nrelation<|#|>Hunyuan-TurboS<|#|>Hybrid Mixture of Experts<|#|>architecture, design<|#|>Hunyuan-TurboS uses the Hybrid Mixture of Experts architecture.\\nrelation<|#|>Hunyuan-TurboS<|#|>GRPO<|#|>training algorithm, method<|#|>Hunyuan-TurboS is trained with the GRPO algorithm.\\nrelation<|#|>Hunyuan-TurboS<|#|>Text Modality<|#|>data support, capability<|#|>Hunyuan-TurboS supports the Text Modality.\\nrelation<|#|>Hunyuan-TurboS<|#|>Open-Source Models<|#|>model type, classification<|#|>Hunyuan-TurboS is a type of Open-Source Model.\\nrelation<|#|>SkyworkOR-1<|#|>Skywork<|#|>development, affiliation<|#|>SkyworkOR-1 was developed by the organization Skywork.\\nrelation<|#|>SkyworkOR-1<|#|>Dense Architecture<|#|>architecture, design<|#|>SkyworkOR-1 uses the Dense Architecture.\\nrelation<|#|>SkyworkOR-1<|#|>GRPO<|#|>training algorithm, method<|#|>SkyworkOR-1 is trained with the GRPO algorithm.\\nrelation<|#|>SkyworkOR-1<|#|>Text Modality<|#|>data support, capability<|#|>SkyworkOR-1 supports the Text Modality.\\nrelation<|#|>SkyworkOR-1<|#|>Open-Source Models<|#|>model type, classification<|#|>SkyworkOR-1 is a type of Open-Source Model.\\nrelation<|#|>DeepSeek-R1-0528<|#|>DeepSeek<|#|>development, affiliation<|#|>DeepSeek-R1-0528 was developed by the organization DeepSeek.\\nrelation<|#|>DeepSeek-R1-0528<|#|>Mixture of Experts<|#|>architecture, design<|#|>DeepSeek-R1-0528 uses the Mixture of Experts architecture.\\nrelation<|#|>DeepSeek-R1-0528<|#|>Multi-Layer Attention<|#|>architecture, design<|#|>DeepSeek-R1-0528 uses the Multi-Layer Attention architecture.\\nrelation<|#|>DeepSeek-R1-0528<|#|>GRPO<|#|>training algorithm, method<|#|>DeepSeek-R1-0528 is trained with the GRPO algorithm.\\nrelation<|#|>DeepSeek-R1-0528<|#|>Text Modality<|#|>data support, capability<|#|>DeepSeek-R1-0528 supports the Text Modality.\\nrelation<|#|>DeepSeek-R1-0528<|#|>Open-Source Models<|#|>model type, classification<|#|>DeepSeek-R1-0528 is a type of Open-Source Model.\\nrelation<|#|>Magistral<|#|>MistralAI<|#|>development, affiliation<|#|>Magistral was developed by the organization MistralAI.\\nrelation<|#|>Magistral<|#|>Dense Architecture<|#|>architecture, design<|#|>Magistral uses the Dense Architecture.\\nrelation<|#|>Magistral<|#|>GRPO<|#|>training algorithm, method<|#|>Magistral is trained with the GRPO algorithm.\\nrelation<|#|>Magistral<|#|>Text Modality<|#|>data support, capability<|#|>Magistral supports the Text Modality.\\nrelation<|#|>Magistral<|#|>Open-Source Models<|#|>model type, classification<|#|>Magistral is a type of Open-Source Model.\\nrelation<|#|>Minimax-M1<|#|>Minimax<|#|>development, affiliation<|#|>Minimax-M1 was developed by the organization Minimax.\\nrelation<|#|>Minimax-M1<|#|>Hybrid Mixture of Experts<|#|>architecture, design<|#|>Minimax-M1 uses the Hybrid Mixture of Experts architecture.\\nrelation<|#|>Minimax-M1<|#|>Clipped IS-weight Policy Optimization<|#|>training algorithm, method<|#|>Minimax-M1 is trained with the Clipped IS-weight Policy Optimization algorithm.\\nrelation<|#|>Minimax-M1<|#|>Text Modality<|#|>data support, capability<|#|>Minimax-M1 supports the Text Modality.\\nrelation<|#|>Minimax-M1<|#|>Open-Source Models<|#|>model type, classification<|#|>Minimax-M1 is a type of Open-Source Model.\\nrelation<|#|>Intern-S1<|#|>ShanghaiAILab<|#|>development, affiliation<|#|>Intern-S1 was developed by the organization ShanghaiAILab.\\nrelation<|#|>Intern-S1<|#|>Mixture of Experts<|#|>architecture, design<|#|>Intern-S1 uses the Mixture of Experts architecture.\\nrelation<|#|>Intern-S1<|#|>GRPO<|#|>training algorithm, method<|#|>Intern-S1 is trained with the GRPO algorithm.\\nrelation<|#|>Intern-S1<|#|>Text Modality<|#|>data support, capability<|#|>Intern-S1 supports the Text Modality.\\nrelation<|#|>Intern-S1<|#|>Image Modality<|#|>data support, capability<|#|>Intern-S1 supports the Image Modality.\\nrelation<|#|>Intern-S1<|#|>Video Modality<|#|>data support, capability<|#|>Intern-S1 supports the Video Modality.\\nrelation<|#|>Intern-S1<|#|>Open-Source Models<|#|>model type, classification<|#|>Intern-S1 is a type of Open-Source Model.\\nrelation<|#|>KimiK2<|#|>Kimi<|#|>development, affiliation<|#|>KimiK2 was developed by the organization Kimi.\\nrelation<|#|>KimiK2<|#|>Mixture of Experts<|#|>architecture, design<|#|>KimiK2 uses the Mixture of Experts architecture.\\nrelation<|#|>KimiK2<|#|>Online Policy Mirror Descent<|#|>training algorithm, method<|#|>KimiK2 is trained with the Online Policy Mirror Descent algorithm.\\nrelation<|#|>KimiK2<|#|>Text Modality<|#|>data support, capability<|#|>KimiK2 supports the Text Modality.\\nrelation<|#|>KimiK2<|#|>Open-Source Models<|#|>model type, classification<|#|>KimiK2 is a type of Open-Source Model.\\nrelation<|#|>Step3<|#|>StepAI<|#|>development, affiliation<|#|>Step3 was developed by the organization StepAI.\\nrelation<|#|>Step3<|#|>Mixture of Experts<|#|>architecture, design<|#|>Step3 uses the Mixture of Experts architecture.\\nrelation<|#|>Step3<|#|>Text Modality<|#|>data support, capability<|#|>Step3 supports the Text Modality.\\nrelation<|#|>Step3<|#|>Image Modality<|#|>data support, capability<|#|>Step3 supports the Image Modality.\\nrelation<|#|>Step3<|#|>Video Modality<|#|>data support, capability<|#|>Step3 supports the Video Modality.\\nrelation<|#|>Step3<|#|>Open-Source Models<|#|>model type, classification<|#|>Step3 is a type of Open-Source Model.\\nrelation<|#|>Qwen3-2507<|#|>AlibabaQwen<|#|>development, affiliation<|#|>Qwen3-2507 was developed by the organization AlibabaQwen.\\nrelation<|#|>Qwen3-2507<|#|>Mixture of Experts<|#|>architecture, design<|#|>Qwen3-2507 uses the Mixture of Experts architecture.\\nrelation<|#|>Qwen3-2507<|#|>Dense Architecture<|#|>architecture, design<|#|>Qwen3-2507 uses the Dense Architecture.\\nrelation<|#|>Qwen3-2507<|#|>GSPO<|#|>training algorithm, method<|#|>Qwen3-2507 is trained with the GSPO algorithm.\\nrelation<|#|>Qwen3-2507<|#|>Text Modality<|#|>data support, capability<|#|>Qwen3-2507 supports the Text Modality.\\nrelation<|#|>Qwen3-2507<|#|>Open-Source Models<|#|>model type, classification<|#|>Qwen3-2507 is a type of Open-Source Model.\\nrelation<|#|>GLM-4.1V-Thinking<|#|>ZhipuAI<|#|>development, affiliation<|#|>GLM-4.1V-Thinking was developed by the organization ZhipuAI.\\nrelation<|#|>GLM-4.1V-Thinking<|#|>Dense Architecture<|#|>architecture, design<|#|>GLM-4.1V-Thinking uses the Dense Architecture.\\nrelation<|#|>GLM-4.1V-Thinking<|#|>GRPO<|#|>training algorithm, method<|#|>GLM-4.1V-Thinking is trained with the GRPO algorithm.\\nrelation<|#|>GLM-4.1V-Thinking<|#|>Text Modality<|#|>data support, capability<|#|>GLM-4.1V-Thinking supports the Text Modality.\\nrelation<|#|>GLM-4.1V-Thinking<|#|>Image Modality<|#|>data support, capability<|#|>GLM-4.1V-Thinking supports the Image Modality.\\nrelation<|#|>GLM-4.1V-Thinking<|#|>Video Modality<|#|>data support, capability<|#|>GLM-4.1V-Thinking supports the Video Modality.\\nrelation<|#|>GLM-4.1V-Thinking<|#|>Open-Source Models<|#|>model type, classification<|#|>GLM-4.1V-Thinking is a type of Open-Source Model.\\nrelation<|#|>GLM-4.5<|#|>ZhipuAI<|#|>development, affiliation<|#|>GLM-4.5 was developed by the organization ZhipuAI.\\nrelation<|#|>GLM-4.5<|#|>Mixture of Experts<|#|>architecture, design<|#|>GLM-4.5 uses the Mixture of Experts architecture.\\nrelation<|#|>GLM-4.5<|#|>GRPO<|#|>training algorithm, method<|#|>GLM-4.5 is trained with the GRPO algorithm.\\nrelation<|#|>GLM-4.5<|#|>Text Modality<|#|>data support, capability<|#|>GLM-4.5 supports the Text Modality.\\nrelation<|#|>GLM-4.5<|#|>Open-Source Models<|#|>model type, classification<|#|>GLM-4.5 is a type of Open-Source Model.\\nrelation<|#|>Skywork-R1V3<|#|>Skywork<|#|>development, affiliation<|#|>Skywork-R1V3 was developed by the organization Skywork.\\nrelation<|#|>Skywork-R1V3<|#|>Dense Architecture<|#|>architecture, design<|#|>Skywork-R1V3 uses the Dense Architecture.\\nrelation<|#|>Skywork-R1V3<|#|>GRPO<|#|>training algorithm, method<|#|>Skywork-R1V3 is trained with the GRPO algorithm.\\nrelation<|#|>Skywork-R1V3<|#|>Text Modality<|#|>data support, capability<|#|>Skywork-R1V3 supports the Text Modality.\\nrelation<|#|>Skywork-R1V3<|#|>Image Modality<|#|>data support, capability<|#|>Skywork-R1V3 supports the Image Modality.\\nrelation<|#|>Skywork-R1V3<|#|>Open-Source Models<|#|>model type, classification<|#|>Skywork-R1V3 is a type of Open-Source Model.\\nrelation<|#|>gpt-oss<|#|>OpenAI<|#|>development, affiliation<|#|>gpt-oss was developed by the organization OpenAI.\\nrelation<|#|>gpt-oss<|#|>Mixture of Experts<|#|>architecture, design<|#|>gpt-oss uses the Mixture of Experts architecture.\\nrelation<|#|>gpt-oss<|#|>Text Modality<|#|>data support, capability<|#|>gpt-oss supports the Text Modality.\\nrelation<|#|>gpt-oss<|#|>Open-Source Models<|#|>model type, classification<|#|>gpt-oss is a type of Open-Source Model.\\nrelation<|#|>Seed-OSS<|#|>BytedanceSeed<|#|>development, affiliation<|#|>Seed-OSS was developed by the organization BytedanceSeed.\\nrelation<|#|>Seed-OSS<|#|>Dense Architecture<|#|>architecture, design<|#|>Seed-OSS uses the Dense Architecture.\\nrelation<|#|>Seed-OSS<|#|>Text Modality<|#|>data support, capability<|#|>Seed-OSS supports the Text Modality.\\nrelation<|#|>Seed-OSS<|#|>Open-Source Models<|#|>model type, classification<|#|>Seed-OSS is a type of Open-Source Model.\\nrelation<|#|>GLM-4.5V<|#|>ZhipuAI<|#|>development, affiliation<|#|>GLM-4.5V was developed by the organization ZhipuAI.\\nrelation<|#|>GLM-4.5V<|#|>Mixture of Experts<|#|>architecture, design<|#|>GLM-4.5V uses the Mixture of Experts architecture.\\nrelation<|#|>GLM-4.5V<|#|>GRPO<|#|>training algorithm, method<|#|>GLM-4.5V is trained with the GRPO algorithm.\\nrelation<|#|>GLM-4.5V<|#|>Text Modality<|#|>data support, capability<|#|>GLM-4.5V supports the Text Modality.\\nrelation<|#|>GLM-4.5V<|#|>Image Modality<|#|>data support, capability<|#|>GLM-4.5V supports the Image Modality.\\nrelation<|#|>GLM-4.5V<|#|>Video Modality<|#|>data support, capability<|#|>GLM-4.5V supports the Video Modality.\\nrelation<|#|>GLM-4.5V<|#|>Open-Source Models<|#|>model type, classification<|#|>GLM-4.5V is a type of Open-Source Model.\\nrelation<|#|>InternVL3.5<|#|>ShanghaiAILab<|#|>development, affiliation<|#|>InternVL3.5 was developed by the organization ShanghaiAILab.\\nrelation<|#|>InternVL3.5<|#|>Mixture of Experts<|#|>architecture, design<|#|>InternVL3.5 uses the Mixture of Experts architecture.\\nrelation<|#|>InternVL3.5<|#|>Dense Architecture<|#|>architecture, design<|#|>InternVL3.5 uses the Dense Architecture.\\nrelation<|#|>InternVL3.5<|#|>Mixed Preference Optimization<|#|>training algorithm, method<|#|>InternVL3.5 is trained with the Mixed Preference Optimization algorithm.\\nrelation<|#|>InternVL3.5<|#|>GSPO<|#|>training algorithm, method<|#|>InternVL3.5 is trained with the GSPO algorithm.\\nrelation<|#|>InternVL3.5<|#|>Text Modality<|#|>data support, capability<|#|>InternVL3.5 supports the Text Modality.\\nrelation<|#|>InternVL3.5<|#|>Image Modality<|#|>data support, capability<|#|>InternVL3.5 supports the Image Modality.\\nrelation<|#|>InternVL3.5<|#|>Video Modality<|#|>data support, capability<|#|>InternVL3.5 supports the Video Modality.\\nrelation<|#|>InternVL3.5<|#|>Open-Source Models<|#|>model type, classification<|#|>InternVL3.5 is a type of Open-Source Model.\\nrelation<|#|>ERNIE-4.5-Thinking<|#|>Baidu<|#|>development, affiliation<|#|>ERNIE-4.5-Thinking was developed by the organization Baidu.\\nrelation<|#|>ERNIE-4.5-Thinking<|#|>Mixture of Experts<|#|>architecture, design<|#|>ERNIE-4.5-Thinking uses the Mixture of Experts architecture.\\nrelation<|#|>ERNIE-4.5-Thinking<|#|>Text Modality<|#|>data support, capability<|#|>ERNIE-4.5-Thinking supports the Text Modality.\\nrelation<|#|>ERNIE-4.5-Thinking<|#|>Open-Source Models<|#|>model type, classification<|#|>ERNIE-4.5-Thinking is a type of Open-Source Model.\\n<|COMPLETE|>\"}]",
    "queryparam": null,
    "create_time": 1765219257,
    "update_time": 1765219257,
    "_id": "default:extract:af0b6e687e220b0cf3245a40a244f515"
  }
}