{
  "doc-32bcdbba2d266d061d85f94faf74b18a": {
    "entity_names": [
      "Foundational Problems",
      "Sihang Zeng",
      "Training Resources",
      "Sharpening Discovery",
      "Critic-Free Algorithms",
      "Compute",
      "Yihao Liu",
      "RL's Role",
      "Mathematics",
      "Algorithm Design",
      "Policy Gradient Algorithms",
      "Ning Ding",
      "Computational Resources",
      "Bingxiang He",
      "Tsinghua University",
      "Haozhan Li",
      "Off-Policy Regularization",
      "Xinwei Long",
      "Yu Fu",
      "Process Outcome",
      "Peking University",
      "Training Data",
      "Junqi Gao",
      "OpenRLHF",
      "Weize Chen",
      "Huazhong University of Science and Technology",
      "Weak Strong",
      "Model Ensemble",
      "Tricks Traps",
      "Static Corpus Training",
      "Mixture",
      "Infrastructure",
      "Section 6",
      "Multimodal Robotics Tasks",
      "Ganqu Cui",
      "Action",
      "Kaiyan Zhang",
      "veRL",
      "University of Science and Technology of China",
      "Runze Liu",
      "Tsinghua C3I",
      "Medical Systems Tasks",
      "Kai Tian",
      "Agent",
      "University College London",
      "Dong Li",
      "Sampling Hyper-Parameters",
      "Reward Type",
      "Xiaoye Qu",
      "Section 4",
      "Yuru Wang",
      "DeepSeek-R1",
      "Supervised Fine-Tuning (SFT)",
      "Zhiyuan Ma",
      "Unsupervised Rewards",
      "Math Code",
      "Bowen Zhou",
      "Xingtai Lv",
      "Policy Optimization",
      "Game",
      "Multi-Agent Sampling",
      "Verifiable Generative Rewards",
      "Large Language Models (LLMs)",
      "Awesome-RL-for-LRMs",
      "Section 3",
      "Reward",
      "Huayu Chen",
      "Evolution (Training Step)",
      "Che Jiang",
      "Large Reasoning Models (LRMs)",
      "Figure 1",
      "Harbin Institute of Technology",
      "Yuchen Fan",
      "Fangfu Liu",
      "Dynamic Environment",
      "Ermo Hua",
      "slime",
      "Optimization Objectives",
      "Coding",
      "RL vs. SFT",
      "University of Washington",
      "Agentic Tasks",
      "Reinforcement Learning (RL)",
      "TRL",
      "Xiang Xu",
      "Model Prior",
      "Xuekai Zhu",
      "Coding Tasks",
      "Section 5",
      "Zhenzhao Yuan",
      "Zonglin Li",
      "Guoli Jia",
      "Abstract",
      "Yuxin Zuo",
      "Reward Design",
      "Environment",
      "A Survey of Reinforcement Learning for Large Reasoning Models",
      "Training Recipes",
      "Yuchen Zhang",
      "RL Infrastructure & Frameworks",
      "Pengfei Li",
      "Youbang Sun",
      "Sampling Strategy",
      "Yafu Li",
      "AReaL",
      "Interaction (Episode)",
      "Shanghai AI Laboratory",
      "Shang Qu",
      "Shaping Rewards",
      "Generalize Memorize",
      "Jiaze Ma",
      "Dynamic Sampling",
      "Applications",
      "Zhiyuan Liu",
      "Shanghai Jiao Tong University",
      "Artificial SuperIntelligence (ASI)",
      "Shijie Wang",
      "Foundational Components",
      "Policy Critic-Based Algorithms",
      "Biqing Qi",
      "STEM Agent"
    ],
    "count": 121,
    "create_time": 1765218608,
    "update_time": 1765218608,
    "_id": "doc-32bcdbba2d266d061d85f94faf74b18a"
  },
  "doc-b836ebc3527208cd954c81601d5fc314": {
    "entity_names": [
      "Generative Rewards",
      "Foundational Problems",
      "Policy Optimization",
      "Page 47",
      "Background",
      "Reward Design",
      "Off-Policy Optimization",
      "Training Resources",
      "RL Vs. SFT: Generalize Or Memorize",
      "Regularization Objectives",
      "Critic-Free Algorithms",
      "Rewards Shaping",
      "Related Surveys",
      "Sampling Strategy",
      "Model Prior: Weak And Strong",
      "Dynamic Environment",
      "Sampling Hyper-Parameters",
      "Policy Gradient Objective",
      "Page 52",
      "Preliminaries",
      "Applications",
      "Page 43",
      "Dense Rewards",
      "Agentic Tasks",
      "Page 49",
      "Reward Type: Process Or Outcome",
      "Page 46",
      "Verifiable Rewards",
      "Introduction",
      "Foundational Components",
      "RL Infrastructure",
      "Frontier Models",
      "Dynamic And Structured Sampling",
      "Multimodal Tasks",
      "Unsupervised Rewards",
      "RL's Role: Sharpening Or Discovery",
      "Coding Tasks",
      "Static Corpus",
      "Critic-Based Algorithms",
      "Training Recipes: Tricks Or Traps"
    ],
    "count": 40,
    "create_time": 1765218641,
    "update_time": 1765218641,
    "_id": "doc-b836ebc3527208cd954c81601d5fc314"
  },
  "doc-0f35d639cf51f0ab7d4017f06d35581f": {
    "entity_names": [
      "Robotics Tasks",
      "ASurveyofReinforcementLearningforLargeReasoningModels",
      "Continual RL for LLMs",
      "Large Language Models",
      "RL for LLMs Pre-training",
      "RL for LLMs in Scientific Discovery",
      "Future Directions",
      "Large Reasoning Models",
      "Author Contributions",
      "Multi-Agent Systems",
      "Reinforcement Learning",
      "Teaching LRMs Efficient Reasoning",
      "Medical Tasks",
      "Model-based RL for LLMs",
      "Memory-based RL for LLMs",
      "RL for Architecture-Algorithm Co-Design",
      "Conclusion",
      "RL for Diffusion-based LLMs",
      "Teaching LLMs Latent Space Reasoning"
    ],
    "count": 19,
    "create_time": 1765218686,
    "update_time": 1765218686,
    "_id": "doc-0f35d639cf51f0ab7d4017f06d35581f"
  },
  "doc-8b7f62b7202fa8bc4469bb44567df186": {
    "entity_names": [
      "Christiano et al., 2017",
      "OpenAI, 2025a,b",
      "Shogi",
      "Brown et al., 2024",
      "Silver et al., 2021",
      "Zhao et al., 2023a",
      "Foundational Components of RL for LRMs",
      "Mathematics",
      "Compute Budget",
      "Data Limitations",
      "Bai et al., 2022b",
      "Algorithm Design",
      "Computational Resources",
      "Group Relative Policy Optimization (GRPO)",
      "Self-Correction",
      "Unit-Test Pass Rates for Code",
      "Snell et al., 2024",
      "Liu et al., 2025m",
      "Training Data",
      "Preliminary Definitions of RL Modeling",
      "AlphaGo",
      "Human Alignment",
      "Competitive Programming",
      "Infrastructure",
      "Reward Maximization Objective",
      "Shumailov et al., 2024",
      "OpenAI o1",
      "Bai et al., 2025",
      "Reward Feedback",
      "Artificial Agents",
      "Ouyang et al., 2022",
      "Silver et al., 2016",
      "Chess",
      "Silver et al., 2018",
      "Go",
      "Automatically Checkable Rewards",
      "Artificial Superintelligence (ASI)",
      "Aghajanyan et al., 2023",
      "DeepSeek-R1",
      "Pre-Training",
      "Sutton et al., 1998",
      "Rafailov et al., 2023",
      "Planning",
      "AlphaZero",
      "Chain-of-Thought",
      "Train-Time Compute",
      "Test-Time Compute",
      "Kaplan et al., 2020",
      "Large Language Models (LLMs)",
      "Reinforcement Learning from Human Feedback (RLHF)",
      "Perolat et al., 2022",
      "Competition Mathematics",
      "Silver et al., 2017",
      "Xu et al., 2025a",
      "Direct Preference Optimization (DPO)",
      "RL for Large Reasoning Models (LRMs)",
      "Stratego",
      "Answer Correctness for Mathematics",
      "El-Kishky et al., 2025",
      "Reinforcement Learning (RL)",
      "Reflection",
      "Reasoning",
      "Coding Tasks",
      "Reward Signals",
      "ASurveyofReinforcementLearningforLargeReasoningModels",
      "Wei et al., 2022",
      "Jaech et al., 2024",
      "Schrittwieser et al., 2020",
      "Self-Generated Training Data",
      "Frontier Reasoning Models",
      "Long-Form Reasoning",
      "Guo et al., 2025a",
      "Helpfulness, Honesty, and Harmlessness (3H)",
      "Villalobos et al., 2022",
      "Verifiable Rewards",
      "Reinforcement Learning with Verifiable Rewards (RLVR)",
      "Scaling Axis",
      "Selected Scientific Domains",
      "Base Models"
    ],
    "count": 79,
    "create_time": 1765218773,
    "update_time": 1765218773,
    "_id": "doc-8b7f62b7202fa8bc4469bb44567df186"
  },
  "doc-8355cb8033a4afe3e01992fcee557002": {
    "entity_names": [
      "Medical Applications",
      "Controversial Problems",
      "Robotics Tasks",
      "ASurveyofReinforcementLearningforLargeReasoningModels",
      "Complex Task Solving",
      "Foundational Problems",
      "Static Corpora",
      "Novel Algorithms",
      "Policy Optimization",
      "Research Directions",
      "Technical Approaches",
      "Environment",
      "Large Language Models (LLMs)",
      "Reward Definitions",
      "Markov Decision Process",
      "Training Resources",
      "Training Recipes",
      "RLHF",
      "GPT-3.5",
      "Role of RL",
      "Multi-Agent Systems",
      "Model Priors",
      "Large Reasoning Models (LRMs)",
      "Reward-Free RL",
      "GPT-4",
      "Features",
      "Scaling RL",
      "Open-Ended RL",
      "Agent",
      "Research Avenues",
      "Rule-Based RL",
      "Actions",
      "Training Infrastructure",
      "O1",
      "Reward-Based RL",
      "Sequential Decision Making",
      "Dynamic Environments",
      "Cumulative Reward",
      "Qwen 2.5",
      "DPO",
      "RL From Human Direct Preference",
      "RL With Verifiable Feedback Optimization Reward",
      "Agentic Tasks",
      "DeepSeek-R1",
      "Reinforcement Learning (RL)",
      "Supervised Fine-Tuning (SFT)",
      "RLVR",
      "Multimodal Tasks",
      "Human Alignment",
      "Sampling Strategies",
      "Coding Tasks",
      "Llama 3",
      "Mechanisms"
    ],
    "count": 53,
    "create_time": 1765218861,
    "update_time": 1765218861,
    "_id": "doc-8355cb8033a4afe3e01992fcee557002"
  },
  "doc-42002d06ba2f88fbde0f4fdbf912a18f": {
    "entity_names": [
      "Segment (y(k))",
      "ASurveyofReinforcementLearningforLargeReasoningModels",
      "Large Reasoning Models",
      "Token (a ∈ V)",
      "Return (G)",
      "Environment",
      "Large Language Models (LLMs)",
      "Dataset D",
      "Policy (πθ)",
      "Reward Function (R)",
      "Trajectory",
      "Action",
      "Reward",
      "Prompt/Task (x)",
      "Markov Decision Process (MDP)",
      "Agent",
      "Data Distribution (D)",
      "State",
      "Sequence (y)",
      "Completion Tokens",
      "State Space (S)",
      "Terminal State",
      "Learning Objective",
      "Discount Factor (γ)",
      "Response",
      "Table 2",
      "Language Models (LMs)",
      "Reinforcement Learning (RL)",
      "EOS Token",
      "Sutton et al., 1998",
      "Action Space (A)",
      "Transition Dynamics (P)"
    ],
    "count": 32,
    "create_time": 1765218908,
    "update_time": 1765218908,
    "_id": "doc-42002d06ba2f88fbde0f4fdbf912a18f"
  },
  "doc-37f12102e8df23d0bf48e221b1e75d9e": {
    "entity_names": [
      "Kimi 1.5",
      "Skywork R1V2",
      "Large Reasoning Models",
      "MPO",
      "Comanici et al., 2025",
      "Qwen3 Series",
      "Qwen Family",
      "Team, 2025a",
      "QVQ",
      "Generalization Across Domains",
      "Tool-Use",
      "Rastogi et al., 2025",
      "Multi-Stage Training Pipeline",
      "Anthropic, 2025a",
      "Qwen Team",
      "R1-Distilled Models",
      "QwQ-32B",
      "Gemini 2.0",
      "gpt-oss-120b",
      "InternVL Series",
      "Images Domain",
      "Bercovich et al., 2025",
      "Long Context Scaling",
      "Kimi K2",
      "Zero RL",
      "Hybrid RL",
      "Qwen3-235B",
      "KL-Divergence Constraints",
      "Wang et al., 2025k",
      "Jaechetal.,2024",
      "Zhu et al., 2025c",
      "Gemini Family",
      "Science Benchmarks",
      "Minimax-M1",
      "Seed et al., 2025b",
      "Zeng et al., 2025a",
      "Reinforcement Learning",
      "Seed-Thinking 1.5",
      "Benchmark Scores",
      "Anthropic",
      "Hybrid Attention",
      "DeepSeek",
      "Qwen Team, 2025",
      "AI System",
      "OpenAI, 2025a",
      "Team, 2025g",
      "Skywork-OR1",
      "Claude-4.1-Opus",
      "Long-Context Reasoning Abilities",
      "Frontier Models",
      "Llama-Nemotron-Ultra",
      "Magistral24B",
      "Distillation from Prior Models",
      "SWE-bench",
      "Reference Policy",
      "Widespread Adoption of Reasoning Models",
      "Scaling Train-Time RL",
      "DeepSeek-V3.1",
      "Chen et al., 2025a",
      "Training",
      "Claude Series",
      "Jimenez et al., 2023",
      "Coding Benchmarks",
      "Text Domain",
      "OpenAI, 2025b",
      "Anthropic, 2025b",
      "o1 Series",
      "Multimodal Reasoning",
      "Joint Reasoning Over Text and Vision Domains",
      "Multimodal LRMs",
      "Language Quality",
      "GLM4.5",
      "Data Mixtures",
      "Agentic Tasks",
      "Mathematics Benchmarks",
      "Scalable RL Training",
      "OpenAI",
      "GPT-5 Thinking",
      "Hybrid Reasoning",
      "General RL Procedure",
      "Team, 2025d",
      "Scaling Test-Time Compute",
      "Multimodality",
      "Efficient Model",
      "He et al., 2025d",
      "Agarwal et al., 2025a",
      "o3 Series",
      "Claude-3.7-Sonnet",
      "Video Domain",
      "Advanced Reasoning Abilities",
      "Large-Scale Agentic Training Data Synthesis",
      "Accuracy",
      "A Survey of Reinforcement Learning for Large Reasoning Models",
      "Analytical Thinking",
      "Algorithmic Innovations",
      "Non-Verifiable Rewards",
      "Yang et al., 2025a",
      "LRMs",
      "R1",
      "Longer Context Lengths",
      "GRPO",
      "Guo et al., 2025a",
      "Learned Policy",
      "GPT5",
      "Efficiency",
      "Agentic LRMs",
      "Visual Reasoning",
      "Seed-OSS",
      "InternVL3",
      "Reinforcement Learning from Scratch",
      "Gemini 2.5",
      "General Abilities",
      "Reasoning Abilities",
      "Audio Domain"
    ],
    "count": 114,
    "create_time": 1765219044,
    "update_time": 1765219044,
    "_id": "doc-37f12102e8df23d0bf48e221b1e75d9e"
  },
  "doc-aa95a088a3c7d1c923aa324fd7c02a63": {
    "entity_names": [
      "QVQ-Max",
      "InternVL3 1-78B",
      "Visual Multimodal Benchmarks",
      "Large Reasoning Models",
      "Skywork-R1V3 38B",
      "GLM-4.5V 106B",
      "Hunyuan-TurboS Step 3321B",
      "Sun et al. [2025b]",
      "Li et al. [2025w]",
      "Adaptive Behaviors",
      "o3 Magistral 24B",
      "GLM-4.1V-Thinking9B",
      "Table 1",
      "DeepSeek-V3.1 671B",
      "o1-mini",
      "Gemini 2.5 Flash",
      "Chen et al. [2025m]",
      "Kimi1.5",
      "Classical RL",
      "o3-mini",
      "Intern-S 1241B",
      "RLVR",
      "gpt-oss 21/117B",
      "Multimodal Scientific Reasoning",
      "QWQ 32B",
      "Feng et al. [2025c]",
      "Figure 4",
      "Two-Stage Cascade RL Framework",
      "Sui et al. [2025]",
      "Wang et al. [2025a]",
      "InternVL3.5",
      "Agentic Models",
      "DeepSeek-R1 671B",
      "MiMo 7B",
      "Gemini 2.0 Flash",
      "Step3",
      "Reinforcement Learning",
      "Seed-Thinking 1.5",
      "ORZ 0.5-32B",
      "Qwen3-2507-Thinking4-235B",
      "Qwen3 0.6-235B",
      "Kimik 21T",
      "Reasoning LLMs",
      "System 2 Reasoning",
      "DeepSeek-R1",
      "Skywork-R1V2 38B",
      "GPT-5",
      "Multimodal o3-pro",
      "Bai et al. [2025]",
      "Gemini 2.5 Flash-Lite",
      "LLM Architectures",
      "o4-mini",
      "Xia et al. [2024]",
      "Zhang et al. [2024b]",
      "Llama-Nemotron-Ultra 253B",
      "Reasoning via Foundation Models",
      "Self-Play Techniques",
      "Claude Opus 4.1",
      "Skywork OR-1 7/32B",
      "Minimax-M1 456B",
      "INTELLECT-2 32B",
      "GLM-4.5V",
      "o1-preview",
      "Mixture-of-Rewards Design",
      "Qwen 3-Next-80B-A3B-Thinking 80B",
      "DeepSeek-R1-0528 671B",
      "InternVL3.5 1-241B",
      "System 1 Reasoning",
      "Efficient Training",
      "LLMs",
      "Minimizing Decoding Costs",
      "Intern-S1",
      "Seed-Thinking 1.6",
      "O3-Pro",
      "Multimodal Models",
      "Zhang et al. [2025a]",
      "RL in Computer Vision Tasks",
      "Ghasemi et al. [2024]",
      "Phi-4 Reasoning 14B",
      "ASurveyofReinforcementLearningforLargeReasoningModels",
      "Language Models",
      "Wu et al. [2025h]",
      "Huh and Mohapatra [2023]",
      "RLHF",
      "Unified Native Multimodal Pretraining Phase",
      "Foundation Models",
      "Claude 3.7 Sonnet",
      "Online RL",
      "Ring-mini-2.0 16B",
      "Zhao et al. [2023a]",
      "Grok4",
      "Gemini 2.5 Pro",
      "Team et al. [2025a]",
      "Claude 4",
      "21/117B",
      "Wang et al. [2025o]",
      "Multi-Agent RL",
      "o1-2024-12-17",
      "Long Chain-of-Thought Reasoning",
      "GLM-4.5355B"
    ],
    "count": 100,
    "create_time": 1765219167,
    "update_time": 1765219167,
    "_id": "doc-aa95a088a3c7d1c923aa324fd7c02a63"
  },
  "doc-9f6704aa27cd8e9483d9f72f2cf4c3fe": {
    "entity_names": [
      "gpt-oss",
      "Mixed Preference Optimization",
      "Tencent",
      "Qwen3",
      "Microsoft",
      "Multi-Layer Attention",
      "Minimax",
      "Large Reasoning Models",
      "InternVL3.5",
      "Hunyuan-TurboS",
      "A Survey of Reinforcement Learning for Large Reasoning Models",
      "ShanghaiAILab",
      "BytedanceSeed",
      "QwQ",
      "Phi-4Reasoning",
      "NVIDIA",
      "Minimax-M1",
      "Xiaomi",
      "ZhipuAI",
      "SkyworkOR-1",
      "StepAI",
      "Skywork-R1V2",
      "Skywork-R1V3",
      "GLM-4.5V",
      "Skywork",
      "Mixture of Experts",
      "Step3",
      "MiMo",
      "Reinforcement Learning",
      "ERNIE-4.5-Thinking",
      "AlibabaQwen",
      "Text Modality",
      "DeepSeek-R1-0528",
      "Clipped IS-weight Policy Optimization",
      "Magistral",
      "GRPO",
      "Video Modality",
      "GSPO",
      "ORZ",
      "Open-Source Models",
      "GLM-4.1V-Thinking",
      "Table 1",
      "DeepSeek",
      "KimiK2",
      "Seed-OSS",
      "PPO",
      "InternVL3",
      "Image Modality",
      "Intern-S1",
      "DeepSeek-R1",
      "OpenAI",
      "INTELLECT-2",
      "Hybrid Mixture of Experts",
      "IntellectAI",
      "Dense Architecture",
      "MistralAI",
      "Qwen3-2507",
      "Llama-Nemotron-Ultra",
      "Online Policy Mirror Descent",
      "GLM-4.5",
      "Baidu",
      "Kimi"
    ],
    "count": 62,
    "create_time": 1765219262,
    "update_time": 1765219262,
    "_id": "doc-9f6704aa27cd8e9483d9f72f2cf4c3fe"
  }
}