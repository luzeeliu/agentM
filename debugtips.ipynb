{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd0c36c3",
   "metadata": {},
   "source": [
    "We introduce two memory architectures for AI agents. (1) Mem0 implements a novel paradigm that extracts,\n",
    "evaluates, and manages salient information from conversations through dedicated modules for memory\n",
    "extraction and updation. The system processes a pair of messages between either two user participants or a\n",
    "user and an assistant. (2) Mem0g extends this foundation by incorporating graph-based memory representa\n",
    "tions, where memories are stored as directed labeled graphs with entities as nodes and relationships as edges.\n",
    "This structure enables a deeper understanding of the connections between entities. By explicitly modeling\n",
    "both entities and their relationships, Mem0g supports more advanced reasoning across interconnected facts,\n",
    "especially for queries that require navigating complex relational paths across multiple memories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2187fb78",
   "metadata": {},
   "source": [
    "the mem0 pipeline is input \n",
    "- -> summary generator will generate post memory summary(async not each time trigger) \n",
    "- -> extract part will combine the summary + short chat history(nearest chat history) + current message to extract  different metadata. in mem0g it will extract entities and relations but in mem0 base it will extract content timestamp user_id embedding type, source confidence status \n",
    "- -> the extraction will go update part and here a llm will decide add update and delete to manage memory database\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525cae6d",
   "metadata": {},
   "source": [
    "when config mem0 we need :\n",
    "- add .mem0 and set it in env by using MEM0_DIR because mem0 will automatically save memory in system path\n",
    "- after set config.json we need use from_config to read it \n",
    "- the path in config is from root not current config position\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a222b8c",
   "metadata": {},
   "source": [
    "✦ The FAISS index dimension mismatch (1536 vs 1024) might stem from mem0 defaulting to a 1536-dim OpenAI model if the config isn't properly loaded.\n",
    "-  I've noticed a potential bug in Mem0Memory._load_config where it might incorrectly handle self.config_path. I'll now update\n",
    "-  reproduce_mem0_error.py to print the actual config object to verify if the settings from mem0_config.json are being correctly passed to\n",
    "-  Memory.from_config.\n",
    "\n",
    "delete FAISS and build from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5e187b",
   "metadata": {},
   "source": [
    "> 那全局graph state 现在好像没有用上，他可能用在哪里\n",
    "\n",
    "✦ GraphState（现在被拆分为 PlannerState 和 ToolState）仍然起着至关重要的作用，尽管我们把“事实存储”的重担移交给了 Mem0。\n",
    "\n",
    "  Global State (`PlannerState`) 的核心作用现在是：\n",
    "\n",
    "   1. 控制流与状态机（LangGraph 的基础）：\n",
    "       * 路由（Routing）：check_tool_call 函数必须读取 State 中的 last_message 才能决定是跳转到 tool_agent 还是结束（END）。没有\n",
    "         State，图就动不起来。\n",
    "       * 对话历史（Short-term Memory）：message 列表依然保存在 State 中。这对于维持当前多轮对话的连贯性是必须的。比如用户说“再多找找”，Planner\n",
    "         必须看之前的 message 才知道“找什么”。\n",
    "\n",
    "   2. 上下文传递（Context Passing）：\n",
    "       * User ID：state['user_id'] 是贯穿整个流程的唯一标识，Mem0 初始化全靠它。\n",
    "       * Query：state['query'] 原始问题需要从 Planner 传递给 Tool Agent，再用于 Mem0 检索。\n",
    "\n",
    "   3. 临时事实缓存（Facts Buffer）：\n",
    "       * 虽然我们现在用 Mem0 做持久化，但 state['facts'] 并没有完全作废。\n",
    "       * 即时性：Mem0 的写入和索引可能（理论上）有微小延迟，或者出于 Debug 目的，在本次 Session 中保留一份 facts 副本在内存里是有好处的。\n",
    "       * Tool Agent 间同步：在并行执行时，agent_runner 负责把所有 Tool Agent 产生的 Facts 汇聚到 State 里。如果未来需要做一个“Fact Check”节点（不走\n",
    "         LLM，纯代码逻辑），直接读 State 比去查 DB 要快得多。\n",
    "\n",
    "  它可能被用于哪里（未来的扩展性）：\n",
    "\n",
    "   * Max Iterations（防死循环）：state['max_iterations'] 可以用来限制 Planner 和 Tool Agent 之间踢皮球的次数。\n",
    "   * Artifacts（复杂产物）：如果 Tool Agent 生成了一个 PDF 报告或一张图表，这个文件的路径或对象可以存在 State 里传递给前端，而不适合塞进 Mem0\n",
    "     的文本字段。\n",
    "   * Human-in-the-loop：如果引入人工确认环节，State 需要挂起并等待人工输入修改 state 后再继续。\n",
    "\n",
    "  总结：\n",
    "  Mem0 接管了“长期知识（Long-term Knowledge）”的存储。\n",
    "  State 依然负责“短期会话（Session Context）”和“程序控制流（Control Flow）”。\n",
    "  二者各司其职。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad584680",
   "metadata": {},
   "source": [
    "✦ 现在机制已经完善：\n",
    "\n",
    "   1. Tool Agent 执行任务，生成一份最终的文本报告（Report）。\n",
    "   2. Runner 捕获这个报告。\n",
    "   3. 双重更新：\n",
    "       * 作为 Message：包装成 ToolMessage 发回给 Planner，让 Planner 知道“工具调用结束了”，闭环对话逻辑。\n",
    "       * 作为 Fact：将 Report 的内容标记为 \"Fact derived from ...\" 并强制加入到 Global State 的 facts 列表中。\n",
    "\n",
    "  这样 Planner 在后续的思考中，既可以通过查看对话历史（Context）知道刚才发生了什么，也可以通过查询 facts\n",
    "  列表（Knowledge）来获取刚才查到的确切信息。这确实比仅仅返回一个 Message 更符合“多 Agent 协作”的设计初衷。\n",
    "\n",
    "> 作为 Message：包装成 ToolMessage 发回给 Planner，让 Planner 知道“工具调用结束了”，闭环对话逻辑。目前在哪里包装成了ToolMessage\n",
    "  他不是直接返回了state 吗？\n",
    "\n",
    "✦ 就在 agent/agent_runner.py 的 tool_agent_node 函数中。\n",
    "\n",
    "  虽然 Tool Agent 的子图返回的是它自己的 ToolState（包含它自己内部的 message 列表），但 Runner（即 `tool_agent_node`）\n",
    "  扮演了“中间人”的角色，负责将子图的结果翻译成 Planner 能够理解的消息格式。\n",
    "\n",
    "  具体代码在这里（agent/agent_runner.py 第 87-95 行附近）：\n",
    "\n",
    "    1 # 1. Runner 拿到 Tool Agent 子图的执行结果 result\n",
    "    2 # 2. 从 result[\"message\"] 中提取最后一条 AI 回复作为报告内容\n",
    "    3 if final_messages:\n",
    "    4     last_tool_msg = final_messages[-1]\n",
    "    5     content_to_report = last_tool_msg.content # ...\n",
    "    6\n",
    "    7 # 3. 在这里【包装】成了 ToolMessage\n",
    "    8 new_tool_messages.append(ToolMessage(\n",
    "    9     content=f\"Tool Agent Report:\\n{content_to_report}\",\n",
    "   10     tool_call_id=tool_call[\"id\"], # 这里关联了 Planner 发起的 Call ID\n",
    "   11     name=tool_call[\"name\"]\n",
    "   12 ))\n",
    "   13\n",
    "   14 # ...\n",
    "   15\n",
    "   16 # 4. 最后将这个包装好的 ToolMessage 追加到 Planner 的消息历史中\n",
    "   17 updated_state[\"message\"] = messages + new_tool_messages\n",
    "   18\n",
    "   19 # 5. 返回更新后的全图 state\n",
    "   20 return updated_state\n",
    "\n",
    "  为什么要这么做？\n",
    "  因为 Planner 发起的是一个 Tool Call（delegate_to_tool_agent）。在 LangChain/OpenAI 的协议里，每一个 tool_call 后面必须紧跟一个\n",
    "  ToolMessage，且它们的 tool_call_id 必须匹配。\n",
    "\n",
    "   * Tool Agent 的内部消息：包含它搜索、浏览的细节，这些被留在了子状态里，Planner 看不见。\n",
    "   * Runner 的包装：将 Tool Agent 的总结变成一个标准的 ToolMessage。这样当状态返回到 Planner 的 agent 节点时，LLM\n",
    "     看到自己的“委派请求”得到了“工具回复”，它才能逻辑自洽地进行下一步（比如生成最终答案）。\n",
    "\n",
    "  所以，子图返回的是状态，但 Runner 在返回前把状态里的关键结论“提炼”并“包装”成了消息。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f51c55",
   "metadata": {},
   "source": [
    " 目前的 agent_runner.py 实现不支持并行。\n",
    "\n",
    "  问题出在 tool_agent_node 函数中：\n",
    "\n",
    "    1 if isinstance(last_message, AIMessage) and getattr(last_message, \"tool_calls\", None):\n",
    "    2     for tool_call in last_message.tool_calls:\n",
    "    3         if tool_call[\"name\"] == \"delegate_to_tool_agent\":\n",
    "    4             # ... (准备参数)\n",
    "    5\n",
    "    6             # 这里的 await tool_app.ainvoke(tool_agent_input) 是阻塞的\n",
    "    7             result = await tool_app.ainvoke(tool_agent_input)\n",
    "    8\n",
    "    9             # ... (处理结果)\n",
    "   10             return updated_state # 直接返回了！\n",
    "   11             break # 循环被打断！\n",
    "\n",
    "  现在的逻辑是：\n",
    "   1. 遍历 tool_calls。\n",
    "   2. 遇到第一个 delegate 调用。\n",
    "   3. await 等待 Tool Agent 执行完成。\n",
    "   4. 拿到结果，更新 State，直接 return。\n",
    "   5. 这就意味着后续的 tool_calls（如果 Planner 一次发出了多个）会被丢弃或者在这次循环中被忽略。\n",
    "\n",
    "  要实现并行查 A 和查 B，我们需要做以下修改：\n",
    "\n",
    "   1. 收集所有任务：不要遇到一个就执行一个并返回。而是遍历 tool_calls，把所有 delegate_to_tool_agent 的请求收集起来。\n",
    "   2. 并发执行：使用 asyncio.gather 同时启动多个 Tool Agent 实例（或者复用 App 跑不同的 Input）。\n",
    "   3. 结果聚合：等待所有并发任务完成，收集它们产生的 ToolMessage 和新的 facts。\n",
    "   4. 统一返回：把所有结果一次性追加到 message 列表，并合并所有新 facts，然后返回更新后的 State。\n",
    "\n",
    "  计划修改 agent/agent_runner.py 的 tool_agent_node\n",
    "\n",
    "  我将重写 tool_agent_node 来支持并行执行。\n",
    "\n",
    "  修改逻辑：\n",
    "   1. 创建一个列表 tasks 用来存 asyncio.create_task（或者直接存 coroutine）。\n",
    "   2. 遍历 tool_calls，对每个 delegate 调用，准备好它的 input，并创建一个 coroutine。\n",
    "   3. 使用 await asyncio.gather(*tasks) 并行执行。\n",
    "   4. 遍历执行结果，生成对应的 ToolMessage 列表。\n",
    "   5. 合并所有结果中的 facts（去重）。\n",
    "   6. 更新 Planner State。\n",
    "\n",
    "  我来执行这个修改。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd31b61",
   "metadata": {},
   "source": [
    "However, there is one critical \"hidden\" component you missed between the Image Encoder and the LLM: the Projector (or Adapter).Here is the complete, accurate pipeline:$$\\text{Image} \\rightarrow \\boxed{\\text{Image Encoder}} \\rightarrow \\text{Visual Features} \\rightarrow \\boxed{\\text{Projector}} \\rightarrow \\text{Visual Tokens} \\rightarrow \\boxed{\\text{LLM Decoder}} \\rightarrow \\text{Text}$$\n",
    "- Here is the detailed breakdown of each component's role in \"seeing\":\n",
    "- 1. The Eye: Image Encoder (Vision Transformer)Input: Raw pixels (e.g., 224x224 image).What it does: It breaks the image into patches (like puzzle pieces) and turns them into mathematical vectors (embeddings).\n",
    "    - Common Models: CLIP, SigLIP, or ViT (Vision Transformer).\n",
    "    - Result: A set of \"feature vectors\" that represent visual concepts (e.g., \"curve,\" \"red,\" \"dog-ear shape\"). At this stage, the data is still \"visual math,\" not \"language math.\"\n",
    "- 2. The Bridge: **Projector** (The Missing Link)Input: Visual features from the Encoder.\n",
    "    - What it does: This is the translator. The Image Encoder might output vectors of size 1024, but the LLM expects input vectors of size 4096. The Projector reshapes and aligns the visual features so they \"look like\" word embeddings to the LLM.\n",
    "    - Common Types:MLP (Linear Layer): A simple mathematical projection (used in LLaVA).\n",
    "    - Q-Former: A more complex attention mechanism (used in BLIP-2) that compresses visual info.\n",
    "    - Result: \"Visual Tokens\". These are vectors that the LLM treats exactly the same as text tokens. To the LLM, an image just looks like a long string of weird words at the start of the sentence.\n",
    "- 3. The Brain: LLM DecoderInput: A sequence containing both Visual Tokens + Text Tokens (user prompt).Example input to LLM: [IMG_TOKEN_1] [IMG_TOKEN_2] ... [IMG_TOKEN_576] \"Describe this image\"What it does: It uses self-attention to look at the visual tokens and the text tokens together. It predicts the next text token based on the visual context.Result: The final textual description.\n",
    "- Why is the \"Projector\" so important?\n",
    "    - Without the projector, the LLM cannot understand the output of the Image Encoder.\n",
    "    - Analogy: The Image Encoder speaks \"French\" (Visual language). The LLM speaks \"English\" (Text language). The Projector is the dictionary that translates French words into English words so the LLM can read them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551b157e",
   "metadata": {},
   "source": [
    "mem0.Memory.search returns a dict with a results list (and a relations key if graph mode is enabled). Each result is a MemoryItem dict shaped like:\n",
    "\n",
    "- id: string\n",
    "- memory: the stored text/fact\n",
    "- hash: optional\n",
    "- metadata: optional dict of extra fields\n",
    "- score: float similarity score\n",
    "- created_at / updated_at: optional timestamps"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
