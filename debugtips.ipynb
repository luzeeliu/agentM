{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18e4ed9c",
   "metadata": {},
   "source": [
    "- The LLM replied with a plan and no tool_calls, so there was nothing to execute.\n",
    "\n",
    "- What I changed\n",
    "\n",
    "- Implemented actual tool execution when the LLM requests it.\n",
    "    - agent/llm_core.py:5 added ToolMessage import.\n",
    "    - agent/llm_core.py:35–63 replaced single-shot invoke with a loop that:\n",
    "        - Invokes the LLM with tools bound.\n",
    "        - Checks response.tool_calls.\n",
    "        - Executes the requested tool(s) via tool.invoke(args).\n",
    "        - Appends ToolMessage with results and re-invokes until no more tool calls or max loops reached.\n",
    "Why this matters\n",
    "\n",
    "- Previously, even if the model requested a tool, the code never ran it. Now, tool calls are executed and fed back to the LLM to produce a final answer.\n",
    "How to verify\n",
    "\n",
    "- Rebuild/restart your backend, then ask something requiring fresh info.\n",
    "- You should see logs showing tool_calls and subsequent tool execution.\n",
    "- If the LLM still doesn’t call tools, that’s a prompting/model choice issue, not a runtime failure.\n",
    "If it still doesn’t call tools\n",
    "\n",
    "Strengthen the system prompt to explicitly instruct calling a search tool for current info.\n",
    "Consider switching to a Gemini model with robust tool-calling in LangChain, e.g. gemini-1.5-flash or gemini-1.5-pro, if gemini-2.0-flash-exp doesn’t reliably issue tool calls.\n",
    "You can also force tool usage by guiding the user prompt or adding a routing step in code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d576ca3c",
   "metadata": {},
   "source": [
    "mcp server tips:\n",
    "- a bed project experience but it may useful: first we dont need build specific mcp server file like a python server file unless we want to put our own tool in the mcp server\n",
    "- second, we need build our own mcp client make agent have ability to use mcp , the over view pipeline is \n",
    "    - agent call tool and it open a session for mcp client recall the tool list in the mcp(langchain_adaptor) \n",
    "    - by the same time langchain tool adaptor will start server of this specific mcp tool \n",
    "    - after finish load the tool langchain adaptor will return langchain tools list\n",
    "    - Your Code                        langchain_mcp_adapters                MCP Server\n",
    "    |                                    |                                  |\n",
    "    |---> get_mcp_tools()                |                                  |\n",
    "    |                                    |                                  |\n",
    "    |         <--- reads mcp-server-config.json                             |\n",
    "    |                                    |                                  |\n",
    "    |         <--- spawns subprocess: npx @langgpt/arxiv-paper-mcp -------> | starts\n",
    "    |                                    |                                  |\n",
    "    |                                    |---- \"list_tools\" request ------> |\n",
    "    |                                    |                                  |\n",
    "    |                                    | <--- tools: [search_arxiv, ...] -|\n",
    "    |                                    |                                  |\n",
    "    | <--- returns LangChain tools       |                                  |\n",
    "    |                                    |                                  |\n",
    "\n",
    "- the code detail is \n",
    "    - 1. we need load our mcp config.json \n",
    "    - 2. use langchain session stdio connect to mcp server (using langchain_mcp_adapters.tools.load_mcp_tools for seamless integration) AND after connect we use load_mcp_tools for mcp toll load.(langchain_mcp_adapters.session . stdioConnetion)\n",
    "    - 3. build get_mcp_tools for synchronous return the list of langchain BaseTool\n",
    "    - 4. maybe we need change json encoder for handing special types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bdc537",
   "metadata": {},
   "source": [
    "because the mcp tool dont have common attribute for agent to use it. so we need separate different example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12970b14",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    " tool = tools[tool_name]\n",
    "\n",
    "        try:\n",
    "            # Try different invocation methods based on tool type\n",
    "            result = None\n",
    "\n",
    "            # Method 1: Try ainvoke (for MCP tools and modern LangChain tools)\n",
    "            if hasattr(tool, 'ainvoke'):\n",
    "                try:\n",
    "                    result = await tool.ainvoke(tool_args)\n",
    "                except TypeError as e:\n",
    "                    # MCP tools might need config parameter\n",
    "                    if 'config' in str(e):\n",
    "                        from langchain_core.runnables import RunnableConfig\n",
    "                        config = RunnableConfig()\n",
    "                        result = await tool.ainvoke(tool_args, config=config)\n",
    "                    else:\n",
    "                        raise\n",
    "\n",
    "            # Method 2: Try _arun (for custom async tools)\n",
    "            elif hasattr(tool, '_arun'):\n",
    "                result = await tool._arun(**tool_args)\n",
    "\n",
    "            # Method 3: Fall back to sync invoke\n",
    "            elif hasattr(tool, 'invoke'):\n",
    "                result = tool.invoke(tool_args)\n",
    "\n",
    "            # Method 4: Fall back to sync _run\n",
    "            elif hasattr(tool, '_run'):\n",
    "                result = tool._run(**tool_args)\n",
    "            else:\n",
    "                raise ValueError(f\"Tool {tool_name} has no invocation method\")\n",
    "\n",
    "            print(f\"[tool_wrapper] Tool {tool_name} result: {str(result)[:200]}...\")\n",
    "\n",
    "            tool_messages.append(ToolMessage(\n",
    "                content=str(result),\n",
    "                tool_call_id=tool_call_id,\n",
    "                name=tool_name\n",
    "            ))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
