{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfa6e2bc-7c28-4846-801c-17bbe0b39073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model: deepseek-ai/deepseek-coder-7b-instruct-v1.5\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "083cffc1271a4fe7888642a97d2c9c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running baseline eval before training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/miniconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='198' max='99' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [99/99 09:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 100015}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline eval_loss: 1.6175 | ppl: 5.04\n",
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='117' max='117' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [117/117 20:00, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Model Preparation Time</th>\n",
       "      <th>Entropy</th>\n",
       "      <th>Num Tokens</th>\n",
       "      <th>Mean Token Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.218000</td>\n",
       "      <td>0.166968</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.657846</td>\n",
       "      <td>78356.000000</td>\n",
       "      <td>0.948561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.236600</td>\n",
       "      <td>0.149940</td>\n",
       "      <td>0.004900</td>\n",
       "      <td>0.636915</td>\n",
       "      <td>156914.000000</td>\n",
       "      <td>0.953703</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running eval after training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.bfloat16 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
      "/root/miniconda3/lib/python3.12/site-packages/bitsandbytes/autograd/_functions.py:123: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n",
      "  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='99' max='99' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [99/99 00:36]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final eval_loss: 0.1493 | ppl: 1.16\n",
      "Loss improvement: 1.4682\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value\n",
    "\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value\n",
    "\n",
    "import os\n",
    "import math\n",
    "import inspect\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    ")\n",
    "from peft import LoraConfig\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "from data_collactor import DataCollator\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_ID = \"deepseek-ai/deepseek-coder-7b-instruct-v1.5\" # Or \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "DATA_PATH = \"lora_data_train.json\"\n",
    "VAL_DATA_PATH = \"lora_data_val.json\"\n",
    "OUTPUT_DIR = \"./results_qlora\"\n",
    "\n",
    "# QLoRA Parameters\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 32\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(f\"Loading model: {MODEL_ID}\")\n",
    "    \n",
    "    # 1. Quantization Config (8-bit loading)\n",
    "    # 8-bit provides better precision than 4-bit, but uses more VRAM (~10GB vs ~6GB for 7B model).\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        bnb_8bit_quant_type=\"nf8\",\n",
    "        bnb_8bit_use_double_quant=True,\n",
    "        bnb_8bit_compute_dtype=torch.float16\n",
    "    )\n",
    "\n",
    "    # 2. Load Model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_ID,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map={\"\": 0},\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # 3. Load Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\" # Fix for fp16 training\n",
    "\n",
    "    # 4. Load Dataset\n",
    "    dataset = load_dataset(\"json\", data_files={\"train\": DATA_PATH, \"validation\": VAL_DATA_PATH})\n",
    "\n",
    "    # 5. Define Masking (Crucial Step)\n",
    "    # We want the model to learn ONLY the Assistant's response.\n",
    "    # The DataCollator finds the \"response_template\" and masks everything before it.\n",
    "    collator = DataCollator(tokenizer=tokenizer, max_length=1024)\n",
    "\n",
    "    # 6. LoRA Configuration\n",
    "    peft_config = LoraConfig(\n",
    "        r=LORA_R,\n",
    "        lora_alpha=LORA_ALPHA,\n",
    "        lora_dropout=LORA_DROPOUT,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"] # Target attention layers\n",
    "    )\n",
    "\n",
    "    # 7. Training Arguments\n",
    "    args = SFTConfig(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_train_epochs= 1,\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=8,\n",
    "        learning_rate=2e-4,\n",
    "        logging_steps=10,\n",
    "        gradient_checkpointing=True,\n",
    "        save_strategy=\"epoch\",\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        report_to=\"none\",\n",
    "        remove_unused_columns=False,\n",
    "        per_device_eval_batch_size=1,\n",
    "        dataset_kwargs={\"skip_prepare_dataset\": True},\n",
    "    )\n",
    "\n",
    "    # 8. Trainer\n",
    "    trainer = SFTTrainer(\n",
    "        model = model, \n",
    "        train_dataset = dataset[\"train\"],\n",
    "        eval_dataset= dataset[\"validation\"],\n",
    "        processing_class = tokenizer,\n",
    "        data_collator = collator,\n",
    "        peft_config = peft_config,\n",
    "        args = args\n",
    "    )\n",
    "\n",
    "    print(\"Running baseline eval before training...\")\n",
    "    base_metrics = trainer.evaluate()\n",
    "    base_loss = base_metrics.get(\"eval_loss\")\n",
    "    if base_loss is not None:\n",
    "        base_ppl = math.exp(base_loss)\n",
    "        print(f\"Baseline eval_loss: {base_loss:.4f} | ppl: {base_ppl:.2f}\")\n",
    "    else:\n",
    "        print(f\"Baseline eval metrics: {base_metrics}\")\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    print(\"Running eval after training...\")\n",
    "    final_metrics = trainer.evaluate()\n",
    "    final_loss = final_metrics.get(\"eval_loss\")\n",
    "    if final_loss is not None:\n",
    "        final_ppl = math.exp(final_loss)\n",
    "        print(f\"Final eval_loss: {final_loss:.4f} | ppl: {final_ppl:.2f}\")\n",
    "        if base_loss is not None:\n",
    "            improvement = base_loss - final_loss\n",
    "            print(f\"Loss improvement: {improvement:.4f}\")\n",
    "    else:\n",
    "        print(f\"Final eval metrics: {final_metrics}\")\n",
    "\n",
    "    print(\"Saving model...\")\n",
    "    trainer.save_model(os.path.join(OUTPUT_DIR, \"final_adapter\"))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b30e735-4843-4e22-bd82-47c81363b439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trl: 0.26.2\n",
      "['DPODataCollatorWithPadding', 'DataCollatorForChatML', 'RewardDataCollatorWithPadding']\n"
     ]
    }
   ],
   "source": [
    "import trl, inspect, trl.trainer.utils as u\n",
    "print(\"trl:\", trl.__version__)\n",
    "names = [n for n in dir(u) if \"Collator\" in n]\n",
    "print(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06acfda4-e2df-4bc8-a3fb-628b5a9a1a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRL version: 0.26.2\n",
      "DataCollatorForCompletionOnlyLM NOT found in trl\n",
      "DataCollatorForCompletionOnlyLM NOT found in trl.trainer\n",
      "DataCollatorForCompletionOnlyLM NOT found in trl.trainer.utils\n",
      "DataCollatorForChatML NOT found in trl\n",
      "DataCollatorForChatML found in trl.trainer.utils\n"
     ]
    }
   ],
   "source": [
    "import trl\n",
    "import inspect\n",
    "\n",
    "print(f\"TRL version: {trl.__version__}\")\n",
    "\n",
    "try:\n",
    "    from trl import DataCollatorForCompletionOnlyLM\n",
    "    print(\"DataCollatorForCompletionOnlyLM found in trl\")\n",
    "except ImportError:\n",
    "    print(\"DataCollatorForCompletionOnlyLM NOT found in trl\")\n",
    "\n",
    "try:\n",
    "    from trl.trainer import DataCollatorForCompletionOnlyLM\n",
    "    print(\"DataCollatorForCompletionOnlyLM found in trl.trainer\")\n",
    "except ImportError:\n",
    "    print(\"DataCollatorForCompletionOnlyLM NOT found in trl.trainer\")\n",
    "\n",
    "try:\n",
    "    from trl.trainer.utils import DataCollatorForCompletionOnlyLM\n",
    "    print(\"DataCollatorForCompletionOnlyLM found in trl.trainer.utils\")\n",
    "except ImportError:\n",
    "    print(\"DataCollatorForCompletionOnlyLM NOT found in trl.trainer.utils\")\n",
    "\n",
    "try:\n",
    "    from trl import DataCollatorForChatML\n",
    "    print(\"DataCollatorForChatML found in trl\")\n",
    "except ImportError:\n",
    "    print(\"DataCollatorForChatML NOT found in trl\")\n",
    "\n",
    "try:\n",
    "    from trl.trainer.utils import DataCollatorForChatML\n",
    "    print(\"DataCollatorForChatML found in trl.trainer.utils\")\n",
    "except ImportError:\n",
    "    print(\"DataCollatorForChatML NOT found in trl.trainer.utils\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9219923d-5c5d-41ef-bd0d-f0399c555da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id overlap: 0\n",
      "query overlap: 37\n",
      "2340 260\n",
      "l2_market_92ff02cd l2_crossref_fc4c51a4\n",
      "labeled token ratio: 0.09540636092424393\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "MODEL_ID = \"deepseek-ai/deepseek-coder-7b-instruct-v1.5\" # Or \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "DATA_PATH = \"lora_data_train.json\"\n",
    "VAL_DATA_PATH = \"lora_data_val.json\"\n",
    "OUTPUT_DIR = \"./results_qlora\"\n",
    "\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": DATA_PATH, \"validation\": VAL_DATA_PATH})\n",
    "train_ids = set(dataset[\"train\"][\"id\"])\n",
    "val_ids   = set(dataset[\"validation\"][\"id\"])\n",
    "print(\"id overlap:\", len(train_ids & val_ids))\n",
    "\n",
    "train_q = set(dataset[\"train\"][\"query\"])\n",
    "val_q   = set(dataset[\"validation\"][\"query\"])\n",
    "print(\"query overlap:\", len(train_q & val_q))\n",
    "\n",
    "print(len(dataset[\"train\"]), len(dataset[\"validation\"]))\n",
    "print(dataset[\"train\"][0][\"id\"], dataset[\"validation\"][0][\"id\"])\n",
    "\n",
    "from trl.trainer.utils import DataCollatorForChatML\n",
    "response_template = \"<|assistant|>\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\" # Fix for fp16 training\n",
    "sig = inspect.signature(DataCollatorForChatML)\n",
    "kwargs = {\"tokenizer\": tokenizer}\n",
    "if \"response_template\" in sig.parameters:\n",
    "    kwargs[\"response_template\"] = response_template\n",
    "\n",
    "collator = DataCollatorForChatML(**kwargs)\n",
    "if not hasattr(collator, \"response_template\"):\n",
    "    collator.response_template = response_template\n",
    "if hasattr(collator, \"prompt_key\"):\n",
    "    # Use the formatted text produced by formatting_func instead of raw messages\n",
    "    collator.prompt_key = \"text\"\n",
    "\n",
    "\n",
    "dl = DataLoader(dataset[\"train\"], batch_size=1, collate_fn=collator)\n",
    "batch = next(iter(dl))\n",
    "labels = batch[\"labels\"]\n",
    "print(\"labeled token ratio:\", (labels != -100).float().mean().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba22ba51-bf53-4414-94f7-ea1a8601656b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labeled tokens: 27\n",
      "total tokens: 283\n",
      "first labeled index: 256 last: 282\n"
     ]
    }
   ],
   "source": [
    "# the dataset have lackage and the label token ratio have \n",
    "ids = batch[\"input_ids\"][0]\n",
    "labels = batch[\"labels\"][0]\n",
    "text = tokenizer.decode(ids, skip_special_tokens=False)\n",
    "\n",
    "# Roughly count labeled span length\n",
    "labeled_positions = (labels != -100).nonzero().squeeze(-1)\n",
    "print(\"labeled tokens:\", labeled_positions.numel())\n",
    "print(\"total tokens:\", labels.numel())\n",
    "print(\"first labeled index:\", labeled_positions[0].item(), \"last:\", labeled_positions[-1].item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa0a813-3aea-408e-b480-9472c5382901",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
